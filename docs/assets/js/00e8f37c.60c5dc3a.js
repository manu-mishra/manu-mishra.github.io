"use strict";(self.webpackChunkuiv_2=self.webpackChunkuiv_2||[]).push([[3255],{6190:e=>{e.exports=JSON.parse('{"permalink":"/blog/deploy-microsoft-bitnet-llm-on-aws-lambda","source":"@site/blog/2025-06-20-bitnet-lambda-serverless-llm.mdx","title":"Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization","description":"Deploy Microsoft BitNet 1.58-bit quantized LLM on AWS Lambda using container-based architecture. Includes performance benchmarks, multi-stage Docker build, and complete deployment guide.","date":"2025-06-20T00:00:00.000Z","tags":[{"inline":true,"label":"aws lambda","permalink":"/blog/tags/aws-lambda"},{"inline":true,"label":"llm","permalink":"/blog/tags/llm"},{"inline":true,"label":"quantization","permalink":"/blog/tags/quantization"},{"inline":true,"label":"serverless","permalink":"/blog/tags/serverless"},{"inline":true,"label":"bitnet","permalink":"/blog/tags/bitnet"},{"inline":true,"label":"machine learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"cost optimization","permalink":"/blog/tags/cost-optimization"}],"readingTime":5.87,"hasTruncateMarker":true,"authors":[{"name":"Manu Mishra","title":"Solutions Architect & Applied Software Engineer","url":"https://github.com/manu-mishra","image_url":"/img/logo.png","imageURL":"/img/logo.png","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"deploy-microsoft-bitnet-llm-on-aws-lambda","title":"Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization","description":"Deploy Microsoft BitNet 1.58-bit quantized LLM on AWS Lambda using container-based architecture. Includes performance benchmarks, multi-stage Docker build, and complete deployment guide.","keywords":["microsoft bitnet","aws lambda llm","serverless ai","1.58-bit quantization","cpu inference","bitnet deployment","lambda container","quantized models","aws ai inference","docker multi-stage build"],"image":"/img/blog/bitnet-on-lambda.png","authors":{"name":"Manu Mishra","title":"Solutions Architect & Applied Software Engineer","url":"https://github.com/manu-mishra","image_url":"/img/logo.png","imageURL":"/img/logo.png"},"tags":["aws lambda","llm","quantization","serverless","bitnet","machine learning","cost optimization"],"date":"2025-06-20T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Google\'s EmbeddingGemma on AWS Lambda - A Curiosity-Driven Experiment","permalink":"/blog/embeddings-gemma-on-lambda"},"nextItem":{"title":"Threat Modeling for Autonomous AI - What OWASP Wants You to Know","permalink":"/blog/threat-modeling-autonomous-ai"}}')},6397:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>s,default:()=>c,frontMatter:()=>l,metadata:()=>i,toc:()=>m});var i=n(6190),a=n(4848),o=n(8453);const l={slug:"deploy-microsoft-bitnet-llm-on-aws-lambda",title:"Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization",description:"Deploy Microsoft BitNet 1.58-bit quantized LLM on AWS Lambda using container-based architecture. Includes performance benchmarks, multi-stage Docker build, and complete deployment guide.",keywords:["microsoft bitnet","aws lambda llm","serverless ai","1.58-bit quantization","cpu inference","bitnet deployment","lambda container","quantized models","aws ai inference","docker multi-stage build"],image:"/img/blog/bitnet-on-lambda.png",authors:{name:"Manu Mishra",title:"Solutions Architect & Applied Software Engineer",url:"https://github.com/manu-mishra",image_url:"/img/logo.png"},tags:["aws lambda","llm","quantization","serverless","bitnet","machine learning","cost optimization"],date:new Date("2025-06-20T00:00:00.000Z")},s=void 0,r={authorsImageUrls:[void 0]},m=[];function d(e){const t={code:"code",img:"img",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"BitNet on AWS Lambda",src:n(9672).A+"",width:"1536",height:"1024"})}),"\n",(0,a.jsxs)(t.p,{children:["\u2728 ",(0,a.jsx)(t.strong,{children:"What you'll learn (tl;dr)"})," In ~12 minutes you'll see how to deploy Microsoft's BitNet 1.58-bit quantized LLM on AWS Lambda, the container-based architecture, and performance benchmarks across different memory configurations using the ",(0,a.jsx)(t.code,{children:"microsoft/bitnet-b1.58-2B-4T"})," model."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Big idea"}),": 1.58-bit quantization enables LLM deployment on Lambda's CPU infrastructure. At ~1.1GB, the model fits within Lambda's constraints for serverless AI inference."]})]})}function c(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>s});var i=n(6540);const a={},o=i.createContext(a);function l(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(o.Provider,{value:t},e.children)}},9672:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/bitnet-on-lambda-79b5780ed52f60f02aded51ddc499e0f.png"}}]);