"use strict";(self.webpackChunkuiv_2=self.webpackChunkuiv_2||[]).push([[364],{4825:(e,i,n)=>{n.d(i,{A:()=>t});const t="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 13.0.1 (20250615.1724)
 -->
<!-- Title: INFRA Pages: 1 -->
<svg width="778pt" height="349pt"
 viewBox="0.00 0.00 778.00 349.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 345.47)">
<title>INFRA</title>
<polygon fill="white" stroke="none" points="-4,4 -4,-345.47 773.61,-345.47 773.61,4 -4,4"/>
<!-- User -->
<g id="node1" class="node">
<title>User</title>
<ellipse fill="#ffe5b4" stroke="black" stroke-width="2" cx="64.29" cy="-303.48" rx="64.29" ry="36"/>
<text xml:space="preserve" text-anchor="middle" x="64.29" y="-305.93" font-family="Arial" font-size="14.00">User/Client</text>
<text xml:space="preserve" text-anchor="middle" x="64.29" y="-290.18" font-family="Arial" font-size="14.00">(API Requests)</text>
</g>
<!-- BitNetLambda -->
<g id="node2" class="node">
<title>BitNetLambda</title>
<path fill="#ff9900" stroke="black" stroke-width="2" d="M403.66,-196.5C403.66,-196.5 254.91,-196.5 254.91,-196.5 248.91,-196.5 242.91,-190.5 242.91,-184.5 242.91,-184.5 242.91,-136.5 242.91,-136.5 242.91,-130.5 248.91,-124.5 254.91,-124.5 254.91,-124.5 403.66,-124.5 403.66,-124.5 409.66,-124.5 415.66,-130.5 415.66,-136.5 415.66,-136.5 415.66,-184.5 415.66,-184.5 415.66,-190.5 409.66,-196.5 403.66,-196.5"/>
<text xml:space="preserve" text-anchor="middle" x="329.29" y="-170.82" font-family="Arial" font-size="14.00">BitNet Lambda Function</text>
<text xml:space="preserve" text-anchor="middle" x="329.29" y="-155.07" font-family="Arial" font-size="14.00">(AWS::Lambda::Function)</text>
<text xml:space="preserve" text-anchor="middle" x="329.29" y="-139.32" font-family="Arial" font-size="14.00">Container Image</text>
</g>
<!-- User&#45;&gt;BitNetLambda -->
<g id="edge1" class="edge">
<title>User&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M46.95,-268.66C40.42,-250.74 37.19,-229.41 49.54,-214.5 72.55,-186.72 160.24,-173.39 231.39,-167.06"/>
<polygon fill="black" stroke="black" points="231.48,-170.57 241.15,-166.23 230.89,-163.59 231.48,-170.57"/>
<text xml:space="preserve" text-anchor="middle" x="93.41" y="-234.2" font-family="Times,serif" font-size="14.00">HTTP Request</text>
<text xml:space="preserve" text-anchor="middle" x="93.41" y="-217.7" font-family="Times,serif" font-size="14.00">(JSON payload)</text>
</g>
<!-- BitNetLambda&#45;&gt;User -->
<g id="edge2" class="edge">
<title>BitNetLambda&#45;&gt;User</title>
<path fill="none" stroke="black" d="M242.75,-176.09C214,-184 183.14,-196.09 158.79,-214.5 144.83,-225.06 149.24,-234.71 137.29,-247.5 130.44,-254.83 122.53,-261.89 114.49,-268.37"/>
<polygon fill="black" stroke="black" points="112.35,-265.6 106.6,-274.49 116.65,-271.13 112.35,-265.6"/>
<text xml:space="preserve" text-anchor="middle" x="203.04" y="-234.2" font-family="Times,serif" font-size="14.00">AI Response</text>
<text xml:space="preserve" text-anchor="middle" x="203.04" y="-217.7" font-family="Times,serif" font-size="14.00">(Generated text)</text>
</g>
<!-- CloudWatchLogs -->
<g id="node4" class="node">
<title>CloudWatchLogs</title>
<path fill="#90ee90" stroke="black" stroke-width="2" d="M582.04,-72C582.04,-72 444.54,-72 444.54,-72 438.54,-72 432.54,-66 432.54,-60 432.54,-60 432.54,-12 432.54,-12 432.54,-6 438.54,0 444.54,0 444.54,0 582.04,0 582.04,0 588.04,0 594.04,-6 594.04,-12 594.04,-12 594.04,-60 594.04,-60 594.04,-66 588.04,-72 582.04,-72"/>
<text xml:space="preserve" text-anchor="middle" x="513.29" y="-38.45" font-family="Arial" font-size="14.00">CloudWatch Logs</text>
<text xml:space="preserve" text-anchor="middle" x="513.29" y="-22.7" font-family="Arial" font-size="14.00">(AWS::Logs::LogGroup)</text>
</g>
<!-- BitNetLambda&#45;&gt;CloudWatchLogs -->
<g id="edge4" class="edge">
<title>BitNetLambda&#45;&gt;CloudWatchLogs</title>
<path fill="none" stroke="black" d="M382.51,-124.07C403.87,-109.85 428.64,-93.35 450.85,-78.57"/>
<polygon fill="black" stroke="black" points="452.54,-81.65 458.93,-73.19 448.66,-75.82 452.54,-81.65"/>
<text xml:space="preserve" text-anchor="middle" x="471.68" y="-93.2" font-family="Times,serif" font-size="14.00">Function Logs</text>
</g>
<!-- ECRRepo -->
<g id="node3" class="node">
<title>ECRRepo</title>
<path fill="#b7e0ff" stroke="black" stroke-width="2" d="M293.66,-332.94C293.66,-336.55 263.02,-339.48 225.29,-339.48 187.56,-339.48 156.91,-336.55 156.91,-332.94 156.91,-332.94 156.91,-274.03 156.91,-274.03 156.91,-270.42 187.56,-267.48 225.29,-267.48 263.02,-267.48 293.66,-270.42 293.66,-274.03 293.66,-274.03 293.66,-332.94 293.66,-332.94"/>
<path fill="none" stroke="black" stroke-width="2" d="M293.66,-332.94C293.66,-329.33 263.02,-326.39 225.29,-326.39 187.56,-326.39 156.91,-329.33 156.91,-332.94"/>
<text xml:space="preserve" text-anchor="middle" x="225.29" y="-305.93" font-family="Arial" font-size="14.00">ECR Repository</text>
<text xml:space="preserve" text-anchor="middle" x="225.29" y="-290.18" font-family="Arial" font-size="14.00">(Container Registry)</text>
</g>
<!-- ECRRepo&#45;&gt;BitNetLambda -->
<g id="edge3" class="edge">
<title>ECRRepo&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M240.98,-267.29C250.41,-246.88 261.88,-223.48 268.79,-214.5 271.27,-211.26 273.98,-208.08 276.83,-204.97"/>
<polygon fill="black" stroke="black" points="279.11,-207.65 283.57,-198.04 274.09,-202.77 279.11,-207.65"/>
<text xml:space="preserve" text-anchor="middle" x="314.54" y="-225.95" font-family="Times,serif" font-size="14.00">Container Image</text>
</g>
<!-- BitNetModel -->
<g id="node5" class="node">
<title>BitNetModel</title>
<path fill="#fff5cd" stroke="black" stroke-width="2" d="M470.54,-334.56C470.54,-338.37 435.02,-341.47 391.29,-341.47 347.56,-341.47 312.04,-338.37 312.04,-334.56 312.04,-334.56 312.04,-272.41 312.04,-272.41 312.04,-268.6 347.56,-265.5 391.29,-265.5 435.02,-265.5 470.54,-268.6 470.54,-272.41 470.54,-272.41 470.54,-334.56 470.54,-334.56"/>
<path fill="none" stroke="black" stroke-width="2" d="M470.54,-334.56C470.54,-330.75 435.02,-327.66 391.29,-327.66 347.56,-327.66 312.04,-330.75 312.04,-334.56"/>
<text xml:space="preserve" text-anchor="middle" x="391.29" y="-313.81" font-family="Arial" font-size="14.00">BitNet 1.58B Model</text>
<text xml:space="preserve" text-anchor="middle" x="391.29" y="-298.06" font-family="Arial" font-size="14.00">(ggml&#45;model&#45;i2_s.gguf)</text>
<text xml:space="preserve" text-anchor="middle" x="391.29" y="-282.31" font-family="Arial" font-size="14.00">Embedded in Container</text>
</g>
<!-- BitNetModel&#45;&gt;BitNetLambda -->
<g id="edge5" class="edge">
<title>BitNetModel&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M382.4,-265.22C377.93,-249.21 371.85,-230.54 364.29,-214.5 363.07,-211.93 361.75,-209.33 360.35,-206.75"/>
<polygon fill="black" stroke="black" points="363.44,-205.1 355.4,-198.21 357.38,-208.61 363.44,-205.1"/>
<text xml:space="preserve" text-anchor="middle" x="422.43" y="-225.95" font-family="Times,serif" font-size="14.00">Model Inference</text>
</g>
<!-- LambdaRole -->
<g id="node6" class="node">
<title>LambdaRole</title>
<path fill="#dda0dd" stroke="black" stroke-width="2" d="M622.69,-336.4C622.69,-336.4 510.57,-306.57 510.57,-306.57 504.77,-305.03 504.77,-301.94 510.57,-300.4 510.57,-300.4 622.69,-270.57 622.69,-270.57 628.49,-269.03 640.09,-269.03 645.88,-270.57 645.88,-270.57 758.01,-300.4 758.01,-300.4 763.81,-301.94 763.81,-305.03 758.01,-306.57 758.01,-306.57 645.88,-336.4 645.88,-336.4 640.09,-337.94 628.49,-337.94 622.69,-336.4"/>
<text xml:space="preserve" text-anchor="middle" x="634.29" y="-305.93" font-family="Arial" font-size="14.00">Lambda Execution Role</text>
<text xml:space="preserve" text-anchor="middle" x="634.29" y="-290.18" font-family="Arial" font-size="14.00">(AWS::IAM::Role)</text>
</g>
<!-- LambdaRole&#45;&gt;BitNetLambda -->
<g id="edge6" class="edge">
<title>LambdaRole&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M572.63,-283.46C549.23,-274.61 523.11,-262.64 501.79,-247.5 485.75,-236.11 488.55,-225.58 472.29,-214.5 458.29,-204.96 442.35,-196.88 426.33,-190.12"/>
<polygon fill="black" stroke="black" points="427.79,-186.94 417.2,-186.44 425.16,-193.43 427.79,-186.94"/>
<text xml:space="preserve" text-anchor="middle" x="564.04" y="-225.95" font-family="Times,serif" font-size="14.00">Execution Permissions</text>
</g>
<!-- LambdaRole&#45;&gt;CloudWatchLogs -->
<g id="edge7" class="edge">
<title>LambdaRole&#45;&gt;CloudWatchLogs</title>
<path fill="none" stroke="black" d="M634.88,-267.43C634.21,-250.98 632.03,-231.34 626.29,-214.5 609.4,-164.99 576.04,-115.33 549.93,-81.18"/>
<polygon fill="black" stroke="black" points="552.95,-79.35 544.05,-73.6 547.42,-83.64 552.95,-79.35"/>
<text xml:space="preserve" text-anchor="middle" x="664.1" y="-155.45" font-family="Times,serif" font-size="14.00">Log Permissions</text>
</g>
</g>
</svg>
"},5640:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});var t=n(6190),s=n(4848),l=n(8453);const a={slug:"deploy-microsoft-bitnet-llm-on-aws-lambda",title:"Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization",description:"Deploy Microsoft BitNet 1.58-bit quantized LLM on AWS Lambda using container-based architecture. Includes performance benchmarks, multi-stage Docker build, and complete deployment guide.",keywords:["microsoft bitnet","aws lambda llm","serverless ai","1.58-bit quantization","cpu inference","bitnet deployment","lambda container","quantized models","aws ai inference","docker multi-stage build"],image:"/img/blog/bitnet-on-lambda.png",authors:{name:"Manu Mishra",title:"Solutions Architect & Applied Software Engineer",url:"https://github.com/manu-mishra",image_url:"/img/logo.png"},tags:["aws lambda","llm","quantization","serverless","bitnet","machine learning","cost optimization"],date:new Date("2025-06-20T00:00:00.000Z")},r=void 0,d={authorsImageUrls:[void 0]},c=[{value:"Deploying BitNet on Lambda",id:"deploying-bitnet-on-lambda",level:2},{value:"Model Characteristics",id:"model-characteristics",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Multi-Stage Docker Build",id:"multi-stage-docker-build",level:3},{value:"Lambda Runtime Optimizations",id:"lambda-runtime-optimizations",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"1. Initialize and Download",id:"1-initialize-and-download",level:3},{value:"2. Deploy Infrastructure",id:"2-deploy-infrastructure",level:3},{value:"3. Test Inference",id:"3-test-inference",level:3},{value:"Performance Results",id:"performance-results",level:2},{value:"Memory Configuration",id:"memory-configuration",level:3},{value:"Test Results",id:"test-results",level:3},{value:"Inference Parameters",id:"inference-parameters",level:3},{value:"Monitoring and Debugging",id:"monitoring-and-debugging",level:2},{value:"Implementation Notes",id:"implementation-notes",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function o(e){const i={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"BitNet on AWS Lambda",src:n(9672).A+"",width:"1536",height:"1024"})}),"\n",(0,s.jsxs)(i.p,{children:["\u2728 ",(0,s.jsx)(i.strong,{children:"What you'll learn (tl;dr)"})," In ~12 minutes you'll see how to deploy Microsoft's BitNet 1.58-bit quantized LLM on AWS Lambda, the container-based architecture, and performance benchmarks across different memory configurations using the ",(0,s.jsx)(i.code,{children:"microsoft/bitnet-b1.58-2B-4T"})," model."]}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Big idea"}),": 1.58-bit quantization enables LLM deployment on Lambda's CPU infrastructure. At ~1.1GB, the model fits within Lambda's constraints for serverless AI inference."]}),"\n",(0,s.jsx)(i.h2,{id:"deploying-bitnet-on-lambda",children:"Deploying BitNet on Lambda"}),"\n",(0,s.jsxs)(i.p,{children:["Microsoft's BitNet ",(0,s.jsx)(i.code,{children:"microsoft/bitnet-b1.58-2B-4T"})," is a 1.58-bit quantized model that uses ternary values ",1,". At ~1.1GB, it fits within Lambda's memory and storage constraints."]}),"\n",(0,s.jsx)(i.h2,{id:"model-characteristics",children:"Model Characteristics"}),"\n",(0,s.jsxs)(i.p,{children:["Microsoft's BitNet ",(0,s.jsx)(i.code,{children:"microsoft/bitnet-b1.58-2B-4T"})," uses 1.58-bit quantization with ternary values ",1,":"]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Model size"}),": ~1.1GB including dependencies"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"CPU inference"}),": No GPU required"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Memory requirements"}),": Fits within Lambda's memory limits"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Text processing"}),": Designed for natural language tasks"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Lambda bills per millisecond and doesn't provide GPU access, making CPU-optimized models necessary."}),"\n",(0,s.jsx)(i.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"BitNet Lambda Architecture",src:n(4825).A+"",width:"1037",height:"465"})}),"\n",(0,s.jsx)(i.p,{children:"The deployment uses serverless execution with the model embedded in the container image:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"No network calls during inference"})," - Model and code are in the same container"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Single deployment unit"})," - No external model storage dependencies"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Consistent behavior"})," - Same environment across all invocations"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"multi-stage-docker-build",children:"Multi-Stage Docker Build"}),"\n",(0,s.jsx)(i.p,{children:"The deployment uses a multi-stage Docker build that separates compilation from runtime."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Stage 1: Builder Environment"})}),"\n",(0,s.jsxs)(i.p,{children:["Creates a development environment to compile BitNet from source. Uses ",(0,s.jsx)(i.code,{children:"python:3.9-bullseye"})," with cmake, build-essential, git, and clang."]}),"\n",(0,s.jsx)(i.p,{children:"The critical step is generating ARM-optimized computational kernels. Lambda runs on ARM64 processors, so BitNet's code generation utility pre-compiles optimized matrix multiplication kernels for this architecture."}),"\n",(0,s.jsxs)(i.p,{children:["Compilation includes Lambda-specific optimizations: OpenMP disabled (",(0,s.jsx)(i.code,{children:"-DGGML_OPENMP=OFF"}),") because Lambda's sandboxed environment doesn't support shared memory operations. ARM template optimizations enabled (",(0,s.jsx)(i.code,{children:"-DBITNET_ARM_TL1=ON"}),") for ARM64 instruction sets. Static linking (",(0,s.jsx)(i.code,{children:"-DBUILD_SHARED_LIBS=OFF"}),") embeds all dependencies into the binary."]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Stage 2: Runtime Environment"})}),"\n",(0,s.jsxs)(i.p,{children:["Creates a minimal production environment using ",(0,s.jsx)(i.code,{children:"python:3.9-slim"}),". Installs only AWS Lambda Runtime Interface Client (",(0,s.jsx)(i.code,{children:"awslambdaric"}),") and ",(0,s.jsx)(i.code,{children:"requests"})," library."]}),"\n",(0,s.jsxs)(i.p,{children:["Copies only the compiled ",(0,s.jsx)(i.code,{children:"llama-server"})," binary and BitNet model file from the builder stage. The final container includes the optimized binary without build tools, source code, or compilation artifacts."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-dockerfile",children:'# Stage 1: Builder - Heavy build environment\nFROM python:3.9-bullseye as builder\n\n# Install build dependencies\nRUN apt-get update && \\\n    apt-get install -y cmake build-essential git clang && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Copy BitNet source and model\nCOPY temp/BitNet /app/BitNet\nCOPY temp/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf /app/BitNet/models/BitNet-b1.58-2B-4T/\n\n# Generate ARM-optimized kernels for Lambda\'s ARM64 runtime\nRUN python utils/codegen_tl1.py --model bitnet_b1_58-3B --BM 160,320,320 --BK 64,128,64 --bm 32,64,32\n\n# Build with Lambda-specific optimizations\nRUN cmake -B build -DBITNET_ARM_TL1=ON -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_OPENMP=OFF\nRUN cmake --build build --config Release\n\n# Stage 2: Runtime - Minimal production environment\nFROM python:3.9-slim\n\n# Install only runtime dependencies\nRUN pip install --no-cache-dir awslambdaric requests\n\n# Copy only the compiled binary and model from builder stage\nCOPY --from=builder /app/BitNet/build/bin/llama-server /app/bin/\nCOPY --from=builder /app/BitNet/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf /app/models/\n\n# Copy Lambda handler and set up runtime\nCOPY app/lambda_handler.py /var/task/\nWORKDIR /var/task\nCMD ["python", "-m", "awslambdaric", "lambda_handler.lambda_handler"]\n'})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Build Process"})}),"\n",(0,s.jsx)(i.p,{children:"This multi-stage approach reduces final image size and ensures Lambda compatibility by including ARM64 optimizations and removing problematic dependencies like OpenMP. Each deployment requires full compilation, but this produces a container optimized for Lambda's constraints."}),"\n",(0,s.jsx)(i.h3,{id:"lambda-runtime-optimizations",children:"Lambda Runtime Optimizations"}),"\n",(0,s.jsx)(i.p,{children:"The Lambda environment requires specific threading configurations to prevent model initialization failures:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-python",children:"# Critical environment overrides for Lambda\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['OMP_THREAD_LIMIT'] = '1'\nos.environ['GGML_OPENMP'] = 'OFF'\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n"})}),"\n",(0,s.jsx)(i.p,{children:"These settings prevent the threading conflicts that plague many ML workloads in Lambda's sandboxed environment."}),"\n",(0,s.jsx)(i.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,s.jsxs)(i.p,{children:["The complete working implementation is available at ",(0,s.jsx)(i.strong,{children:(0,s.jsx)(i.a,{href:"https://github.com/manu-mishra/one-bit-llm-on-lambda",children:"github.com/manu-mishra/one-bit-llm-on-lambda"})}),". The deployment process is streamlined into three modular scripts:"]}),"\n",(0,s.jsx)(i.h3,{id:"1-initialize-and-download",children:"1. Initialize and Download"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"git clone https://github.com/manu-mishra/one-bit-llm-on-lambda.git\ncd one-bit-llm-on-lambda\n./scripts/1-initialize.sh\n"})}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Important:"})," You need a Hugging Face token to download the BitNet model:"]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Get your token from: ",(0,s.jsx)(i.a,{href:"https://huggingface.co/settings/tokens",children:"https://huggingface.co/settings/tokens"})]}),"\n",(0,s.jsx)(i.li,{children:'Create a token with "Read" permissions'}),"\n",(0,s.jsx)(i.li,{children:"The script includes retry logic if authentication fails"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Downloads BitNet source and model (~1.1GB) from Microsoft's Hugging Face repository."}),"\n",(0,s.jsx)(i.h3,{id:"2-deploy-infrastructure",children:"2. Deploy Infrastructure"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"cd cdk && python -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt && cd ..\n./scripts/2-deploy-lambda.sh\n"})}),"\n",(0,s.jsx)(i.p,{children:"Uses AWS CDK to provision AWS Lambda, Amazon ECR, IAM roles, and supporting infrastructure."}),"\n",(0,s.jsx)(i.h3,{id:"3-test-inference",children:"3. Test Inference"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"./scripts/3-test-lambda.sh\n"})}),"\n",(0,s.jsx)(i.p,{children:"Tests the deployment with a sample prompt. For performance testing across memory configurations:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"./scripts/5-benchmark.sh\n"})}),"\n",(0,s.jsx)(i.p,{children:"The repository includes AWS CDK infrastructure code, Docker configuration, testing utilities, and documentation."}),"\n",(0,s.jsx)(i.h2,{id:"performance-results",children:"Performance Results"}),"\n",(0,s.jsx)(i.p,{children:"Benchmarking across different memory configurations:"}),"\n",(0,s.jsx)(i.h3,{id:"memory-configuration",children:"Memory Configuration"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-python",children:"LAMBDA_MEMORY_SIZE = 2048  # 2GB recommended\n"})}),"\n",(0,s.jsx)(i.h3,{id:"test-results",children:"Test Results"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Memory"}),(0,s.jsx)(i.th,{children:"Cold Start"}),(0,s.jsx)(i.th,{children:"Warm (10 tokens)"}),(0,s.jsx)(i.th,{children:"Warm (50 tokens)"}),(0,s.jsx)(i.th,{children:"Warm (100 tokens)"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"2GB"}),(0,s.jsx)(i.td,{children:"12s"}),(0,s.jsx)(i.td,{children:"7s"}),(0,s.jsx)(i.td,{children:"18s"}),(0,s.jsx)(i.td,{children:"32s"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"6GB"}),(0,s.jsx)(i.td,{children:"13s"}),(0,s.jsx)(i.td,{children:"6s"}),(0,s.jsx)(i.td,{children:"18s"}),(0,s.jsx)(i.td,{children:"32s"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"10GB"}),(0,s.jsx)(i.td,{children:"12s"}),(0,s.jsx)(i.td,{children:"7s"}),(0,s.jsx)(i.td,{children:"18s"}),(0,s.jsx)(i.td,{children:"32s"})]})]})]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Observations:"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Cold start times: 12-13 seconds across memory configurations"}),"\n",(0,s.jsx)(i.li,{children:"Warm inference scales with token count"}),"\n",(0,s.jsx)(i.li,{children:"Memory above 2GB shows minimal improvement"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"inference-parameters",children:"Inference Parameters"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\n  "prompt": "User: Explain 1-bit quantization benefits\\n\\nAssistant:",\n  "n_predict": 32,\n  "temperature": 0.7,\n  "top_p": 0.9,\n  "top_k": 40,\n  "repeat_penalty": 1.1\n}\n'})}),"\n",(0,s.jsx)(i.p,{children:"These parameters control response generation. The model handles conversational AI, code generation, and text analysis tasks."}),"\n",(0,s.jsx)(i.h2,{id:"monitoring-and-debugging",children:"Monitoring and Debugging"}),"\n",(0,s.jsx)(i.p,{children:"CloudWatch Logs capture everything:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"# View recent logs\naws logs tail /aws/lambda/{your-log-group-name} --follow\n"})}),"\n",(0,s.jsx)(i.p,{children:"Key metrics to monitor:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Cold start duration"}),": 12-13 seconds"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Inference latency"}),": Scales with token count (7s for 10 tokens, 32s for 100 tokens)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Memory utilization"}),": Monitor against allocated limit"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Error rates"}),": Watch for OOM or timeout errors"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"implementation-notes",children:"Implementation Notes"}),"\n",(0,s.jsx)(i.p,{children:"This deployment pattern shows that:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Quantized models"})," can run on Lambda's CPU infrastructure"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Container-based deployment"})," enables ML workloads on Lambda"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Performance scales"})," with token generation requirements"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Cold start times"})," are consistent across memory configurations"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The approach works for applications with sporadic inference needs where 12-13 second cold starts are acceptable."}),"\n",(0,s.jsx)(i.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,s.jsx)(i.p,{children:"BitNet + Lambda deployment has specific trade-offs. 1.58-bit quantization enables serverless deployment but has accuracy limitations compared to full-precision models. This makes it suitable for specific use cases."}),"\n",(0,s.jsx)(i.p,{children:"Areas of development include:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Quantization techniques"}),": Improving model accuracy while maintaining efficiency"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Application matching"}),": Finding use cases where the efficiency/accuracy trade-off works"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Hybrid workflows"}),": Combining lightweight models with full-precision models for different tasks"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Model routing"}),": Automatically selecting appropriate model sizes based on request complexity"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The field is developing, and current approaches may be replaced by better quantization methods or deployment patterns."}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Key takeaway"}),": 1.58-bit quantization enables LLM deployment on Lambda's CPU infrastructure. This approach has specific use cases and performance characteristics, demonstrating one path for serverless AI inference without GPU requirements."]})]})}function m(e={}){const{wrapper:i}={...(0,l.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},6190:e=>{e.exports=JSON.parse('{"permalink":"/blog/deploy-microsoft-bitnet-llm-on-aws-lambda","source":"@site/blog/2025-06-20-bitnet-lambda-serverless-llm.mdx","title":"Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization","description":"Deploy Microsoft BitNet 1.58-bit quantized LLM on AWS Lambda using container-based architecture. Includes performance benchmarks, multi-stage Docker build, and complete deployment guide.","date":"2025-06-20T00:00:00.000Z","tags":[{"inline":true,"label":"aws lambda","permalink":"/blog/tags/aws-lambda"},{"inline":true,"label":"llm","permalink":"/blog/tags/llm"},{"inline":true,"label":"quantization","permalink":"/blog/tags/quantization"},{"inline":true,"label":"serverless","permalink":"/blog/tags/serverless"},{"inline":true,"label":"bitnet","permalink":"/blog/tags/bitnet"},{"inline":true,"label":"machine learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"cost optimization","permalink":"/blog/tags/cost-optimization"}],"readingTime":5.87,"hasTruncateMarker":true,"authors":[{"name":"Manu Mishra","title":"Solutions Architect & Applied Software Engineer","url":"https://github.com/manu-mishra","image_url":"/img/logo.png","imageURL":"/img/logo.png","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"deploy-microsoft-bitnet-llm-on-aws-lambda","title":"Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization","description":"Deploy Microsoft BitNet 1.58-bit quantized LLM on AWS Lambda using container-based architecture. Includes performance benchmarks, multi-stage Docker build, and complete deployment guide.","keywords":["microsoft bitnet","aws lambda llm","serverless ai","1.58-bit quantization","cpu inference","bitnet deployment","lambda container","quantized models","aws ai inference","docker multi-stage build"],"image":"/img/blog/bitnet-on-lambda.png","authors":{"name":"Manu Mishra","title":"Solutions Architect & Applied Software Engineer","url":"https://github.com/manu-mishra","image_url":"/img/logo.png","imageURL":"/img/logo.png"},"tags":["aws lambda","llm","quantization","serverless","bitnet","machine learning","cost optimization"],"date":"2025-06-20T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"Threat Modeling for Autonomous AI - What OWASP Wants You to Know","permalink":"/blog/threat-modeling-autonomous-ai"}}')},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>r});var t=n(6540);const s={},l=t.createContext(s);function a(e){const i=t.useContext(l);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(l.Provider,{value:i},e.children)}},9672:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/bitnet-on-lambda-79b5780ed52f60f02aded51ddc499e0f.png"}}]);