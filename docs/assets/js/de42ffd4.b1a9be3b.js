"use strict";(self.webpackChunkuiv_2=self.webpackChunkuiv_2||[]).push([[8700],{6484:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/embeddings-gemma-lambda-287f228bdf79dafe957097edaad2ce75.png"},7258:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>d});var t=i(7562),s=i(4848),a=i(8453);const r={slug:"embeddings-gemma-on-lambda",title:"Google's EmbeddingGemma on AWS Lambda - A Curiosity-Driven Experiment",description:"Deploy Google's EmbeddingGemma 300M parameter embedding model on AWS Lambda using container-based architecture. Includes performance benchmarks, cold start analysis, and complete deployment guide.",keywords:["google embeddinggemma","aws lambda embeddings","serverless ai","embedding models","lambda container","multilingual embeddings","aws ai inference","docker deployment","embedding inference"],image:"/img/blog/embeddings-gemma-lambda.png",authors:"manu",tags:["aws lambda","embeddings","google gemma","serverless","machine learning","multilingual","cost optimization"],date:new Date("2025-09-21T00:00:00.000Z")},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"1. The idea",id:"1-the-idea",level:2},{value:"2. Why embeddings matter",id:"2-why-embeddings-matter",level:2},{value:"3. The architecture",id:"3-the-architecture",level:2},{value:"4. Amazon Q as co-pilot",id:"4-amazon-q-as-co-pilot",level:2},{value:"5. Performance results",id:"5-performance-results",level:2},{value:"6. The convergence",id:"6-the-convergence",level:2},{value:"7. Reality check",id:"7-reality-check",level:2},{value:"8. Why not production",id:"8-why-not-production",level:2},{value:"9. The real value",id:"9-the-real-value",level:2},{value:"10. Wrapping up",id:"10-wrapping-up",level:2},{value:"References",id:"references",level:2}];function m(e){const n={a:"a",em:"em",h2:"h2",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"EmbeddingGemma on AWS Lambda",src:i(6484).A+"",width:"1024",height:"1024"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.em,{children:["Note: This is a curiosity-driven experiment, not a production recommendation. For real workloads, ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/sagemaker/",children:"Amazon SageMaker"})," is the right choice. This project explores what's possible when you push serverless boundaries."]})}),"\n",(0,s.jsx)(n.h2,{id:"1-the-idea",children:"1. The idea"}),"\n",(0,s.jsxs)(n.p,{children:["After my ",(0,s.jsx)(n.a,{href:"https://community.aws/content/2ynHjrct8JLUEN6mADtT2IYh5bR/microsoft-bitnet-1-58-bit-llms-on-aws-lambda",children:"BitNet Lambda experiment"}),", I kept thinking: what about embeddings? I had text generation working on Lambda, but what about the other half of modern AI applications?"]}),"\n",(0,s.jsx)(n.p,{children:"Google's EmbeddingGemma caught my attention\u2014300M parameters, multilingual, designed for efficiency. Could it work on Lambda? Only one way to find out."}),"\n",(0,s.jsx)(n.p,{children:"So I fired up Amazon Q Developer and started experimenting."}),"\n","\n",(0,s.jsx)(n.h2,{id:"2-why-embeddings-matter",children:"2. Why embeddings matter"}),"\n",(0,s.jsx)(n.p,{children:"Modern AI applications need both text generation and embeddings. RAG systems, semantic search, document processing\u2014they all require this dual capability. I had the generation part working with BitNet, but what about embeddings?"}),"\n",(0,s.jsx)(n.p,{children:"EmbeddingGemma sits in a sweet spot: 300M parameters (~1.2GB) with multilingual support for 100+ languages. Unlike massive text generation models, embedding models are:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Predictable"}),": Fixed output dimensions (768 floats)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient"}),": Single forward pass, no autoregressive generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compact"}),": Smaller memory footprint than multi-billion parameter LLMs"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'That efficiency profile makes "Lambda + Embeddings" the perfect complement to my BitNet experiment\u2014completing the serverless AI toolkit.'}),"\n",(0,s.jsx)(n.h2,{id:"3-the-architecture",children:"3. The architecture"}),"\n",(0,s.jsx)(n.p,{children:"The architecture stayed simple: API Gateway triggers a Lambda function with 2GB memory. Inside lives a container image with transformers, sentence-transformers, and the complete EmbeddingGemma model. Lambda processes the text and returns a 768-dimensional vector."}),"\n",(0,s.jsx)(n.p,{children:"Thanks to Amazon Q's help, I optimized the container to embed the entire model (~1.2GB) while keeping cold starts reasonable. No external model loading, no S3 downloads\u2014everything lives in the container."}),"\n",(0,s.jsx)(n.h2,{id:"4-amazon-q-as-co-pilot",children:"4. Amazon Q as co-pilot"}),"\n",(0,s.jsx)(n.p,{children:"Amazon Q CLI didn't just automate\u2014it elevated the entire workflow. When I asked it to create a Dockerfile that could efficiently package transformers and the EmbeddingGemma model, it didn't just generate code\u2014it explained why sentence-transformers was the right choice over raw transformers."}),"\n",(0,s.jsx)(n.p,{children:"For infrastructure, Q generated a clean CDK stack targeting Lambda with ARM64 architecture and 2GB memory. When builds failed or performance lagged, Q helped interpret CloudWatch logs and suggested memory optimizations."}),"\n",(0,s.jsx)(n.p,{children:"Having Claude Sonnet inside Q made this feel like pair programming with someone who actually understood ML deployment patterns."}),"\n",(0,s.jsx)(n.h2,{id:"5-performance-results",children:"5. Performance results"}),"\n",(0,s.jsx)(n.p,{children:"The numbers tell the story:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cold start"}),": 12 seconds (not bad for a 300M model)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Warm inference"}),": 0.12-0.33 seconds per embedding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost"}),": ~$0.001 per request for short texts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory sweet spot"}),": 2GB (4GB+ shows no improvement)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Combined with BitNet for text generation, this setup creates a complete serverless AI toolkit that shines for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RAG systems"}),": BitNet for generation, EmbeddingGemma for retrieval"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic search"}),": Document vectorization and similarity matching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prototype APIs"}),": Quick AI services for testing and experimentation"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"It struggles with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch processing"}),": Linear scaling kills economics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time chat"}),": 12-second cold starts hurt UX"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High throughput"}),": Concurrent requests need full memory allocation"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"6-the-convergence",children:"6. The convergence"}),"\n",(0,s.jsx)(n.p,{children:'Two trends are colliding: models are getting more efficient while serverless platforms evolve. EmbeddingGemma represents the "efficient model" side\u2014compact, purpose-built, and CPU-friendly.'}),"\n",(0,s.jsx)(n.p,{children:"On the platform side, we're seeing serverless runtimes optimize for AI workloads. When these trends meet\u2014lightweight models and AI-aware serverless compute\u2014deploying embeddings will be as casual as deploying a REST API."}),"\n",(0,s.jsx)(n.h2,{id:"7-reality-check",children:"7. Reality check"}),"\n",(0,s.jsx)(n.p,{children:"Let's be honest about the numbers:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Text length scaling"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"10 characters: 0.32s"}),"\n",(0,s.jsx)(n.li,{children:"99 characters: 1.05s"}),"\n",(0,s.jsx)(n.li,{children:"588 characters: 4.06s"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Memory efficiency"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"2GB: Optimal performance"}),"\n",(0,s.jsx)(n.li,{children:"4GB+: No improvement, 2x cost"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Infrastructure overhead"}),": 0.7-0.8 seconds of the total latency is network + AWS API processing, not model inference."]}),"\n",(0,s.jsx)(n.h2,{id:"8-why-not-production",children:"8. Why not production"}),"\n",(0,s.jsx)(n.p,{children:"While technically successful, several factors make this unsuitable for serious workloads:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Economics don't scale"}),": 2GB memory allocation for sporadic requests burns money. SageMaker's auto-scaling and GPU optimization provide better cost-per-embedding at volume."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cold start penalty"}),": 12-second delays kill user experience for interactive applications."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Better alternatives exist"}),": Purpose-built ML infrastructure (SageMaker, ECS with GPUs) offers superior performance and economics for production embedding workloads."]}),"\n",(0,s.jsx)(n.h2,{id:"9-the-real-value",children:"9. The real value"}),"\n",(0,s.jsx)(n.p,{children:"This experiment's worth isn't in production deployment\u2014it's about curiosity. What happens when you run Google's EmbeddingGemma in AWS Lambda? Can a 300M parameter embedding model really work in serverless compute? How does it perform?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Curiosity-driven insights"}),": How EmbeddingGemma behaves in Lambda's constraints, memory optimization patterns for embedding models, and container packaging strategies you can only discover by trying."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Learning by doing"}),": Understanding where EmbeddingGemma's efficiency meets Lambda's limitations, and where the serverless tax becomes prohibitive for ML workloads."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Future signals"}),": As embedding models get more efficient and Lambda evolves, today's experiments with EmbeddingGemma become tomorrow's possibilities."]}),"\n",(0,s.jsx)(n.h2,{id:"10-wrapping-up",children:"10. Wrapping up"}),"\n",(0,s.jsx)(n.p,{children:"Running Google's EmbeddingGemma on AWS Lambda isn't about beating dedicated ML infrastructure\u2014it's about curiosity. What if you could deploy Google's embedding model as easily as a REST API? What would EmbeddingGemma's performance look like in Lambda? How much would it cost?"}),"\n",(0,s.jsx)(n.p,{children:"The question was simple: \"What about embeddings on Lambda?\" Sometimes the best experiments come from pure curiosity about what's possible when you combine Google's efficient embedding model with AWS's serverless compute."}),"\n",(0,s.jsxs)(n.p,{children:["The complete EmbeddingGemma-on-Lambda implementation is on ",(0,s.jsx)(n.a,{href:"https://github.com/manu-mishra/embeddings-gemma-on-lambda",children:"GitHub"}),". Clone it, try it, break it. See how far you can push EmbeddingGemma in Lambda before reaching for SageMaker."]}),"\n",(0,s.jsx)(n.p,{children:"And if you're curious about other Google models on AWS Lambda, let's chat about what other \"impossible\" combinations might be worth trying."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"This project was built using vibe coding techniques with Amazon Q Developer, demonstrating how AI-assisted development can accelerate experimentation while maintaining architectural rigor."})}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/google/embeddinggemma-300m",children:"Google EmbeddingGemma Model"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://ai.google.dev/gemma/docs/embeddinggemma",children:"EmbeddingGemma Overview"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/manu-mishra/embeddings-gemma-on-lambda",children:"Project Repository"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/manu-mishra/embeddings-gemma-on-lambda/blob/main/docs/PERFORMANCE.md",children:"Performance Benchmarks"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://community.aws/content/2ynHjrct8JLUEN6mADtT2IYh5bR/microsoft-bitnet-1-58-bit-llms-on-aws-lambda",children:"Previous BitNet Lambda Article"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.linkedin.com/pulse/vibe-coding-vegas-158-bit-llm-aws-lambda-manu-mishra-s0joc/",children:"Vibe Coding in Vegas LinkedIn Article"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://aws.amazon.com/q/developer/",children:"Amazon Q Developer"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://aws.amazon.com/sagemaker/",children:"Amazon SageMaker"})}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},7562:e=>{e.exports=JSON.parse('{"permalink":"/blog/embeddings-gemma-on-lambda","source":"@site/blog/2025-09-21-embeddings-gemma-on-lambda.mdx","title":"Google\'s EmbeddingGemma on AWS Lambda - A Curiosity-Driven Experiment","description":"Deploy Google\'s EmbeddingGemma 300M parameter embedding model on AWS Lambda using container-based architecture. Includes performance benchmarks, cold start analysis, and complete deployment guide.","date":"2025-09-21T00:00:00.000Z","tags":[{"inline":true,"label":"aws lambda","permalink":"/blog/tags/aws-lambda"},{"inline":true,"label":"embeddings","permalink":"/blog/tags/embeddings"},{"inline":true,"label":"google gemma","permalink":"/blog/tags/google-gemma"},{"inline":true,"label":"serverless","permalink":"/blog/tags/serverless"},{"inline":true,"label":"machine learning","permalink":"/blog/tags/machine-learning"},{"inline":true,"label":"multilingual","permalink":"/blog/tags/multilingual"},{"inline":true,"label":"cost optimization","permalink":"/blog/tags/cost-optimization"}],"readingTime":5.15,"hasTruncateMarker":true,"authors":[{"name":"Manu Mishra","title":"Distinguished Solutions Architect, Author & Researcher in AI & Cloud","url":"https://github.com/manu-mishra","imageURL":"/img/logo.png","key":"manu","page":null}],"frontMatter":{"slug":"embeddings-gemma-on-lambda","title":"Google\'s EmbeddingGemma on AWS Lambda - A Curiosity-Driven Experiment","description":"Deploy Google\'s EmbeddingGemma 300M parameter embedding model on AWS Lambda using container-based architecture. Includes performance benchmarks, cold start analysis, and complete deployment guide.","keywords":["google embeddinggemma","aws lambda embeddings","serverless ai","embedding models","lambda container","multilingual embeddings","aws ai inference","docker deployment","embedding inference"],"image":"/img/blog/embeddings-gemma-lambda.png","authors":"manu","tags":["aws lambda","embeddings","google gemma","serverless","machine learning","multilingual","cost optimization"],"date":"2025-09-21T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"AWS Re:Invent 2025, Reinvented \u2014 Powered by MCP","permalink":"/blog/aws-reinvent-2025-reinvented-powered-by-mcp"},"nextItem":{"title":"Running 1.58-bit LLMs on AWS Lambda with BitNet","permalink":"/blog/deploy-microsoft-bitnet-llm-on-aws-lambda"}}')},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);