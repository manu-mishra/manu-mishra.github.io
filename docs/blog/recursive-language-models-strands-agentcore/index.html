<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.0">
<title data-rh="true">Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore | Manu Mishra</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" name="author" content="Manu Mishra"><meta data-rh="true" name="msvalidate.01" content="8D78E08118D465445E0BA072DB19A547"><meta data-rh="true" property="og:url" content="https://manumishra.com"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="twitter:title" content="Manu Mishra - Distinguished Solutions Architect, Author &amp; Researcher in AI &amp; Cloud"><meta data-rh="true" name="twitter:description" content="Discover cutting-edge insights, published research, and proven expertise in AI, machine learning, and cloud-native architectures from an industry-leading solutions architect, technical author, and researcher."><meta data-rh="true" property="og:title" content="Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore | Manu Mishra"><meta data-rh="true" name="description" content="Learn how to implement Recursive Language Models (RLMs) from MIT CSAIL research using Strands Agents and Amazon Bedrock AgentCore. Scale to inputs 100x beyond context windows with minimal code."><meta data-rh="true" property="og:description" content="Learn how to implement Recursive Language Models (RLMs) from MIT CSAIL research using Strands Agents and Amazon Bedrock AgentCore. Scale to inputs 100x beyond context windows with minimal code."><meta data-rh="true" name="keywords" content="recursive language models,strands agents,amazon bedrock agentcore,long context,rlm,ai agents,aws bedrock,code generation,serverless ai,context windows"><meta data-rh="true" property="og:image" content="https://manumishra.com/img/blog/rlm-on-aws.png"><meta data-rh="true" name="twitter:image" content="https://manumishra.com/img/blog/rlm-on-aws.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2026-01-19T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/manu-mishra"><meta data-rh="true" property="article:tag" content="amazon bedrock,agentcore,strands agents,recursive language models,generative ai,machine learning,aws,long context"><link data-rh="true" rel="icon" href="/img/manu-mishra-fav-icon.png"><link data-rh="true" rel="canonical" href="https://manumishra.com/blog/recursive-language-models-strands-agentcore"><link data-rh="true" rel="alternate" href="https://manumishra.com/blog/recursive-language-models-strands-agentcore" hreflang="en"><link data-rh="true" rel="alternate" href="https://manumishra.com/blog/recursive-language-models-strands-agentcore" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://manumishra.com/blog/recursive-language-models-strands-agentcore","mainEntityOfPage":"https://manumishra.com/blog/recursive-language-models-strands-agentcore","url":"https://manumishra.com/blog/recursive-language-models-strands-agentcore","headline":"Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore","name":"Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore","description":"Learn how to implement Recursive Language Models (RLMs) from MIT CSAIL research using Strands Agents and Amazon Bedrock AgentCore. Scale to inputs 100x beyond context windows with minimal code.","datePublished":"2026-01-19T00:00:00.000Z","author":{"@type":"Person","name":"Manu Mishra","description":"Solutions Architect & Applied Software Engineer","url":"https://github.com/manu-mishra","image":"/img/logo.png"},"image":{"@type":"ImageObject","@id":"https://manumishra.com/img/blog/rlm-on-aws.png","url":"https://manumishra.com/img/blog/rlm-on-aws.png","contentUrl":"https://manumishra.com/img/blog/rlm-on-aws.png","caption":"title image for the blog post: Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore"},"keywords":["recursive language models","strands agents","amazon bedrock agentcore","long context","rlm","ai agents","aws bedrock","code generation","serverless ai","context windows"],"isPartOf":{"@type":"Blog","@id":"https://manumishra.com/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Manu Mishra RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Manu Mishra Atom Feed">




<script src="/scripts/apply-theme.js"></script><link rel="stylesheet" href="/assets/css/styles.f32cad2d.css">
<script src="/assets/js/runtime~main.e9b712ad.js" defer="defer"></script>
<script src="/assets/js/main.2f123bbb.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Manu Mishra Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.png" alt="Manu Mishra Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Manu Mishra</b></a><a class="navbar__item navbar__link" href="/experience">Experience</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/about">About</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/manu-mishra" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://linkedin.com/in/manu-mishra" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All Blog Posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2026</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/universal-image-mcp-multi-provider-image-generation">Universal Image MCP - One Server, Three AI Image Providers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/s3-tables-streaming-analytics">Building Serverless Real-Time Streaming Analytics with Amazon S3 Tables and Kinesis Data Firehose</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/recursive-language-models-strands-agentcore">Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/aws-reinvent-2025-reinvented-powered-by-mcp">AWS Re:Invent 2025, Reinvented — Powered by MCP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/embeddings-gemma-on-lambda">Google&#x27;s EmbeddingGemma on AWS Lambda - A Curiosity-Driven Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/deploy-microsoft-bitnet-llm-on-aws-lambda">Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/threat-modeling-autonomous-ai">Threat Modeling for Autonomous AI - What OWASP Wants You to Know</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/data-governance-playbook">From Data Chaos to Data Confidence - A Pragmatic Playbook for Self‑Sustaining Data Governance</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/digital-standstill-theory-constraints">Tackling Digital Standstill Through the Theory of Constraints - A New Lens on Technical Debt</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/three-cs-of-coe">The Three &quot;C&quot;s of COE - From Center to Centering to Culture of Excellence</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/priming-business-flywheel-genai">Priming Business Flywheel with Gen-AI</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/unified-systems">Unified Systems - The Tech Trend You Never Knew You Needed</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/api-first-ai-era">Rethinking API-First - Unveiling Its True Power in the AI Era</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-technical-debt">The Future with Large Language Models - A Technical Debt Worth Taking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/software-craftsmanship">The Craftsmanship of Software Engineering - Why We Should Objectify Tools, Not Debates</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/software-engineer-vs-developer">Software Engineer vs. Developer through the Lens of Socratic Questioning</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/digital-inertia">KTLO Can Lead to Digital Inertia and Hinder Digital Transformation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/dont-keep-lights-on">Don&#x27;t Keep The Lights On</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore</h1><div class="container_mt6G margin-vert--md"><time datetime="2026-01-19T00:00:00.000Z">January 19, 2026</time> · <!-- -->12 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/manu-mishra" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="/img/logo.png" alt="Manu Mishra"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/manu-mishra" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Manu Mishra</span></a></div><small class="authorTitle_nd0D" title="Solutions Architect &amp; Applied Software Engineer">Solutions Architect &amp; Applied Software Engineer</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p><img decoding="async" loading="lazy" alt="RLM on AWS Architecture" src="/assets/images/rlm-on-aws-e13ef244388c42150924e6a3424bf966.png" width="1536" height="1024" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>Modern large language models face a fundamental limitation: context windows. While frontier models now reach 1 million tokens (Nova Premier, Claude Sonnet 4.5), workloads analyzing entire codebases, document collections, or multi-hour conversations can easily exceed 10 million tokens—far beyond any single model&#x27;s capacity.</p>
<p>This post demonstrates <strong>Recursive Language Models (RLMs)</strong>, an inference strategy from <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">MIT CSAIL research</a> that enables scaling to inputs far beyond context windows. What makes this implementation special: <strong>Strands Agents</strong> and <strong>Amazon Bedrock AgentCore</strong> reduce what could be weeks of glue code and deployment work to just a few hours of development.</p>
<!-- -->
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-recursive-language-models">What Are Recursive Language Models?<a href="#what-are-recursive-language-models" class="hash-link" aria-label="Direct link to What Are Recursive Language Models?" title="Direct link to What Are Recursive Language Models?">​</a></h2>
<p>Traditional approaches to long contexts use summarization or RAG. These struggle with information-dense tasks requiring examination of the full input.</p>
<p><strong>Recursive Language Models</strong> are an inference pattern for handling inputs far larger than any model context window. Instead of stuffing the entire dataset into the prompt, the full context lives outside the model as an external context buffer (a Python variable). The model then generates code to search, filter, and chunk the context, and uses recursive sub-calls on only the relevant pieces. Finally, results are aggregated through deterministic code rather than attention over the full input.</p>
<p>This approach is especially effective for tasks like &quot;find all occurrences,&quot; &quot;count all items,&quot; or &quot;extract all endpoints,&quot; where missing even one chunk causes incorrect results.</p>
<p><strong>In our implementation, recursion depth is 1:</strong> the root model uses sub-calls to smaller models, and aggregation happens in code (not through nested RLMs).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rlm-in-action-minimal-pattern">RLM in Action: Minimal Pattern<a href="#rlm-in-action-minimal-pattern" class="hash-link" aria-label="Direct link to RLM in Action: Minimal Pattern" title="Direct link to RLM in Action: Minimal Pattern">​</a></h3>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Context lives outside the model as a Python variable</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">context </span><span class="token operator">=</span><span class="token plain"> load_large_codebase</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># can exceed model context windows by 100x+</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Model writes code to probe and filter</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">api_files </span><span class="token operator">=</span><span class="token plain"> search</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">context</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> pattern</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">r&quot;@app\.route&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Recursive sub-calls on filtered chunks</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">endpoints </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">file</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> api_files</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    result </span><span class="token operator">=</span><span class="token plain"> llm_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f&quot;Extract endpoint info from:\n</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation builtin" style="color:rgb(189, 147, 249)">file</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    endpoints</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">append</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">result</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Deterministic aggregation</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> aggregate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">endpoints</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rlm-vs-rag-when-to-use-each">RLM vs RAG: When to Use Each<a href="#rlm-vs-rag-when-to-use-each" class="hash-link" aria-label="Direct link to RLM vs RAG: When to Use Each" title="Direct link to RLM vs RAG: When to Use Each">​</a></h3>
<table><thead><tr><th>Scenario</th><th>RAG</th><th>RLM</th></tr></thead><tbody><tr><td><strong>Known query upfront</strong></td><td>✅ Efficient retrieval</td><td>⚠️ Overkill</td></tr><tr><td><strong>&quot;Find all&quot; / &quot;Count all&quot;</strong></td><td>❌ May miss chunks</td><td>✅ Exhaustive coverage</td></tr><tr><td><strong>Relevance-based QA</strong></td><td>✅ Fast, targeted</td><td>⚠️ Slower</td></tr><tr><td><strong>Multi-step reasoning over full context</strong></td><td>❌ Limited by retrieval</td><td>✅ Code-driven exploration</td></tr></tbody></table>
<p><strong>RAG struggles when:</strong> you don&#x27;t know what to retrieve upfront, when correctness depends on coverage (not relevance), or when you need &quot;verify-all&quot; guarantees.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="research-results">Research Results<a href="#research-results" class="hash-link" aria-label="Direct link to Research Results" title="Direct link to Research Results">​</a></h3>
<p>The <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">MIT paper</a> demonstrates RLMs:</p>
<ul>
<li>Successfully handle inputs <strong>up to 100x beyond model context windows</strong></li>
<li>Outperform base LLMs and common long-context scaffolds on long-context retrieval benchmarks</li>
<li>Scale costs with task complexity, not input size</li>
</ul>
<p><strong>Note</strong>: Performance examples in this post are illustrative. Actual results vary by model, task complexity, and context structure.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-strands--agentcore-makes-this-easy">Why Strands + AgentCore Makes This Easy<a href="#why-strands--agentcore-makes-this-easy" class="hash-link" aria-label="Direct link to Why Strands + AgentCore Makes This Easy" title="Direct link to Why Strands + AgentCore Makes This Easy">​</a></h2>
<p>Building RLMs from scratch requires weeks of work: agent loop orchestration, REPL sandboxing, model invocation logic, deployment infrastructure, scaling configuration, and observability. <strong>Strands Agents</strong> and <strong>Amazon Bedrock AgentCore</strong> eliminate this complexity.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-overview">Architecture Overview<a href="#architecture-overview" class="hash-link" aria-label="Direct link to Architecture Overview" title="Direct link to Architecture Overview">​</a></h3>
<p><img decoding="async" loading="lazy" alt="RLM on AWS Architecture" src="/assets/images/rlm-on-aws-e13ef244388c42150924e6a3424bf966.png" width="1536" height="1024" class="img_ev3q">
<em>RLM implementation using Strands Agents and Amazon Bedrock AgentCore</em></p>
<p>The architecture shows the complete flow from user query to final answer:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="strands-agents-orchestration-made-simple">Strands Agents: Orchestration Made Simple<a href="#strands-agents-orchestration-made-simple" class="hash-link" aria-label="Direct link to Strands Agents: Orchestration Made Simple" title="Direct link to Strands Agents: Orchestration Made Simple">​</a></h3>
<p><a href="https://github.com/awslabs/strands-agents" target="_blank" rel="noopener noreferrer">Strands Agents</a> is an open-source Python SDK for building AI agents. It provides:</p>
<ul>
<li><strong>Agent loop orchestration</strong> - Handles iterative LLM → code execution → LLM cycles automatically</li>
<li><strong>Tool calling</strong> - Built-in Python REPL tool with output management</li>
<li><strong>Model integration</strong> - Native <a href="https://aws.amazon.com/bedrock/" target="_blank" rel="noopener noreferrer">Amazon Bedrock</a> support with streaming</li>
<li><strong>State management</strong> - Tracks conversation history and execution context</li>
</ul>
<p><strong>Impact:</strong> What would take weeks of infrastructure code becomes ~200 lines.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="amazon-bedrock-agentcore-deployment-without-the-pain">Amazon Bedrock AgentCore: Deployment Without the Pain<a href="#amazon-bedrock-agentcore-deployment-without-the-pain" class="hash-link" aria-label="Direct link to Amazon Bedrock AgentCore: Deployment Without the Pain" title="Direct link to Amazon Bedrock AgentCore: Deployment Without the Pain">​</a></h3>
<p><a href="https://aws.amazon.com/bedrock/agentcore/" target="_blank" rel="noopener noreferrer">Amazon Bedrock AgentCore</a> provides serverless runtime for AI agents:</p>
<ul>
<li><strong>Long-running execution</strong> - 15-minute idle timeout when not processing (not a max runtime limit)</li>
<li><strong>Automatic scaling</strong> - Handles concurrent requests without capacity planning</li>
<li><strong>Session isolation</strong> - Each invocation gets isolated environment</li>
<li><strong>ARM64 optimization</strong> - <a href="https://aws.amazon.com/ec2/graviton/" target="_blank" rel="noopener noreferrer">AWS Graviton</a> processors for cost efficiency</li>
<li><strong>Built-in observability</strong> - <a href="https://aws.amazon.com/cloudwatch/" target="_blank" rel="noopener noreferrer">Amazon CloudWatch</a> logs and metrics</li>
</ul>
<p><strong>Impact:</strong> Deployment is a standard CDK workflow, and invocation happens via the AgentCore Runtime API. RLM experiments that take 5+ minutes run without timeout issues. Safety limits (max sub-calls, output buffer) prevent runaway execution.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cost-intuition">Cost Intuition<a href="#cost-intuition" class="hash-link" aria-label="Direct link to Cost Intuition" title="Direct link to Cost Intuition">​</a></h3>
<p>In practice, the root model spends tokens on planning and code generation, while sub-calls only process filtered slices of the context. This makes total token usage closer to &quot;work performed&quot; rather than &quot;data size&quot;—often landing closer to planning plus the slices you inspect (for example, tens to a few hundred thousand tokens for many workloads, depending on how much content is actually analyzed).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="amazon-bedrock-model-choice">Amazon Bedrock: Model Choice<a href="#amazon-bedrock-model-choice" class="hash-link" aria-label="Direct link to Amazon Bedrock: Model Choice" title="Direct link to Amazon Bedrock: Model Choice">​</a></h3>
<p><a href="https://aws.amazon.com/bedrock/" target="_blank" rel="noopener noreferrer">Amazon Bedrock</a> is a fully managed service that offers a choice of high-performing foundation models from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, OpenAI, and Amazon through a single API. This model choice is critical for RLM implementations—different tasks benefit from different model strengths in reasoning, code generation, and cost efficiency.</p>
<p>Following the MIT paper&#x27;s two-model approach, we use a root model for orchestration and a smaller model for sub-calls:</p>
<p><strong>Supported Root Models:</strong></p>
<ul>
<li><strong>Amazon Nova Pro</strong> - 300K context, strong reasoning</li>
<li><strong>Claude 4.5 Sonnet</strong> - 200K context (1M beta), excellent code generation</li>
<li><strong>Claude 4.5 Opus</strong> - 200K context, frontier performance</li>
<li><strong>GPT-OSS 120B</strong> - 128K context, open-source option</li>
</ul>
<p><strong>Supported Sub-Call Models:</strong></p>
<ul>
<li><strong>Amazon Nova Micro</strong> - 128K context, optimized for speed</li>
<li><strong>Claude 4.5 Haiku</strong> - Fast, cost-efficient</li>
<li><strong>Amazon Nova Lite</strong> - Balanced performance</li>
</ul>
<p>This two-tier approach balances capability and efficiency: a powerful model for strategy, a fast model for execution. Our implementation is model-agnostic—swap models via configuration without code changes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-it-works">How It Works<a href="#how-it-works" class="hash-link" aria-label="Direct link to How It Works" title="Direct link to How It Works">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="repl-environment">REPL Environment<a href="#repl-environment" class="hash-link" aria-label="Direct link to REPL Environment" title="Direct link to REPL Environment">​</a></h3>
<p>The core innovation is loading the entire input context as an external context buffer (Python variable) accessible to the LLM. The environment provides:</p>
<ul>
<li><code>context</code> variable containing the full input</li>
<li><code>llm_query(prompt)</code> function for recursive sub-LM calls</li>
<li>Standard Python libraries for text processing (regex, string manipulation)</li>
<li>Isolated execution preventing access to system resources</li>
</ul>
<p>The LLM writes Python code to interact with this environment, executing operations like regex searches, chunking, and filtering without loading the entire context into its neural network.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="system-prompt-design">System Prompt Design<a href="#system-prompt-design" class="hash-link" aria-label="Direct link to System Prompt Design" title="Direct link to System Prompt Design">​</a></h3>
<p>Based on <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">MIT paper Appendix D</a>, the system prompt is minimal and example-driven:</p>
<p><strong>Core Instructions:</strong></p>
<ul>
<li>Context is available as a Python variable in REPL, not in the prompt</li>
<li>Use code to probe, filter, and chunk the context</li>
<li>Make recursive <code>llm_query()</code> calls on filtered chunks</li>
<li>Return final answer directly (no special format required)</li>
</ul>
<p><strong>Example Strategies:</strong></p>
<ul>
<li>Regex filtering: Search for keywords without reading entire context</li>
<li>Uniform chunking: Split into equal-sized pieces for parallel processing</li>
<li>Semantic chunking: Use document structure (headers, file boundaries)</li>
</ul>
<p><strong>Example Pattern:</strong></p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Search in chunks</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">chunk_size </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">50000</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># ~12.5K tokens</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> i </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">len</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">context</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> chunk_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    chunk </span><span class="token operator">=</span><span class="token plain"> context</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">i</span><span class="token operator">+</span><span class="token plain">chunk_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    result </span><span class="token operator">=</span><span class="token plain"> llm_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f&quot;Find the magic number in: </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">chunk</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">result</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<p>The prompt avoids prescriptive &quot;ALWAYS/NEVER&quot; rules, letting models develop their own strategies.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-loop">Agent Loop<a href="#agent-loop" class="hash-link" aria-label="Direct link to Agent Loop" title="Direct link to Agent Loop">​</a></h3>
<p>The Strands Agent orchestrates iterative REPL interaction:</p>
<ol>
<li>Root LM receives query and context metadata (length, structure)</li>
<li>Root LM generates Python code to execute in REPL</li>
<li>Code executes, output returned (truncated to prevent overflow)</li>
<li>Root LM sees output, decides to continue or return answer</li>
<li>Loop continues until answer provided or timeout</li>
</ol>
<p>This iterative process allows the model to refine its approach based on execution feedback.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="asynchronous-processing">Asynchronous Processing<a href="#asynchronous-processing" class="hash-link" aria-label="Direct link to Asynchronous Processing" title="Direct link to Asynchronous Processing">​</a></h3>
<p>All experiments run asynchronously by default:</p>
<ul>
<li>Immediate response with task ID and session ID</li>
<li>Client polls for results (no timeout issues)</li>
<li>Tasks can run for minutes or hours</li>
<li>Results automatically saved to <a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener noreferrer">Amazon S3</a> with full metadata</li>
<li>15-minute idle timeout only applies when NOT processing</li>
</ul>
<p>This enables long-running RLM tasks without connection timeouts.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="recursive-sub-calls">Recursive Sub-Calls<a href="#recursive-sub-calls" class="hash-link" aria-label="Direct link to Recursive Sub-Calls" title="Direct link to Recursive Sub-Calls">​</a></h3>
<p>The <code>llm_query()</code> function enables decomposition:</p>
<ul>
<li>Sub-calls use smaller model (Amazon Nova Micro) with smaller context</li>
<li>Each sub-call is independent (no shared state except via code variables)</li>
<li>Sub-calls can be batched in code (process multiple chunks in loop)</li>
<li>Results stored in Python variables for aggregation</li>
</ul>
<p>The paper uses max recursion depth of 1 (sub-calls are base LLMs, not RLMs).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deployment">Deployment<a href="#deployment" class="hash-link" aria-label="Direct link to Deployment" title="Direct link to Deployment">​</a></h3>
<p><a href="https://aws.amazon.com/cdk/" target="_blank" rel="noopener noreferrer">AWS CDK</a> deploys the RLM agent to AgentCore Runtime:</p>
<ul>
<li>Build ARM64 Docker image with Strands Agent code</li>
<li>Create IAM role with Bedrock, S3, and CloudWatch permissions</li>
<li>Deploy datasets (TREC, BrowseComp+, CodeQA) to S3 bucket</li>
<li>Configure CloudWatch log group for traces</li>
<li>Deploy AgentCore Runtime with container image</li>
</ul>
<p>AgentCore handles scaling, versioning, and observability automatically. Datasets are uploaded during <code>cdk deploy</code> and downloaded by the runtime on-demand.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-analyzing-large-codebases">Example: Analyzing Large Codebases<a href="#example-analyzing-large-codebases" class="hash-link" aria-label="Direct link to Example: Analyzing Large Codebases" title="Direct link to Example: Analyzing Large Codebases">​</a></h2>
<p><strong>Task:</strong> Identify all API endpoints and their authentication requirements in a codebase with hundreds of files.</p>
<p><strong>Traditional Approach Limitations:</strong></p>
<ul>
<li>Direct LLM call: May exceed context window</li>
<li>RAG: May miss endpoints, requires good chunking strategy upfront</li>
<li>Manual analysis: Time-consuming, error-prone</li>
</ul>
<p><strong>RLM Approach:</strong></p>
<ol>
<li><strong>Probe:</strong> Check total size, identify file boundaries</li>
<li><strong>Filter:</strong> Use regex to find files with route decorators (<code>@app.route</code>, <code>@api.route</code>)</li>
<li><strong>Analyze:</strong> Make sub-LM calls on each API file to extract endpoint details</li>
<li><strong>Aggregate:</strong> Combine results into structured summary</li>
</ol>
<p><strong>Outcome:</strong></p>
<ul>
<li>Processes codebases beyond single model context limits</li>
<li>Identifies all API endpoints systematically</li>
<li>Extracts authentication requirements for each</li>
<li>Completes in minutes with minimal sub-calls</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmarks-and-findings">Benchmarks and Findings<a href="#benchmarks-and-findings" class="hash-link" aria-label="Direct link to Benchmarks and Findings" title="Direct link to Benchmarks and Findings">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-on-long-context-tasks">Performance on Long-Context Tasks<a href="#performance-on-long-context-tasks" class="hash-link" aria-label="Direct link to Performance on Long-Context Tasks" title="Direct link to Performance on Long-Context Tasks">​</a></h3>
<p>We evaluated RLM on benchmarks using real-world datasets (TREC, Tevatron BrowseComp+, LongBench-v2) deployed to S3.</p>
<table><thead><tr><th>Test</th><th>Context Size</th><th>Task</th><th>Dataset Source</th></tr></thead><tbody><tr><td>OOLONG</td><td>5,452 TREC entries</td><td>Count label frequencies</td><td>TREC coarse dataset</td></tr><tr><td>OOLONG-Pairs</td><td>5,452 TREC entries</td><td>Find HUM/LOC pairs</td><td>TREC coarse dataset</td></tr><tr><td>BrowseComp-1K</td><td>1,000 documents</td><td>Answer research query</td><td>Tevatron BrowseComp+</td></tr><tr><td>CodeQA</td><td>Multi-file repos</td><td>Multi-choice reasoning</td><td>LongBench-v2 Code</td></tr></tbody></table>
<p><strong>Key Findings:</strong></p>
<p><strong>Accuracy for Retrieval Tasks:</strong> Code-based counting and searching reduces hallucinations for tasks requiring exact matches. Direct model calls often produce inconsistent results on the same input.</p>
<p><strong>Real Datasets:</strong> All benchmarks use production datasets (~216MB total) deployed to S3 and loaded at runtime, simulating real-world information retrieval scenarios.</p>
<p><strong>Async Execution:</strong> All tests run asynchronously with results saved to S3, enabling long-running tasks without timeout issues.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="emergent-behaviors">Emergent Behaviors<a href="#emergent-behaviors" class="hash-link" aria-label="Direct link to Emergent Behaviors" title="Direct link to Emergent Behaviors">​</a></h3>
<p>RLM trajectories showed interesting patterns:</p>
<ul>
<li><strong>Regex filtering:</strong> Models searched for keywords without reading entire context</li>
<li><strong>Adaptive chunking:</strong> Adjusted chunk sizes based on task complexity</li>
<li><strong>Answer verification:</strong> Made additional sub-calls to validate results</li>
<li><strong>Strategic decisions:</strong> Chose between &quot;process all&quot; vs &quot;filter then process&quot; strategies</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="limitations">Limitations<a href="#limitations" class="hash-link" aria-label="Direct link to Limitations" title="Direct link to Limitations">​</a></h3>
<ol>
<li><strong>Async-first design:</strong> All experiments run asynchronously; synchronous mode available but not recommended for long tasks</li>
<li><strong>Model-specific behavior:</strong> Different models show varying chunking strategies and sub-call patterns</li>
<li><strong>Sub-call limits:</strong> Max 50 sub-calls prevents runaway execution but may limit very complex tasks</li>
<li><strong>Debugging complexity:</strong> Full trajectory examination needed via CloudWatch logs</li>
<li><strong>Dataset size:</strong> Real datasets (TREC, BrowseComp+, CodeQA) are large files (~216MB total) deployed to S3</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="when-to-use-rlms">When to Use RLMs<a href="#when-to-use-rlms" class="hash-link" aria-label="Direct link to When to Use RLMs" title="Direct link to When to Use RLMs">​</a></h2>
<p><strong>Ideal for:</strong></p>
<ul>
<li>Information-dense aggregation across entire datasets</li>
<li>Large codebase analysis (patterns, security, dependencies)</li>
<li>Multi-document reasoning requiring synthesis</li>
<li>Contexts beyond model limits</li>
</ul>
<p><strong>Use alternatives for:</strong></p>
<ul>
<li>Single-document QA within context window (direct calls)</li>
<li>Sparse retrieval (RAG more efficient)</li>
<li>Real-time requirements (RLM takes seconds to minutes)</li>
<li>Simple extraction (regex/parsing sufficient)</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="observability">Observability<a href="#observability" class="hash-link" aria-label="Direct link to Observability" title="Direct link to Observability">​</a></h2>
<p>AgentCore provides CloudWatch integration for monitoring RLM trajectories:</p>
<p><strong>Enable Transaction Search:</strong>
One-time setup to send X-Ray traces to CloudWatch Logs for GenAI Observability dashboard.</p>
<p><strong>View Traces:</strong>
CloudWatch → GenAI Observability shows:</p>
<ul>
<li>Complete RLM trajectories (each REPL iteration)</li>
<li>Sub-LM call patterns (count, timing, token usage)</li>
<li>Token efficiency (processed vs. context size)</li>
<li>Execution time breakdown</li>
</ul>
<p><strong>Key Metrics:</strong></p>
<ul>
<li>Trajectory length: Number of REPL iterations (target: &lt;20)</li>
<li>Sub-call count: Recursive invocations (target: &lt;30)</li>
<li>Token efficiency: % of context actually processed</li>
<li>Success rate: FINAL() vs. timeouts</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="best-practices">Best Practices<a href="#best-practices" class="hash-link" aria-label="Direct link to Best Practices" title="Direct link to Best Practices">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="system-prompt-design-1">System Prompt Design<a href="#system-prompt-design-1" class="hash-link" aria-label="Direct link to System Prompt Design" title="Direct link to System Prompt Design">​</a></h3>
<ul>
<li>Keep it minimal and example-driven (following MIT paper)</li>
<li>Show chunking strategies, don&#x27;t prescribe them</li>
<li>Let models develop their own approaches</li>
<li>Emphasize <code>print()</code> for code outputs</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="context-generation">Context Generation<a href="#context-generation" class="hash-link" aria-label="Direct link to Context Generation" title="Direct link to Context Generation">​</a></h3>
<ul>
<li>Real benchmarks: Deploy production datasets (TREC, BrowseComp+, CodeQA) to S3 (~216MB total)</li>
<li>Runtime downloads datasets on-demand from S3</li>
<li>Makes benchmarks representative of real-world scenarios</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="safety">Safety<a href="#safety" class="hash-link" aria-label="Direct link to Safety" title="Direct link to Safety">​</a></h3>
<ul>
<li>Isolate REPL execution (no file system access)</li>
<li>Limit output buffer (last 100 lines)</li>
<li>Set max recursion depth (depth=1)</li>
<li>Max sub-calls limit (50 in our implementation)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="error-handling">Error Handling<a href="#error-handling" class="hash-link" aria-label="Direct link to Error Handling" title="Direct link to Error Handling">​</a></h3>
<ul>
<li>Return exceptions to root model</li>
<li>Retry transient Bedrock errors</li>
<li>Use async mode for long-running tasks</li>
<li>Log all trajectories to CloudWatch</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started">Getting Started<a href="#getting-started" class="hash-link" aria-label="Direct link to Getting Started" title="Direct link to Getting Started">​</a></h2>
<p>The complete implementation is available on GitHub with interactive CLI, deployment automation, and benchmark suite.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="quick-start">Quick Start<a href="#quick-start" class="hash-link" aria-label="Direct link to Quick Start" title="Direct link to Quick Start">​</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Clone repository</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">git clone https://github.com/manu-mishra/RLMWithStrands</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd RLMWithStrands</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Deploy to AWS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd infra</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cdk deploy</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Run benchmarks</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">python runexperiments</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Interactive menu:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 1. Run Benchmarks → Async (default) → Select model → Run All</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 2. Results saved to S3 automatically</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-directions">Future Directions<a href="#future-directions" class="hash-link" aria-label="Direct link to Future Directions" title="Direct link to Future Directions">​</a></h2>
<ul>
<li><strong>Deeper recursion:</strong> Allow sub-RLMs (depth=2+) for hierarchical decomposition</li>
<li><strong>Parallel sub-calls:</strong> Execute multiple chunks simultaneously to reduce latency</li>
<li><strong>Multi-modal RLMs:</strong> Extend to images, audio, video processing</li>
<li><strong>Fine-tuned models:</strong> Train on RLM trajectories to improve chunking efficiency</li>
<li><strong>Streaming results:</strong> Return partial answers as they&#x27;re computed</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Recursive Language Models (RLMs) enable processing inputs far beyond model context windows by treating the dataset as an external context buffer and using code to probe, filter, and recursively analyze only the relevant parts. By combining Strands Agents (agent loop + REPL orchestration) with Amazon Bedrock AgentCore (serverless runtime + observability), you can go from research idea to working implementation in hours instead of weeks.</p>
<p>This approach is especially useful for &quot;find all / count all / verify all&quot; workloads such as large codebase analysis, multi-document synthesis, and long-horizon agent workflows—where traditional long-context prompting or retrieval-only strategies can miss critical details.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Model choice</strong> - Root + sub-call model flexibility (Nova, Claude, GPT-OSS)</li>
<li><strong>Serverless runtime</strong> - AgentCore handles scaling and deployment</li>
<li><strong>Observability</strong> - CloudWatch GenAI dashboard with full trajectories</li>
<li><strong>Async execution + safety limits</strong> - Long-running tasks with guardrails</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="resources">Resources<a href="#resources" class="hash-link" aria-label="Direct link to Resources" title="Direct link to Resources">​</a></h2>
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/manu-mishra/RLMWithStrands" target="_blank" rel="noopener noreferrer">github.com/manu-mishra/RLMWithStrands</a></li>
<li><strong>Research Paper</strong>: <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">Recursive Language Models (arXiv:2512.24601)</a></li>
<li><strong>Strands Agents</strong>: <a href="https://github.com/awslabs/strands-agents" target="_blank" rel="noopener noreferrer">github.com/awslabs/strands-agents</a></li>
<li><strong>Amazon Bedrock AgentCore</strong>: <a href="https://aws.amazon.com/bedrock/agentcore" target="_blank" rel="noopener noreferrer">aws.amazon.com/bedrock/agentcore</a></li>
<li><strong>Amazon Bedrock</strong>: <a href="https://aws.amazon.com/bedrock" target="_blank" rel="noopener noreferrer">aws.amazon.com/bedrock</a></li>
<li><strong>AWS CDK</strong>: <a href="https://docs.aws.amazon.com/cdk" target="_blank" rel="noopener noreferrer">docs.aws.amazon.com/cdk</a></li>
</ul>
<hr>
<p><strong>Tags</strong>: #AmazonBedrock #GenerativeAI #AgentCore #StrandsAgents #MachineLearning #AWS</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/amazon-bedrock">amazon bedrock</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/agentcore">agentcore</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/strands-agents">strands agents</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/recursive-language-models">recursive language models</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/generative-ai">generative ai</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/machine-learning">machine learning</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/aws">aws</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/long-context">long context</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/s3-tables-streaming-analytics"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Building Serverless Real-Time Streaming Analytics with Amazon S3 Tables and Kinesis Data Firehose</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/aws-reinvent-2025-reinvented-powered-by-mcp"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">AWS Re:Invent 2025, Reinvented — Powered by MCP</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#what-are-recursive-language-models" class="table-of-contents__link toc-highlight">What Are Recursive Language Models?</a><ul><li><a href="#rlm-in-action-minimal-pattern" class="table-of-contents__link toc-highlight">RLM in Action: Minimal Pattern</a></li><li><a href="#rlm-vs-rag-when-to-use-each" class="table-of-contents__link toc-highlight">RLM vs RAG: When to Use Each</a></li><li><a href="#research-results" class="table-of-contents__link toc-highlight">Research Results</a></li></ul></li><li><a href="#why-strands--agentcore-makes-this-easy" class="table-of-contents__link toc-highlight">Why Strands + AgentCore Makes This Easy</a><ul><li><a href="#architecture-overview" class="table-of-contents__link toc-highlight">Architecture Overview</a></li><li><a href="#strands-agents-orchestration-made-simple" class="table-of-contents__link toc-highlight">Strands Agents: Orchestration Made Simple</a></li><li><a href="#amazon-bedrock-agentcore-deployment-without-the-pain" class="table-of-contents__link toc-highlight">Amazon Bedrock AgentCore: Deployment Without the Pain</a></li><li><a href="#cost-intuition" class="table-of-contents__link toc-highlight">Cost Intuition</a></li><li><a href="#amazon-bedrock-model-choice" class="table-of-contents__link toc-highlight">Amazon Bedrock: Model Choice</a></li></ul></li><li><a href="#how-it-works" class="table-of-contents__link toc-highlight">How It Works</a><ul><li><a href="#repl-environment" class="table-of-contents__link toc-highlight">REPL Environment</a></li><li><a href="#system-prompt-design" class="table-of-contents__link toc-highlight">System Prompt Design</a></li><li><a href="#agent-loop" class="table-of-contents__link toc-highlight">Agent Loop</a></li><li><a href="#asynchronous-processing" class="table-of-contents__link toc-highlight">Asynchronous Processing</a></li><li><a href="#recursive-sub-calls" class="table-of-contents__link toc-highlight">Recursive Sub-Calls</a></li><li><a href="#deployment" class="table-of-contents__link toc-highlight">Deployment</a></li></ul></li><li><a href="#example-analyzing-large-codebases" class="table-of-contents__link toc-highlight">Example: Analyzing Large Codebases</a></li><li><a href="#benchmarks-and-findings" class="table-of-contents__link toc-highlight">Benchmarks and Findings</a><ul><li><a href="#performance-on-long-context-tasks" class="table-of-contents__link toc-highlight">Performance on Long-Context Tasks</a></li><li><a href="#emergent-behaviors" class="table-of-contents__link toc-highlight">Emergent Behaviors</a></li><li><a href="#limitations" class="table-of-contents__link toc-highlight">Limitations</a></li></ul></li><li><a href="#when-to-use-rlms" class="table-of-contents__link toc-highlight">When to Use RLMs</a></li><li><a href="#observability" class="table-of-contents__link toc-highlight">Observability</a></li><li><a href="#best-practices" class="table-of-contents__link toc-highlight">Best Practices</a><ul><li><a href="#system-prompt-design-1" class="table-of-contents__link toc-highlight">System Prompt Design</a></li><li><a href="#context-generation" class="table-of-contents__link toc-highlight">Context Generation</a></li><li><a href="#safety" class="table-of-contents__link toc-highlight">Safety</a></li><li><a href="#error-handling" class="table-of-contents__link toc-highlight">Error Handling</a></li></ul></li><li><a href="#getting-started" class="table-of-contents__link toc-highlight">Getting Started</a><ul><li><a href="#quick-start" class="table-of-contents__link toc-highlight">Quick Start</a></li></ul></li><li><a href="#future-directions" class="table-of-contents__link toc-highlight">Future Directions</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#resources" class="table-of-contents__link toc-highlight">Resources</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Portfolio</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/experience">Experience</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/about">About</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Connect</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/manu-mishra" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://linkedin.com/in/manu-mishra" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:mishra.manu@outlook.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Manu Mishra. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>