<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="rss.xsl"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Manu Mishra Blog</title>
        <link>https://manumishra.com/blog</link>
        <description>Manu Mishra Blog</description>
        <lastBuildDate>Mon, 19 Jan 2026 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Build Recursive Language Models on AWS in Minutes with Strands Agents and Amazon Bedrock AgentCore]]></title>
            <link>https://manumishra.com/blog/recursive-language-models-strands-agentcore</link>
            <guid>https://manumishra.com/blog/recursive-language-models-strands-agentcore</guid>
            <pubDate>Mon, 19 Jan 2026 00:00:00 GMT</pubDate>
            <description><![CDATA[Learn how to implement Recursive Language Models (RLMs) from MIT CSAIL research using Strands Agents and Amazon Bedrock AgentCore. Scale to inputs 100x beyond context windows with minimal code.]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" alt="RLM on AWS Architecture" src="https://manumishra.com/assets/images/rlm-on-aws-e13ef244388c42150924e6a3424bf966.png" width="1536" height="1024" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">‚Äã</a></h2>
<p>Modern large language models face a fundamental limitation: context windows. While frontier models now reach 1 million tokens (Nova Premier, Claude Sonnet 4.5), workloads analyzing entire codebases, document collections, or multi-hour conversations can easily exceed 10 million tokens‚Äîfar beyond any single model's capacity.</p>
<p>This post demonstrates <strong>Recursive Language Models (RLMs)</strong>, an inference strategy from <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">MIT CSAIL research</a> that enables scaling to inputs far beyond context windows. What makes this implementation special: <strong>Strands Agents</strong> and <strong>Amazon Bedrock AgentCore</strong> reduce what could be weeks of glue code and deployment work to just a few hours of development.</p>
<!-- -->
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-recursive-language-models">What Are Recursive Language Models?<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#what-are-recursive-language-models" class="hash-link" aria-label="Direct link to What Are Recursive Language Models?" title="Direct link to What Are Recursive Language Models?">‚Äã</a></h2>
<p>Traditional approaches to long contexts use summarization or RAG. These struggle with information-dense tasks requiring examination of the full input.</p>
<p><strong>Recursive Language Models</strong> are an inference pattern for handling inputs far larger than any model context window. Instead of stuffing the entire dataset into the prompt, the full context lives outside the model as an external context buffer (a Python variable). The model then generates code to search, filter, and chunk the context, and uses recursive sub-calls on only the relevant pieces. Finally, results are aggregated through deterministic code rather than attention over the full input.</p>
<p>This approach is especially effective for tasks like "find all occurrences," "count all items," or "extract all endpoints," where missing even one chunk causes incorrect results.</p>
<p><strong>In our implementation, recursion depth is 1:</strong> the root model uses sub-calls to smaller models, and aggregation happens in code (not through nested RLMs).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rlm-in-action-minimal-pattern">RLM in Action: Minimal Pattern<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#rlm-in-action-minimal-pattern" class="hash-link" aria-label="Direct link to RLM in Action: Minimal Pattern" title="Direct link to RLM in Action: Minimal Pattern">‚Äã</a></h3>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Context lives outside the model as a Python variable</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">context </span><span class="token operator">=</span><span class="token plain"> load_large_codebase</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># can exceed model context windows by 100x+</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Model writes code to probe and filter</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">api_files </span><span class="token operator">=</span><span class="token plain"> search</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">context</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> pattern</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">r"@app\.route"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Recursive sub-calls on filtered chunks</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">endpoints </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">file</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> api_files</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    result </span><span class="token operator">=</span><span class="token plain"> llm_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f"Extract endpoint info from:\n</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation builtin" style="color:rgb(189, 147, 249)">file</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    endpoints</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">append</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">result</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Deterministic aggregation</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> aggregate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">endpoints</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rlm-vs-rag-when-to-use-each">RLM vs RAG: When to Use Each<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#rlm-vs-rag-when-to-use-each" class="hash-link" aria-label="Direct link to RLM vs RAG: When to Use Each" title="Direct link to RLM vs RAG: When to Use Each">‚Äã</a></h3>
<table><thead><tr><th>Scenario</th><th>RAG</th><th>RLM</th></tr></thead><tbody><tr><td><strong>Known query upfront</strong></td><td>‚úÖ Efficient retrieval</td><td>‚ö†Ô∏è Overkill</td></tr><tr><td><strong>"Find all" / "Count all"</strong></td><td>‚ùå May miss chunks</td><td>‚úÖ Exhaustive coverage</td></tr><tr><td><strong>Relevance-based QA</strong></td><td>‚úÖ Fast, targeted</td><td>‚ö†Ô∏è Slower</td></tr><tr><td><strong>Multi-step reasoning over full context</strong></td><td>‚ùå Limited by retrieval</td><td>‚úÖ Code-driven exploration</td></tr></tbody></table>
<p><strong>RAG struggles when:</strong> you don't know what to retrieve upfront, when correctness depends on coverage (not relevance), or when you need "verify-all" guarantees.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="research-results">Research Results<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#research-results" class="hash-link" aria-label="Direct link to Research Results" title="Direct link to Research Results">‚Äã</a></h3>
<p>The <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">MIT paper</a> demonstrates RLMs:</p>
<ul>
<li>Successfully handle inputs <strong>up to 100x beyond model context windows</strong></li>
<li>Outperform base LLMs and common long-context scaffolds on long-context retrieval benchmarks</li>
<li>Scale costs with task complexity, not input size</li>
</ul>
<p><strong>Note</strong>: Performance examples in this post are illustrative. Actual results vary by model, task complexity, and context structure.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-strands--agentcore-makes-this-easy">Why Strands + AgentCore Makes This Easy<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#why-strands--agentcore-makes-this-easy" class="hash-link" aria-label="Direct link to Why Strands + AgentCore Makes This Easy" title="Direct link to Why Strands + AgentCore Makes This Easy">‚Äã</a></h2>
<p>Building RLMs from scratch requires weeks of work: agent loop orchestration, REPL sandboxing, model invocation logic, deployment infrastructure, scaling configuration, and observability. <strong>Strands Agents</strong> and <strong>Amazon Bedrock AgentCore</strong> eliminate this complexity.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-overview">Architecture Overview<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#architecture-overview" class="hash-link" aria-label="Direct link to Architecture Overview" title="Direct link to Architecture Overview">‚Äã</a></h3>
<p><img decoding="async" loading="lazy" alt="RLM on AWS Architecture" src="https://manumishra.com/assets/images/rlm-on-aws-e13ef244388c42150924e6a3424bf966.png" width="1536" height="1024" class="img_ev3q">
<em>RLM implementation using Strands Agents and Amazon Bedrock AgentCore</em></p>
<p>The architecture shows the complete flow from user query to final answer:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="strands-agents-orchestration-made-simple">Strands Agents: Orchestration Made Simple<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#strands-agents-orchestration-made-simple" class="hash-link" aria-label="Direct link to Strands Agents: Orchestration Made Simple" title="Direct link to Strands Agents: Orchestration Made Simple">‚Äã</a></h3>
<p><a href="https://github.com/awslabs/strands-agents" target="_blank" rel="noopener noreferrer">Strands Agents</a> is an open-source Python SDK for building AI agents. It provides:</p>
<ul>
<li><strong>Agent loop orchestration</strong> - Handles iterative LLM ‚Üí code execution ‚Üí LLM cycles automatically</li>
<li><strong>Tool calling</strong> - Built-in Python REPL tool with output management</li>
<li><strong>Model integration</strong> - Native <a href="https://aws.amazon.com/bedrock/" target="_blank" rel="noopener noreferrer">Amazon Bedrock</a> support with streaming</li>
<li><strong>State management</strong> - Tracks conversation history and execution context</li>
</ul>
<p><strong>Impact:</strong> What would take weeks of infrastructure code becomes ~200 lines.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="amazon-bedrock-agentcore-deployment-without-the-pain">Amazon Bedrock AgentCore: Deployment Without the Pain<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#amazon-bedrock-agentcore-deployment-without-the-pain" class="hash-link" aria-label="Direct link to Amazon Bedrock AgentCore: Deployment Without the Pain" title="Direct link to Amazon Bedrock AgentCore: Deployment Without the Pain">‚Äã</a></h3>
<p><a href="https://aws.amazon.com/bedrock/agentcore/" target="_blank" rel="noopener noreferrer">Amazon Bedrock AgentCore</a> provides serverless runtime for AI agents:</p>
<ul>
<li><strong>Long-running execution</strong> - 15-minute idle timeout when not processing (not a max runtime limit)</li>
<li><strong>Automatic scaling</strong> - Handles concurrent requests without capacity planning</li>
<li><strong>Session isolation</strong> - Each invocation gets isolated environment</li>
<li><strong>ARM64 optimization</strong> - <a href="https://aws.amazon.com/ec2/graviton/" target="_blank" rel="noopener noreferrer">AWS Graviton</a> processors for cost efficiency</li>
<li><strong>Built-in observability</strong> - <a href="https://aws.amazon.com/cloudwatch/" target="_blank" rel="noopener noreferrer">Amazon CloudWatch</a> logs and metrics</li>
</ul>
<p><strong>Impact:</strong> Deployment is a standard CDK workflow, and invocation happens via the AgentCore Runtime API. RLM experiments that take 5+ minutes run without timeout issues. Safety limits (max sub-calls, output buffer) prevent runaway execution.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cost-intuition">Cost Intuition<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#cost-intuition" class="hash-link" aria-label="Direct link to Cost Intuition" title="Direct link to Cost Intuition">‚Äã</a></h3>
<p>In practice, the root model spends tokens on planning and code generation, while sub-calls only process filtered slices of the context. This makes total token usage closer to "work performed" rather than "data size"‚Äîoften landing closer to planning plus the slices you inspect (for example, tens to a few hundred thousand tokens for many workloads, depending on how much content is actually analyzed).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="amazon-bedrock-model-choice">Amazon Bedrock: Model Choice<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#amazon-bedrock-model-choice" class="hash-link" aria-label="Direct link to Amazon Bedrock: Model Choice" title="Direct link to Amazon Bedrock: Model Choice">‚Äã</a></h3>
<p><a href="https://aws.amazon.com/bedrock/" target="_blank" rel="noopener noreferrer">Amazon Bedrock</a> is a fully managed service that offers a choice of high-performing foundation models from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, OpenAI, and Amazon through a single API. This model choice is critical for RLM implementations‚Äîdifferent tasks benefit from different model strengths in reasoning, code generation, and cost efficiency.</p>
<p>Following the MIT paper's two-model approach, we use a root model for orchestration and a smaller model for sub-calls:</p>
<p><strong>Supported Root Models:</strong></p>
<ul>
<li><strong>Amazon Nova Pro</strong> - 300K context, strong reasoning</li>
<li><strong>Claude 4.5 Sonnet</strong> - 200K context (1M beta), excellent code generation</li>
<li><strong>Claude 4.5 Opus</strong> - 200K context, frontier performance</li>
<li><strong>GPT-OSS 120B</strong> - 128K context, open-source option</li>
</ul>
<p><strong>Supported Sub-Call Models:</strong></p>
<ul>
<li><strong>Amazon Nova Micro</strong> - 128K context, optimized for speed</li>
<li><strong>Claude 4.5 Haiku</strong> - Fast, cost-efficient</li>
<li><strong>Amazon Nova Lite</strong> - Balanced performance</li>
</ul>
<p>This two-tier approach balances capability and efficiency: a powerful model for strategy, a fast model for execution. Our implementation is model-agnostic‚Äîswap models via configuration without code changes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-it-works">How It Works<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#how-it-works" class="hash-link" aria-label="Direct link to How It Works" title="Direct link to How It Works">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="repl-environment">REPL Environment<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#repl-environment" class="hash-link" aria-label="Direct link to REPL Environment" title="Direct link to REPL Environment">‚Äã</a></h3>
<p>The core innovation is loading the entire input context as an external context buffer (Python variable) accessible to the LLM. The environment provides:</p>
<ul>
<li><code>context</code> variable containing the full input</li>
<li><code>llm_query(prompt)</code> function for recursive sub-LM calls</li>
<li>Standard Python libraries for text processing (regex, string manipulation)</li>
<li>Isolated execution preventing access to system resources</li>
</ul>
<p>The LLM writes Python code to interact with this environment, executing operations like regex searches, chunking, and filtering without loading the entire context into its neural network.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="system-prompt-design">System Prompt Design<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#system-prompt-design" class="hash-link" aria-label="Direct link to System Prompt Design" title="Direct link to System Prompt Design">‚Äã</a></h3>
<p>Based on <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">MIT paper Appendix D</a>, the system prompt is minimal and example-driven:</p>
<p><strong>Core Instructions:</strong></p>
<ul>
<li>Context is available as a Python variable in REPL, not in the prompt</li>
<li>Use code to probe, filter, and chunk the context</li>
<li>Make recursive <code>llm_query()</code> calls on filtered chunks</li>
<li>Return final answer directly (no special format required)</li>
</ul>
<p><strong>Example Strategies:</strong></p>
<ul>
<li>Regex filtering: Search for keywords without reading entire context</li>
<li>Uniform chunking: Split into equal-sized pieces for parallel processing</li>
<li>Semantic chunking: Use document structure (headers, file boundaries)</li>
</ul>
<p><strong>Example Pattern:</strong></p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Search in chunks</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">chunk_size </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">50000</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># ~12.5K tokens</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">for</span><span class="token plain"> i </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">range</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(189, 147, 249)">len</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">context</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> chunk_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    chunk </span><span class="token operator">=</span><span class="token plain"> context</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">i</span><span class="token operator">+</span><span class="token plain">chunk_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    result </span><span class="token operator">=</span><span class="token plain"> llm_query</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">f"Find the magic number in: </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token string-interpolation interpolation">chunk</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token string-interpolation string" style="color:rgb(255, 121, 198)">"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">result</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre></div></div>
<p>The prompt avoids prescriptive "ALWAYS/NEVER" rules, letting models develop their own strategies.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-loop">Agent Loop<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#agent-loop" class="hash-link" aria-label="Direct link to Agent Loop" title="Direct link to Agent Loop">‚Äã</a></h3>
<p>The Strands Agent orchestrates iterative REPL interaction:</p>
<ol>
<li>Root LM receives query and context metadata (length, structure)</li>
<li>Root LM generates Python code to execute in REPL</li>
<li>Code executes, output returned (truncated to prevent overflow)</li>
<li>Root LM sees output, decides to continue or return answer</li>
<li>Loop continues until answer provided or timeout</li>
</ol>
<p>This iterative process allows the model to refine its approach based on execution feedback.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="asynchronous-processing">Asynchronous Processing<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#asynchronous-processing" class="hash-link" aria-label="Direct link to Asynchronous Processing" title="Direct link to Asynchronous Processing">‚Äã</a></h3>
<p>All experiments run asynchronously by default:</p>
<ul>
<li>Immediate response with task ID and session ID</li>
<li>Client polls for results (no timeout issues)</li>
<li>Tasks can run for minutes or hours</li>
<li>Results automatically saved to <a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener noreferrer">Amazon S3</a> with full metadata</li>
<li>15-minute idle timeout only applies when NOT processing</li>
</ul>
<p>This enables long-running RLM tasks without connection timeouts.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="recursive-sub-calls">Recursive Sub-Calls<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#recursive-sub-calls" class="hash-link" aria-label="Direct link to Recursive Sub-Calls" title="Direct link to Recursive Sub-Calls">‚Äã</a></h3>
<p>The <code>llm_query()</code> function enables decomposition:</p>
<ul>
<li>Sub-calls use smaller model (Amazon Nova Micro) with smaller context</li>
<li>Each sub-call is independent (no shared state except via code variables)</li>
<li>Sub-calls can be batched in code (process multiple chunks in loop)</li>
<li>Results stored in Python variables for aggregation</li>
</ul>
<p>The paper uses max recursion depth of 1 (sub-calls are base LLMs, not RLMs).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deployment">Deployment<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#deployment" class="hash-link" aria-label="Direct link to Deployment" title="Direct link to Deployment">‚Äã</a></h3>
<p><a href="https://aws.amazon.com/cdk/" target="_blank" rel="noopener noreferrer">AWS CDK</a> deploys the RLM agent to AgentCore Runtime:</p>
<ul>
<li>Build ARM64 Docker image with Strands Agent code</li>
<li>Create IAM role with Bedrock, S3, and CloudWatch permissions</li>
<li>Deploy datasets (TREC, BrowseComp+, CodeQA) to S3 bucket</li>
<li>Configure CloudWatch log group for traces</li>
<li>Deploy AgentCore Runtime with container image</li>
</ul>
<p>AgentCore handles scaling, versioning, and observability automatically. Datasets are uploaded during <code>cdk deploy</code> and downloaded by the runtime on-demand.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-analyzing-large-codebases">Example: Analyzing Large Codebases<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#example-analyzing-large-codebases" class="hash-link" aria-label="Direct link to Example: Analyzing Large Codebases" title="Direct link to Example: Analyzing Large Codebases">‚Äã</a></h2>
<p><strong>Task:</strong> Identify all API endpoints and their authentication requirements in a codebase with hundreds of files.</p>
<p><strong>Traditional Approach Limitations:</strong></p>
<ul>
<li>Direct LLM call: May exceed context window</li>
<li>RAG: May miss endpoints, requires good chunking strategy upfront</li>
<li>Manual analysis: Time-consuming, error-prone</li>
</ul>
<p><strong>RLM Approach:</strong></p>
<ol>
<li><strong>Probe:</strong> Check total size, identify file boundaries</li>
<li><strong>Filter:</strong> Use regex to find files with route decorators (<code>@app.route</code>, <code>@api.route</code>)</li>
<li><strong>Analyze:</strong> Make sub-LM calls on each API file to extract endpoint details</li>
<li><strong>Aggregate:</strong> Combine results into structured summary</li>
</ol>
<p><strong>Outcome:</strong></p>
<ul>
<li>Processes codebases beyond single model context limits</li>
<li>Identifies all API endpoints systematically</li>
<li>Extracts authentication requirements for each</li>
<li>Completes in minutes with minimal sub-calls</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmarks-and-findings">Benchmarks and Findings<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#benchmarks-and-findings" class="hash-link" aria-label="Direct link to Benchmarks and Findings" title="Direct link to Benchmarks and Findings">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-on-long-context-tasks">Performance on Long-Context Tasks<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#performance-on-long-context-tasks" class="hash-link" aria-label="Direct link to Performance on Long-Context Tasks" title="Direct link to Performance on Long-Context Tasks">‚Äã</a></h3>
<p>We evaluated RLM on benchmarks using real-world datasets (TREC, Tevatron BrowseComp+, LongBench-v2) deployed to S3.</p>
<table><thead><tr><th>Test</th><th>Context Size</th><th>Task</th><th>Dataset Source</th></tr></thead><tbody><tr><td>OOLONG</td><td>5,452 TREC entries</td><td>Count label frequencies</td><td>TREC coarse dataset</td></tr><tr><td>OOLONG-Pairs</td><td>5,452 TREC entries</td><td>Find HUM/LOC pairs</td><td>TREC coarse dataset</td></tr><tr><td>BrowseComp-1K</td><td>1,000 documents</td><td>Answer research query</td><td>Tevatron BrowseComp+</td></tr><tr><td>CodeQA</td><td>Multi-file repos</td><td>Multi-choice reasoning</td><td>LongBench-v2 Code</td></tr></tbody></table>
<p><strong>Key Findings:</strong></p>
<p><strong>Accuracy for Retrieval Tasks:</strong> Code-based counting and searching reduces hallucinations for tasks requiring exact matches. Direct model calls often produce inconsistent results on the same input.</p>
<p><strong>Real Datasets:</strong> All benchmarks use production datasets (~216MB total) deployed to S3 and loaded at runtime, simulating real-world information retrieval scenarios.</p>
<p><strong>Async Execution:</strong> All tests run asynchronously with results saved to S3, enabling long-running tasks without timeout issues.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="emergent-behaviors">Emergent Behaviors<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#emergent-behaviors" class="hash-link" aria-label="Direct link to Emergent Behaviors" title="Direct link to Emergent Behaviors">‚Äã</a></h3>
<p>RLM trajectories showed interesting patterns:</p>
<ul>
<li><strong>Regex filtering:</strong> Models searched for keywords without reading entire context</li>
<li><strong>Adaptive chunking:</strong> Adjusted chunk sizes based on task complexity</li>
<li><strong>Answer verification:</strong> Made additional sub-calls to validate results</li>
<li><strong>Strategic decisions:</strong> Chose between "process all" vs "filter then process" strategies</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="limitations">Limitations<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#limitations" class="hash-link" aria-label="Direct link to Limitations" title="Direct link to Limitations">‚Äã</a></h3>
<ol>
<li><strong>Async-first design:</strong> All experiments run asynchronously; synchronous mode available but not recommended for long tasks</li>
<li><strong>Model-specific behavior:</strong> Different models show varying chunking strategies and sub-call patterns</li>
<li><strong>Sub-call limits:</strong> Max 50 sub-calls prevents runaway execution but may limit very complex tasks</li>
<li><strong>Debugging complexity:</strong> Full trajectory examination needed via CloudWatch logs</li>
<li><strong>Dataset size:</strong> Real datasets (TREC, BrowseComp+, CodeQA) are large files (~216MB total) deployed to S3</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="when-to-use-rlms">When to Use RLMs<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#when-to-use-rlms" class="hash-link" aria-label="Direct link to When to Use RLMs" title="Direct link to When to Use RLMs">‚Äã</a></h2>
<p><strong>Ideal for:</strong></p>
<ul>
<li>Information-dense aggregation across entire datasets</li>
<li>Large codebase analysis (patterns, security, dependencies)</li>
<li>Multi-document reasoning requiring synthesis</li>
<li>Contexts beyond model limits</li>
</ul>
<p><strong>Use alternatives for:</strong></p>
<ul>
<li>Single-document QA within context window (direct calls)</li>
<li>Sparse retrieval (RAG more efficient)</li>
<li>Real-time requirements (RLM takes seconds to minutes)</li>
<li>Simple extraction (regex/parsing sufficient)</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="observability">Observability<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#observability" class="hash-link" aria-label="Direct link to Observability" title="Direct link to Observability">‚Äã</a></h2>
<p>AgentCore provides CloudWatch integration for monitoring RLM trajectories:</p>
<p><strong>Enable Transaction Search:</strong>
One-time setup to send X-Ray traces to CloudWatch Logs for GenAI Observability dashboard.</p>
<p><strong>View Traces:</strong>
CloudWatch ‚Üí GenAI Observability shows:</p>
<ul>
<li>Complete RLM trajectories (each REPL iteration)</li>
<li>Sub-LM call patterns (count, timing, token usage)</li>
<li>Token efficiency (processed vs. context size)</li>
<li>Execution time breakdown</li>
</ul>
<p><strong>Key Metrics:</strong></p>
<ul>
<li>Trajectory length: Number of REPL iterations (target: &lt;20)</li>
<li>Sub-call count: Recursive invocations (target: &lt;30)</li>
<li>Token efficiency: % of context actually processed</li>
<li>Success rate: FINAL() vs. timeouts</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="best-practices">Best Practices<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#best-practices" class="hash-link" aria-label="Direct link to Best Practices" title="Direct link to Best Practices">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="system-prompt-design-1">System Prompt Design<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#system-prompt-design-1" class="hash-link" aria-label="Direct link to System Prompt Design" title="Direct link to System Prompt Design">‚Äã</a></h3>
<ul>
<li>Keep it minimal and example-driven (following MIT paper)</li>
<li>Show chunking strategies, don't prescribe them</li>
<li>Let models develop their own approaches</li>
<li>Emphasize <code>print()</code> for code outputs</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="context-generation">Context Generation<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#context-generation" class="hash-link" aria-label="Direct link to Context Generation" title="Direct link to Context Generation">‚Äã</a></h3>
<ul>
<li>Real benchmarks: Deploy production datasets (TREC, BrowseComp+, CodeQA) to S3 (~216MB total)</li>
<li>Runtime downloads datasets on-demand from S3</li>
<li>Makes benchmarks representative of real-world scenarios</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="safety">Safety<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#safety" class="hash-link" aria-label="Direct link to Safety" title="Direct link to Safety">‚Äã</a></h3>
<ul>
<li>Isolate REPL execution (no file system access)</li>
<li>Limit output buffer (last 100 lines)</li>
<li>Set max recursion depth (depth=1)</li>
<li>Max sub-calls limit (50 in our implementation)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="error-handling">Error Handling<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#error-handling" class="hash-link" aria-label="Direct link to Error Handling" title="Direct link to Error Handling">‚Äã</a></h3>
<ul>
<li>Return exceptions to root model</li>
<li>Retry transient Bedrock errors</li>
<li>Use async mode for long-running tasks</li>
<li>Log all trajectories to CloudWatch</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started">Getting Started<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#getting-started" class="hash-link" aria-label="Direct link to Getting Started" title="Direct link to Getting Started">‚Äã</a></h2>
<p>The complete implementation is available on GitHub with interactive CLI, deployment automation, and benchmark suite.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="quick-start">Quick Start<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#quick-start" class="hash-link" aria-label="Direct link to Quick Start" title="Direct link to Quick Start">‚Äã</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Clone repository</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">git clone https://github.com/manu-mishra/RLMWithStrands</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd RLMWithStrands</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Deploy to AWS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd infra</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cdk deploy</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Run benchmarks</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">python runexperiments</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Interactive menu:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 1. Run Benchmarks ‚Üí Async (default) ‚Üí Select model ‚Üí Run All</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 2. Results saved to S3 automatically</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-directions">Future Directions<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#future-directions" class="hash-link" aria-label="Direct link to Future Directions" title="Direct link to Future Directions">‚Äã</a></h2>
<ul>
<li><strong>Deeper recursion:</strong> Allow sub-RLMs (depth=2+) for hierarchical decomposition</li>
<li><strong>Parallel sub-calls:</strong> Execute multiple chunks simultaneously to reduce latency</li>
<li><strong>Multi-modal RLMs:</strong> Extend to images, audio, video processing</li>
<li><strong>Fine-tuned models:</strong> Train on RLM trajectories to improve chunking efficiency</li>
<li><strong>Streaming results:</strong> Return partial answers as they're computed</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>Recursive Language Models (RLMs) enable processing inputs far beyond model context windows by treating the dataset as an external context buffer and using code to probe, filter, and recursively analyze only the relevant parts. By combining Strands Agents (agent loop + REPL orchestration) with Amazon Bedrock AgentCore (serverless runtime + observability), you can go from research idea to working implementation in hours instead of weeks.</p>
<p>This approach is especially useful for "find all / count all / verify all" workloads such as large codebase analysis, multi-document synthesis, and long-horizon agent workflows‚Äîwhere traditional long-context prompting or retrieval-only strategies can miss critical details.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Model choice</strong> - Root + sub-call model flexibility (Nova, Claude, GPT-OSS)</li>
<li><strong>Serverless runtime</strong> - AgentCore handles scaling and deployment</li>
<li><strong>Observability</strong> - CloudWatch GenAI dashboard with full trajectories</li>
<li><strong>Async execution + safety limits</strong> - Long-running tasks with guardrails</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="resources">Resources<a href="https://manumishra.com/blog/recursive-language-models-strands-agentcore#resources" class="hash-link" aria-label="Direct link to Resources" title="Direct link to Resources">‚Äã</a></h2>
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/manu-mishra/RLMWithStrands" target="_blank" rel="noopener noreferrer">github.com/manu-mishra/RLMWithStrands</a></li>
<li><strong>Research Paper</strong>: <a href="https://arxiv.org/abs/2512.24601" target="_blank" rel="noopener noreferrer">Recursive Language Models (arXiv:2512.24601)</a></li>
<li><strong>Strands Agents</strong>: <a href="https://github.com/awslabs/strands-agents" target="_blank" rel="noopener noreferrer">github.com/awslabs/strands-agents</a></li>
<li><strong>Amazon Bedrock AgentCore</strong>: <a href="https://aws.amazon.com/bedrock/agentcore" target="_blank" rel="noopener noreferrer">aws.amazon.com/bedrock/agentcore</a></li>
<li><strong>Amazon Bedrock</strong>: <a href="https://aws.amazon.com/bedrock" target="_blank" rel="noopener noreferrer">aws.amazon.com/bedrock</a></li>
<li><strong>AWS CDK</strong>: <a href="https://docs.aws.amazon.com/cdk" target="_blank" rel="noopener noreferrer">docs.aws.amazon.com/cdk</a></li>
</ul>
<hr>
<p><strong>Tags</strong>: #AmazonBedrock #GenerativeAI #AgentCore #StrandsAgents #MachineLearning #AWS</p>]]></content:encoded>
            <category>amazon bedrock</category>
            <category>agentcore</category>
            <category>strands agents</category>
            <category>recursive language models</category>
            <category>generative ai</category>
            <category>machine learning</category>
            <category>aws</category>
            <category>long context</category>
        </item>
        <item>
            <title><![CDATA[AWS Re:Invent 2025, Reinvented ‚Äî Powered by MCP]]></title>
            <link>https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp</link>
            <guid>https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp</guid>
            <pubDate>Mon, 27 Oct 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Comprehensive Model Context Protocol (MCP) server providing intelligent access to AWS re:Invent 2025 session catalog with 1,843 sessions, advanced search capabilities, and speaker discovery.]]></description>
            <content:encoded><![CDATA[<p>Every year, AWS re<!-- -->:Invent<!-- --> brings together thousands of builders, leaders, and innovators to explore the future of cloud. In 2025, the catalog is bigger than ever ‚Äî <strong>1,843 sessions</strong> across <strong>53 areas of interest</strong> and <strong>19 industries</strong>. Inspiring, yes ‚Äî but also overwhelming.</p>
<p>That's why I built the <strong>re<!-- -->:Invent<!-- --> 2025 MCP Server</strong>: a comprehensive Model Context Protocol server that transforms how professionals navigate AWS's flagship conference, providing intelligent access to the complete session catalog with advanced search capabilities and speaker discovery.</p>
<!-- -->
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-quick-start-for-claude-desktop-users">üöÄ Quick Start for Claude Desktop Users<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-quick-start-for-claude-desktop-users" class="hash-link" aria-label="Direct link to üöÄ Quick Start for Claude Desktop Users" title="Direct link to üöÄ Quick Start for Claude Desktop Users">‚Äã</a></h2>
<p><strong>Get instant access to all 1,843 re<!-- -->:Invent<!-- --> 2025 sessions in Claude Desktop:</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="-download-claude-extension"><a href="https://github.com/manu-mishra/reinvent-mcp-2025/raw/main/claude/reinvent-2025-session-catalog-nodejs.mcpb" target="_blank" rel="noopener noreferrer">üì¶ Download Claude Extension</a><a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-download-claude-extension" class="hash-link" aria-label="Direct link to -download-claude-extension" title="Direct link to -download-claude-extension">‚Äã</a></h3>
<p><strong>Installation</strong>: Just drag &amp; drop the <code>.mcpb</code> file into Claude Desktop Extensions ‚Üí Click "Install" ‚Üí Start exploring!</p>
<p><strong>Try asking Claude</strong>: <em>"Find all AI sessions at re<!-- -->:Invent<!-- --> 2025"</em> or <em>"Show me sessions for developers about serverless"</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-what-it-is">üåü What It Is<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-what-it-is" class="hash-link" aria-label="Direct link to üåü What It Is" title="Direct link to üåü What It Is">‚Äã</a></h2>
<p>The re<!-- -->:Invent<!-- --> 2025 MCP Server transforms how professionals navigate AWS's flagship conference. Built on the <a href="https://modelcontextprotocol.io/" target="_blank" rel="noopener noreferrer">Model Context Protocol</a>, this server bridges the gap between conference content and actionable insights.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>üîç <strong>Universal Search</strong>: Query across titles, abstracts, topics, and metadata</li>
<li>üë• <strong>Speaker Discovery</strong>: Find speakers by name or company with session mapping</li>
<li>üéØ <strong>Advanced Filtering</strong>: 13 specialized tools for precise content discovery</li>
<li>‚ö° <strong>Performance Optimized</strong>: Sub-second startup with &lt;10ms response times</li>
</ul>
<p>And here's something important: üîí <strong>No internet required. No tracking. All requests stay between you and your AI assistant.</strong> I don't see or log anything ‚Äî it's your data, your control.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-who-its-for">üéØ Who It's For<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-who-its-for" class="hash-link" aria-label="Direct link to üéØ Who It's For" title="Direct link to üéØ Who It's For">‚Äã</a></h2>
<p><strong>üíª Developers &amp; Architects</strong> ‚Äî Discover hands-on workshops, certification-aligned content, and service-specific sessions that accelerate skill development.</p>
<p><strong>üìä Executives &amp; Leaders</strong> ‚Äî Access strategic frameworks, industry-specific insights, and quantified business outcomes that inform decision-making.</p>
<p><strong>ü§ù Sales &amp; Partners</strong> ‚Äî Research customer industries, identify co-marketing opportunities, and extract compelling value propositions from 1,843 sessions.</p>
<p><strong>üìÖ Event Teams &amp; Analysts</strong> ‚Äî Analyze content gaps, optimize scheduling, track speaker participation across the world's largest cloud computing conference.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-available-mcp-tools">‚ú® Available MCP Tools<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-available-mcp-tools" class="hash-link" aria-label="Direct link to ‚ú® Available MCP Tools" title="Direct link to ‚ú® Available MCP Tools">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="core-discovery-tools">Core Discovery Tools<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#core-discovery-tools" class="hash-link" aria-label="Direct link to Core Discovery Tools" title="Direct link to Core Discovery Tools">‚Äã</a></h3>
<ul>
<li><code>search_sessions</code> - Universal session search or complete listing</li>
<li><code>search_speakers</code> - Speaker discovery with session mapping</li>
<li><code>get_session_details</code> - Complete session information</li>
<li><code>search_services</code> - AWS service-specific sessions</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="specialized-filters">Specialized Filters<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#specialized-filters" class="hash-link" aria-label="Direct link to Specialized Filters" title="Direct link to Specialized Filters">‚Äã</a></h3>
<ul>
<li><code>get_sessions_by_level</code> - Foundational ‚Üí Distinguished (5 levels)</li>
<li><code>get_sessions_by_role</code> - Developer, Architect, Data Scientist, etc. (19 roles)</li>
<li><code>get_sessions_by_industry</code> - Healthcare, Financial Services, Government, etc. (19 industries)</li>
<li><code>get_sessions_by_topic</code> - AI, Security, Databases, Serverless, etc. (18 topics)</li>
<li><code>get_sessions_by_area_of_interest</code> - Generative AI, DevOps, Cost Optimization, etc. (53 areas)</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="Ô∏è-how-to-get-started">üõ†Ô∏è How to Get Started<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#%EF%B8%8F-how-to-get-started" class="hash-link" aria-label="Direct link to üõ†Ô∏è How to Get Started" title="Direct link to üõ†Ô∏è How to Get Started">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="one-click-installation-claude-desktop">One-Click Installation (Claude Desktop)<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#one-click-installation-claude-desktop" class="hash-link" aria-label="Direct link to One-Click Installation (Claude Desktop)" title="Direct link to One-Click Installation (Claude Desktop)">‚Äã</a></h3>
<p><a href="https://github.com/manu-mishra/reinvent-mcp-2025/raw/main/claude/reinvent-2025-session-catalog-nodejs.mcpb" target="_blank" rel="noopener noreferrer">üì¶ Download Extension</a> - Just drag &amp; drop into Claude Desktop!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="command-line-installation">Command Line Installation<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#command-line-installation" class="hash-link" aria-label="Direct link to Command Line Installation" title="Direct link to Command Line Installation">‚Äã</a></h3>
<p><strong>Node.js:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">npx reinvent2025mcp</span><br></span></code></pre></div></div>
<p><strong>Test with MCP Inspector:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">npx @modelcontextprotocol/inspector npx reinvent2025mcp</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="integration-with-ai-assistants">Integration with AI Assistants<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#integration-with-ai-assistants" class="hash-link" aria-label="Direct link to Integration with AI Assistants" title="Direct link to Integration with AI Assistants">‚Äã</a></h3>
<p><strong>Amazon Q Developer (CLI):</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">q mcp add --name reinvent-2025 --command npx --args reinvent2025mcp</span><br></span></code></pre></div></div>
<p><strong>Claude Desktop Manual Config:</strong></p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token property">"mcpServers"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token property">"reinvent-2025"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token property">"command"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"npx"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token property">"args"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"reinvent2025mcp"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-dataset-overview">üìä Dataset Overview<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-dataset-overview" class="hash-link" aria-label="Direct link to üìä Dataset Overview" title="Direct link to üìä Dataset Overview">‚Äã</a></h2>
<p><strong>Comprehensive Coverage:</strong></p>
<ul>
<li><strong>1,843 Sessions</strong> across all re<!-- -->:Invent<!-- --> 2025 tracks</li>
<li><strong>53 Areas of Interest</strong> from Generative AI to Quantum Technologies</li>
<li><strong>198 AWS Services</strong> with complete service coverage</li>
<li><strong>19 Industries</strong> for vertical-specific content discovery</li>
<li><strong>5 Difficulty Levels</strong> from foundational to distinguished expertise</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-why-this-matters">üåç Why This Matters<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-why-this-matters" class="hash-link" aria-label="Direct link to üåç Why This Matters" title="Direct link to üåç Why This Matters">‚Äã</a></h2>
<p>re<!-- -->:Invent<!-- --> isn't just another conference ‚Äî it's the epicenter of cloud innovation, where breakthrough technologies like agentic AI, quantum computing, and autonomous systems are unveiled. With sessions covering everything from foundational concepts to distinguished-level expertise, re<!-- -->:Invent<!-- --> shapes the future of technology.</p>
<p>This MCP server ensures you don't miss the sessions that matter most to your goals, whether that's technical mastery, business transformation, or competitive advantage ‚Äî all while keeping your data private with a local-first design.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-try-it-today">‚úÖ Try It Today<a href="https://manumishra.com/blog/aws-reinvent-2025-reinvented-powered-by-mcp#-try-it-today" class="hash-link" aria-label="Direct link to ‚úÖ Try It Today" title="Direct link to ‚úÖ Try It Today">‚Äã</a></h2>
<ul>
<li>üì¶ <a href="https://www.npmjs.com/package/reinvent2025mcp" target="_blank" rel="noopener noreferrer">NPM Package</a></li>
<li>üì¶ <a href="https://pypi.org/project/re-invent-2025-mcp/" target="_blank" rel="noopener noreferrer">PyPI Package</a></li>
<li>üíª <a href="https://github.com/manu-mishra/reinvent-mcp-2025" target="_blank" rel="noopener noreferrer">GitHub Repository</a></li>
<li>üîó <a href="https://modelcontextprotocol.io/" target="_blank" rel="noopener noreferrer">Model Context Protocol</a></li>
</ul>
<p>I'd love your feedback, ideas, and contributions ‚Äî this is built for the AWS community, and the more voices we have, the stronger it becomes.</p>]]></content:encoded>
            <category>aws reinvent</category>
            <category>mcp</category>
            <category>ai assistant</category>
            <category>conference</category>
            <category>cloud computing</category>
            <category>aws community</category>
            <category>claude desktop</category>
        </item>
        <item>
            <title><![CDATA[Google's EmbeddingGemma on AWS Lambda - A Curiosity-Driven Experiment]]></title>
            <link>https://manumishra.com/blog/embeddings-gemma-on-lambda</link>
            <guid>https://manumishra.com/blog/embeddings-gemma-on-lambda</guid>
            <pubDate>Sun, 21 Sep 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Deploy Google's EmbeddingGemma 300M parameter embedding model on AWS Lambda using container-based architecture. Includes performance benchmarks, cold start analysis, and complete deployment guide.]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" alt="EmbeddingGemma on AWS Lambda" src="https://manumishra.com/assets/images/embeddings-gemma-lambda-287f228bdf79dafe957097edaad2ce75.png" width="1024" height="1024" class="img_ev3q"></p>
<p><em>Note: This is a curiosity-driven experiment, not a production recommendation. For real workloads, <a href="https://aws.amazon.com/sagemaker/" target="_blank" rel="noopener noreferrer">Amazon SageMaker</a> is the right choice. This project explores what's possible when you push serverless boundaries.</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-the-idea">1. The idea<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#1-the-idea" class="hash-link" aria-label="Direct link to 1. The idea" title="Direct link to 1. The idea">‚Äã</a></h2>
<p>After my <a href="https://community.aws/content/2ynHjrct8JLUEN6mADtT2IYh5bR/microsoft-bitnet-1-58-bit-llms-on-aws-lambda" target="_blank" rel="noopener noreferrer">BitNet Lambda experiment</a>, I kept thinking: what about embeddings? I had text generation working on Lambda, but what about the other half of modern AI applications?</p>
<p>Google's EmbeddingGemma caught my attention‚Äî300M parameters, multilingual, designed for efficiency. Could it work on Lambda? Only one way to find out.</p>
<p>So I fired up Amazon Q Developer and started experimenting.</p>
<!-- -->
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-why-embeddings-matter">2. Why embeddings matter<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#2-why-embeddings-matter" class="hash-link" aria-label="Direct link to 2. Why embeddings matter" title="Direct link to 2. Why embeddings matter">‚Äã</a></h2>
<p>Modern AI applications need both text generation and embeddings. RAG systems, semantic search, document processing‚Äîthey all require this dual capability. I had the generation part working with BitNet, but what about embeddings?</p>
<p>EmbeddingGemma sits in a sweet spot: 300M parameters (~1.2GB) with multilingual support for 100+ languages. Unlike massive text generation models, embedding models are:</p>
<ul>
<li><strong>Predictable</strong>: Fixed output dimensions (768 floats)</li>
<li><strong>Efficient</strong>: Single forward pass, no autoregressive generation</li>
<li><strong>Compact</strong>: Smaller memory footprint than multi-billion parameter LLMs</li>
</ul>
<p>That efficiency profile makes "Lambda + Embeddings" the perfect complement to my BitNet experiment‚Äîcompleting the serverless AI toolkit.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-the-architecture">3. The architecture<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#3-the-architecture" class="hash-link" aria-label="Direct link to 3. The architecture" title="Direct link to 3. The architecture">‚Äã</a></h2>
<p>The architecture stayed simple: API Gateway triggers a Lambda function with 2GB memory. Inside lives a container image with transformers, sentence-transformers, and the complete EmbeddingGemma model. Lambda processes the text and returns a 768-dimensional vector.</p>
<p>Thanks to Amazon Q's help, I optimized the container to embed the entire model (~1.2GB) while keeping cold starts reasonable. No external model loading, no S3 downloads‚Äîeverything lives in the container.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-amazon-q-as-co-pilot">4. Amazon Q as co-pilot<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#4-amazon-q-as-co-pilot" class="hash-link" aria-label="Direct link to 4. Amazon Q as co-pilot" title="Direct link to 4. Amazon Q as co-pilot">‚Äã</a></h2>
<p>Amazon Q CLI didn't just automate‚Äîit elevated the entire workflow. When I asked it to create a Dockerfile that could efficiently package transformers and the EmbeddingGemma model, it didn't just generate code‚Äîit explained why sentence-transformers was the right choice over raw transformers.</p>
<p>For infrastructure, Q generated a clean CDK stack targeting Lambda with ARM64 architecture and 2GB memory. When builds failed or performance lagged, Q helped interpret CloudWatch logs and suggested memory optimizations.</p>
<p>Having Claude Sonnet inside Q made this feel like pair programming with someone who actually understood ML deployment patterns.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-performance-results">5. Performance results<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#5-performance-results" class="hash-link" aria-label="Direct link to 5. Performance results" title="Direct link to 5. Performance results">‚Äã</a></h2>
<p>The numbers tell the story:</p>
<ul>
<li><strong>Cold start</strong>: 12 seconds (not bad for a 300M model)</li>
<li><strong>Warm inference</strong>: 0.12-0.33 seconds per embedding</li>
<li><strong>Cost</strong>: ~$0.001 per request for short texts</li>
<li><strong>Memory sweet spot</strong>: 2GB (4GB+ shows no improvement)</li>
</ul>
<p>Combined with BitNet for text generation, this setup creates a complete serverless AI toolkit that shines for:</p>
<ul>
<li><strong>RAG systems</strong>: BitNet for generation, EmbeddingGemma for retrieval</li>
<li><strong>Semantic search</strong>: Document vectorization and similarity matching</li>
<li><strong>Prototype APIs</strong>: Quick AI services for testing and experimentation</li>
</ul>
<p>It struggles with:</p>
<ul>
<li><strong>Batch processing</strong>: Linear scaling kills economics</li>
<li><strong>Real-time chat</strong>: 12-second cold starts hurt UX</li>
<li><strong>High throughput</strong>: Concurrent requests need full memory allocation</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-the-convergence">6. The convergence<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#6-the-convergence" class="hash-link" aria-label="Direct link to 6. The convergence" title="Direct link to 6. The convergence">‚Äã</a></h2>
<p>Two trends are colliding: models are getting more efficient while serverless platforms evolve. EmbeddingGemma represents the "efficient model" side‚Äîcompact, purpose-built, and CPU-friendly.</p>
<p>On the platform side, we're seeing serverless runtimes optimize for AI workloads. When these trends meet‚Äîlightweight models and AI-aware serverless compute‚Äîdeploying embeddings will be as casual as deploying a REST API.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7-reality-check">7. Reality check<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#7-reality-check" class="hash-link" aria-label="Direct link to 7. Reality check" title="Direct link to 7. Reality check">‚Äã</a></h2>
<p>Let's be honest about the numbers:</p>
<p><strong>Text length scaling</strong>:</p>
<ul>
<li>10 characters: 0.32s</li>
<li>99 characters: 1.05s</li>
<li>588 characters: 4.06s</li>
</ul>
<p><strong>Memory efficiency</strong>:</p>
<ul>
<li>2GB: Optimal performance</li>
<li>4GB+: No improvement, 2x cost</li>
</ul>
<p><strong>Infrastructure overhead</strong>: 0.7-0.8 seconds of the total latency is network + AWS API processing, not model inference.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="8-why-not-production">8. Why not production<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#8-why-not-production" class="hash-link" aria-label="Direct link to 8. Why not production" title="Direct link to 8. Why not production">‚Äã</a></h2>
<p>While technically successful, several factors make this unsuitable for serious workloads:</p>
<p><strong>Economics don't scale</strong>: 2GB memory allocation for sporadic requests burns money. SageMaker's auto-scaling and GPU optimization provide better cost-per-embedding at volume.</p>
<p><strong>Cold start penalty</strong>: 12-second delays kill user experience for interactive applications.</p>
<p><strong>Better alternatives exist</strong>: Purpose-built ML infrastructure (SageMaker, ECS with GPUs) offers superior performance and economics for production embedding workloads.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="9-the-real-value">9. The real value<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#9-the-real-value" class="hash-link" aria-label="Direct link to 9. The real value" title="Direct link to 9. The real value">‚Äã</a></h2>
<p>This experiment's worth isn't in production deployment‚Äîit's about curiosity. What happens when you run Google's EmbeddingGemma in AWS Lambda? Can a 300M parameter embedding model really work in serverless compute? How does it perform?</p>
<p><strong>Curiosity-driven insights</strong>: How EmbeddingGemma behaves in Lambda's constraints, memory optimization patterns for embedding models, and container packaging strategies you can only discover by trying.</p>
<p><strong>Learning by doing</strong>: Understanding where EmbeddingGemma's efficiency meets Lambda's limitations, and where the serverless tax becomes prohibitive for ML workloads.</p>
<p><strong>Future signals</strong>: As embedding models get more efficient and Lambda evolves, today's experiments with EmbeddingGemma become tomorrow's possibilities.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="10-wrapping-up">10. Wrapping up<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#10-wrapping-up" class="hash-link" aria-label="Direct link to 10. Wrapping up" title="Direct link to 10. Wrapping up">‚Äã</a></h2>
<p>Running Google's EmbeddingGemma on AWS Lambda isn't about beating dedicated ML infrastructure‚Äîit's about curiosity. What if you could deploy Google's embedding model as easily as a REST API? What would EmbeddingGemma's performance look like in Lambda? How much would it cost?</p>
<p>The question was simple: "What about embeddings on Lambda?" Sometimes the best experiments come from pure curiosity about what's possible when you combine Google's efficient embedding model with AWS's serverless compute.</p>
<p>The complete EmbeddingGemma-on-Lambda implementation is on <a href="https://github.com/manu-mishra/embeddings-gemma-on-lambda" target="_blank" rel="noopener noreferrer">GitHub</a>. Clone it, try it, break it. See how far you can push EmbeddingGemma in Lambda before reaching for SageMaker.</p>
<p>And if you're curious about other Google models on AWS Lambda, let's chat about what other "impossible" combinations might be worth trying.</p>
<hr>
<p><em>This project was built using vibe coding techniques with Amazon Q Developer, demonstrating how AI-assisted development can accelerate experimentation while maintaining architectural rigor.</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="https://manumishra.com/blog/embeddings-gemma-on-lambda#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">‚Äã</a></h2>
<ul>
<li><a href="https://huggingface.co/google/embeddinggemma-300m" target="_blank" rel="noopener noreferrer">Google EmbeddingGemma Model</a></li>
<li><a href="https://ai.google.dev/gemma/docs/embeddinggemma" target="_blank" rel="noopener noreferrer">EmbeddingGemma Overview</a></li>
<li><a href="https://github.com/manu-mishra/embeddings-gemma-on-lambda" target="_blank" rel="noopener noreferrer">Project Repository</a></li>
<li><a href="https://github.com/manu-mishra/embeddings-gemma-on-lambda/blob/main/docs/PERFORMANCE.md" target="_blank" rel="noopener noreferrer">Performance Benchmarks</a></li>
<li><a href="https://community.aws/content/2ynHjrct8JLUEN6mADtT2IYh5bR/microsoft-bitnet-1-58-bit-llms-on-aws-lambda" target="_blank" rel="noopener noreferrer">Previous BitNet Lambda Article</a></li>
<li><a href="https://www.linkedin.com/pulse/vibe-coding-vegas-158-bit-llm-aws-lambda-manu-mishra-s0joc/" target="_blank" rel="noopener noreferrer">Vibe Coding in Vegas LinkedIn Article</a></li>
<li><a href="https://aws.amazon.com/q/developer/" target="_blank" rel="noopener noreferrer">Amazon Q Developer</a></li>
<li><a href="https://aws.amazon.com/sagemaker/" target="_blank" rel="noopener noreferrer">Amazon SageMaker</a></li>
</ul>]]></content:encoded>
            <category>aws lambda</category>
            <category>embeddings</category>
            <category>google gemma</category>
            <category>serverless</category>
            <category>machine learning</category>
            <category>multilingual</category>
            <category>cost optimization</category>
        </item>
        <item>
            <title><![CDATA[Running 1.58-bit LLMs on AWS Lambda - When Serverless Meets Extreme Quantization]]></title>
            <link>https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda</link>
            <guid>https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda</guid>
            <pubDate>Fri, 20 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Deploy Microsoft BitNet 1.58-bit quantized LLM on AWS Lambda using container-based architecture. Includes performance benchmarks, multi-stage Docker build, and complete deployment guide.]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" alt="BitNet on AWS Lambda" src="https://manumishra.com/assets/images/bitnet-on-lambda-79b5780ed52f60f02aded51ddc499e0f.png" width="1536" height="1024" class="img_ev3q"></p>
<p>‚ú® <strong>What you'll learn (tl;dr)</strong> In ~12 minutes you'll see how to deploy Microsoft's BitNet 1.58-bit quantized LLM on AWS Lambda, the container-based architecture, and performance benchmarks across different memory configurations using the <code>microsoft/bitnet-b1.58-2B-4T</code> model.</p>
<p><strong>Big idea</strong>: 1.58-bit quantization enables LLM deployment on Lambda's CPU infrastructure. At ~1.1GB, the model fits within Lambda's constraints for serverless AI inference.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-bitnet-on-lambda">Deploying BitNet on Lambda<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#deploying-bitnet-on-lambda" class="hash-link" aria-label="Direct link to Deploying BitNet on Lambda" title="Direct link to Deploying BitNet on Lambda">‚Äã</a></h2>
<p>Microsoft's BitNet <code>microsoft/bitnet-b1.58-2B-4T</code> is a 1.58-bit quantized model that uses ternary values <!-- -->1<!-- -->. At ~1.1GB, it fits within Lambda's memory and storage constraints.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-characteristics">Model Characteristics<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#model-characteristics" class="hash-link" aria-label="Direct link to Model Characteristics" title="Direct link to Model Characteristics">‚Äã</a></h2>
<p>Microsoft's BitNet <code>microsoft/bitnet-b1.58-2B-4T</code> uses 1.58-bit quantization with ternary values <!-- -->1<!-- -->:</p>
<ul>
<li><strong>Model size</strong>: ~1.1GB including dependencies</li>
<li><strong>CPU inference</strong>: No GPU required</li>
<li><strong>Memory requirements</strong>: Fits within Lambda's memory limits</li>
<li><strong>Text processing</strong>: Designed for natural language tasks</li>
</ul>
<p>Lambda bills per millisecond and doesn't provide GPU access, making CPU-optimized models necessary.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">‚Äã</a></h2>
<p><img decoding="async" loading="lazy" alt="BitNet Lambda Architecture" src="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 13.0.1 (20250615.1724)
 -->
<!-- Title: INFRA Pages: 1 -->
<svg width="778pt" height="349pt"
 viewBox="0.00 0.00 778.00 349.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 345.47)">
<title>INFRA</title>
<polygon fill="white" stroke="none" points="-4,4 -4,-345.47 773.61,-345.47 773.61,4 -4,4"/>
<!-- User -->
<g id="node1" class="node">
<title>User</title>
<ellipse fill="#ffe5b4" stroke="black" stroke-width="2" cx="64.29" cy="-303.48" rx="64.29" ry="36"/>
<text xml:space="preserve" text-anchor="middle" x="64.29" y="-305.93" font-family="Arial" font-size="14.00">User/Client</text>
<text xml:space="preserve" text-anchor="middle" x="64.29" y="-290.18" font-family="Arial" font-size="14.00">(API Requests)</text>
</g>
<!-- BitNetLambda -->
<g id="node2" class="node">
<title>BitNetLambda</title>
<path fill="#ff9900" stroke="black" stroke-width="2" d="M403.66,-196.5C403.66,-196.5 254.91,-196.5 254.91,-196.5 248.91,-196.5 242.91,-190.5 242.91,-184.5 242.91,-184.5 242.91,-136.5 242.91,-136.5 242.91,-130.5 248.91,-124.5 254.91,-124.5 254.91,-124.5 403.66,-124.5 403.66,-124.5 409.66,-124.5 415.66,-130.5 415.66,-136.5 415.66,-136.5 415.66,-184.5 415.66,-184.5 415.66,-190.5 409.66,-196.5 403.66,-196.5"/>
<text xml:space="preserve" text-anchor="middle" x="329.29" y="-170.82" font-family="Arial" font-size="14.00">BitNet Lambda Function</text>
<text xml:space="preserve" text-anchor="middle" x="329.29" y="-155.07" font-family="Arial" font-size="14.00">(AWS::Lambda::Function)</text>
<text xml:space="preserve" text-anchor="middle" x="329.29" y="-139.32" font-family="Arial" font-size="14.00">Container Image</text>
</g>
<!-- User&#45;&gt;BitNetLambda -->
<g id="edge1" class="edge">
<title>User&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M46.95,-268.66C40.42,-250.74 37.19,-229.41 49.54,-214.5 72.55,-186.72 160.24,-173.39 231.39,-167.06"/>
<polygon fill="black" stroke="black" points="231.48,-170.57 241.15,-166.23 230.89,-163.59 231.48,-170.57"/>
<text xml:space="preserve" text-anchor="middle" x="93.41" y="-234.2" font-family="Times,serif" font-size="14.00">HTTP Request</text>
<text xml:space="preserve" text-anchor="middle" x="93.41" y="-217.7" font-family="Times,serif" font-size="14.00">(JSON payload)</text>
</g>
<!-- BitNetLambda&#45;&gt;User -->
<g id="edge2" class="edge">
<title>BitNetLambda&#45;&gt;User</title>
<path fill="none" stroke="black" d="M242.75,-176.09C214,-184 183.14,-196.09 158.79,-214.5 144.83,-225.06 149.24,-234.71 137.29,-247.5 130.44,-254.83 122.53,-261.89 114.49,-268.37"/>
<polygon fill="black" stroke="black" points="112.35,-265.6 106.6,-274.49 116.65,-271.13 112.35,-265.6"/>
<text xml:space="preserve" text-anchor="middle" x="203.04" y="-234.2" font-family="Times,serif" font-size="14.00">AI Response</text>
<text xml:space="preserve" text-anchor="middle" x="203.04" y="-217.7" font-family="Times,serif" font-size="14.00">(Generated text)</text>
</g>
<!-- CloudWatchLogs -->
<g id="node4" class="node">
<title>CloudWatchLogs</title>
<path fill="#90ee90" stroke="black" stroke-width="2" d="M582.04,-72C582.04,-72 444.54,-72 444.54,-72 438.54,-72 432.54,-66 432.54,-60 432.54,-60 432.54,-12 432.54,-12 432.54,-6 438.54,0 444.54,0 444.54,0 582.04,0 582.04,0 588.04,0 594.04,-6 594.04,-12 594.04,-12 594.04,-60 594.04,-60 594.04,-66 588.04,-72 582.04,-72"/>
<text xml:space="preserve" text-anchor="middle" x="513.29" y="-38.45" font-family="Arial" font-size="14.00">CloudWatch Logs</text>
<text xml:space="preserve" text-anchor="middle" x="513.29" y="-22.7" font-family="Arial" font-size="14.00">(AWS::Logs::LogGroup)</text>
</g>
<!-- BitNetLambda&#45;&gt;CloudWatchLogs -->
<g id="edge4" class="edge">
<title>BitNetLambda&#45;&gt;CloudWatchLogs</title>
<path fill="none" stroke="black" d="M382.51,-124.07C403.87,-109.85 428.64,-93.35 450.85,-78.57"/>
<polygon fill="black" stroke="black" points="452.54,-81.65 458.93,-73.19 448.66,-75.82 452.54,-81.65"/>
<text xml:space="preserve" text-anchor="middle" x="471.68" y="-93.2" font-family="Times,serif" font-size="14.00">Function Logs</text>
</g>
<!-- ECRRepo -->
<g id="node3" class="node">
<title>ECRRepo</title>
<path fill="#b7e0ff" stroke="black" stroke-width="2" d="M293.66,-332.94C293.66,-336.55 263.02,-339.48 225.29,-339.48 187.56,-339.48 156.91,-336.55 156.91,-332.94 156.91,-332.94 156.91,-274.03 156.91,-274.03 156.91,-270.42 187.56,-267.48 225.29,-267.48 263.02,-267.48 293.66,-270.42 293.66,-274.03 293.66,-274.03 293.66,-332.94 293.66,-332.94"/>
<path fill="none" stroke="black" stroke-width="2" d="M293.66,-332.94C293.66,-329.33 263.02,-326.39 225.29,-326.39 187.56,-326.39 156.91,-329.33 156.91,-332.94"/>
<text xml:space="preserve" text-anchor="middle" x="225.29" y="-305.93" font-family="Arial" font-size="14.00">ECR Repository</text>
<text xml:space="preserve" text-anchor="middle" x="225.29" y="-290.18" font-family="Arial" font-size="14.00">(Container Registry)</text>
</g>
<!-- ECRRepo&#45;&gt;BitNetLambda -->
<g id="edge3" class="edge">
<title>ECRRepo&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M240.98,-267.29C250.41,-246.88 261.88,-223.48 268.79,-214.5 271.27,-211.26 273.98,-208.08 276.83,-204.97"/>
<polygon fill="black" stroke="black" points="279.11,-207.65 283.57,-198.04 274.09,-202.77 279.11,-207.65"/>
<text xml:space="preserve" text-anchor="middle" x="314.54" y="-225.95" font-family="Times,serif" font-size="14.00">Container Image</text>
</g>
<!-- BitNetModel -->
<g id="node5" class="node">
<title>BitNetModel</title>
<path fill="#fff5cd" stroke="black" stroke-width="2" d="M470.54,-334.56C470.54,-338.37 435.02,-341.47 391.29,-341.47 347.56,-341.47 312.04,-338.37 312.04,-334.56 312.04,-334.56 312.04,-272.41 312.04,-272.41 312.04,-268.6 347.56,-265.5 391.29,-265.5 435.02,-265.5 470.54,-268.6 470.54,-272.41 470.54,-272.41 470.54,-334.56 470.54,-334.56"/>
<path fill="none" stroke="black" stroke-width="2" d="M470.54,-334.56C470.54,-330.75 435.02,-327.66 391.29,-327.66 347.56,-327.66 312.04,-330.75 312.04,-334.56"/>
<text xml:space="preserve" text-anchor="middle" x="391.29" y="-313.81" font-family="Arial" font-size="14.00">BitNet 1.58B Model</text>
<text xml:space="preserve" text-anchor="middle" x="391.29" y="-298.06" font-family="Arial" font-size="14.00">(ggml&#45;model&#45;i2_s.gguf)</text>
<text xml:space="preserve" text-anchor="middle" x="391.29" y="-282.31" font-family="Arial" font-size="14.00">Embedded in Container</text>
</g>
<!-- BitNetModel&#45;&gt;BitNetLambda -->
<g id="edge5" class="edge">
<title>BitNetModel&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M382.4,-265.22C377.93,-249.21 371.85,-230.54 364.29,-214.5 363.07,-211.93 361.75,-209.33 360.35,-206.75"/>
<polygon fill="black" stroke="black" points="363.44,-205.1 355.4,-198.21 357.38,-208.61 363.44,-205.1"/>
<text xml:space="preserve" text-anchor="middle" x="422.43" y="-225.95" font-family="Times,serif" font-size="14.00">Model Inference</text>
</g>
<!-- LambdaRole -->
<g id="node6" class="node">
<title>LambdaRole</title>
<path fill="#dda0dd" stroke="black" stroke-width="2" d="M622.69,-336.4C622.69,-336.4 510.57,-306.57 510.57,-306.57 504.77,-305.03 504.77,-301.94 510.57,-300.4 510.57,-300.4 622.69,-270.57 622.69,-270.57 628.49,-269.03 640.09,-269.03 645.88,-270.57 645.88,-270.57 758.01,-300.4 758.01,-300.4 763.81,-301.94 763.81,-305.03 758.01,-306.57 758.01,-306.57 645.88,-336.4 645.88,-336.4 640.09,-337.94 628.49,-337.94 622.69,-336.4"/>
<text xml:space="preserve" text-anchor="middle" x="634.29" y="-305.93" font-family="Arial" font-size="14.00">Lambda Execution Role</text>
<text xml:space="preserve" text-anchor="middle" x="634.29" y="-290.18" font-family="Arial" font-size="14.00">(AWS::IAM::Role)</text>
</g>
<!-- LambdaRole&#45;&gt;BitNetLambda -->
<g id="edge6" class="edge">
<title>LambdaRole&#45;&gt;BitNetLambda</title>
<path fill="none" stroke="black" d="M572.63,-283.46C549.23,-274.61 523.11,-262.64 501.79,-247.5 485.75,-236.11 488.55,-225.58 472.29,-214.5 458.29,-204.96 442.35,-196.88 426.33,-190.12"/>
<polygon fill="black" stroke="black" points="427.79,-186.94 417.2,-186.44 425.16,-193.43 427.79,-186.94"/>
<text xml:space="preserve" text-anchor="middle" x="564.04" y="-225.95" font-family="Times,serif" font-size="14.00">Execution Permissions</text>
</g>
<!-- LambdaRole&#45;&gt;CloudWatchLogs -->
<g id="edge7" class="edge">
<title>LambdaRole&#45;&gt;CloudWatchLogs</title>
<path fill="none" stroke="black" d="M634.88,-267.43C634.21,-250.98 632.03,-231.34 626.29,-214.5 609.4,-164.99 576.04,-115.33 549.93,-81.18"/>
<polygon fill="black" stroke="black" points="552.95,-79.35 544.05,-73.6 547.42,-83.64 552.95,-79.35"/>
<text xml:space="preserve" text-anchor="middle" x="664.1" y="-155.45" font-family="Times,serif" font-size="14.00">Log Permissions</text>
</g>
</g>
</svg>
" width="1037" height="465" class="img_ev3q"></p>
<p>The deployment uses serverless execution with the model embedded in the container image:</p>
<ul>
<li><strong>No network calls during inference</strong> - Model and code are in the same container</li>
<li><strong>Single deployment unit</strong> - No external model storage dependencies</li>
<li><strong>Consistent behavior</strong> - Same environment across all invocations</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-stage-docker-build">Multi-Stage Docker Build<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#multi-stage-docker-build" class="hash-link" aria-label="Direct link to Multi-Stage Docker Build" title="Direct link to Multi-Stage Docker Build">‚Äã</a></h3>
<p>The deployment uses a multi-stage Docker build that separates compilation from runtime.</p>
<p><strong>Stage 1: Builder Environment</strong></p>
<p>Creates a development environment to compile BitNet from source. Uses <code>python:3.9-bullseye</code> with cmake, build-essential, git, and clang.</p>
<p>The critical step is generating ARM-optimized computational kernels. Lambda runs on ARM64 processors, so BitNet's code generation utility pre-compiles optimized matrix multiplication kernels for this architecture.</p>
<p>Compilation includes Lambda-specific optimizations: OpenMP disabled (<code>-DGGML_OPENMP=OFF</code>) because Lambda's sandboxed environment doesn't support shared memory operations. ARM template optimizations enabled (<code>-DBITNET_ARM_TL1=ON</code>) for ARM64 instruction sets. Static linking (<code>-DBUILD_SHARED_LIBS=OFF</code>) embeds all dependencies into the binary.</p>
<p><strong>Stage 2: Runtime Environment</strong></p>
<p>Creates a minimal production environment using <code>python:3.9-slim</code>. Installs only AWS Lambda Runtime Interface Client (<code>awslambdaric</code>) and <code>requests</code> library.</p>
<p>Copies only the compiled <code>llama-server</code> binary and BitNet model file from the builder stage. The final container includes the optimized binary without build tools, source code, or compilation artifacts.</p>
<div class="language-dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-dockerfile codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Stage 1: Builder - Heavy build environment</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM python:3.9-bullseye as builder</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Install build dependencies</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN apt-get update &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    apt-get install -y cmake build-essential git clang &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rm -rf /var/lib/apt/lists/*</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Copy BitNet source and model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY temp/BitNet /app/BitNet</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY temp/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf /app/BitNet/models/BitNet-b1.58-2B-4T/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Generate ARM-optimized kernels for Lambda's ARM64 runtime</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN python utils/codegen_tl1.py --model bitnet_b1_58-3B --BM 160,320,320 --BK 64,128,64 --bm 32,64,32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Build with Lambda-specific optimizations</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN cmake -B build -DBITNET_ARM_TL1=ON -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -DBUILD_SHARED_LIBS=OFF -DGGML_OPENMP=OFF</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN cmake --build build --config Release</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Stage 2: Runtime - Minimal production environment</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM python:3.9-slim</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Install only runtime dependencies</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN pip install --no-cache-dir awslambdaric requests</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Copy only the compiled binary and model from builder stage</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY --from=builder /app/BitNet/build/bin/llama-server /app/bin/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY --from=builder /app/BitNet/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf /app/models/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Copy Lambda handler and set up runtime</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY app/lambda_handler.py /var/task/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">WORKDIR /var/task</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">CMD ["python", "-m", "awslambdaric", "lambda_handler.lambda_handler"]</span><br></span></code></pre></div></div>
<p><strong>Build Process</strong></p>
<p>This multi-stage approach reduces final image size and ensures Lambda compatibility by including ARM64 optimizations and removing problematic dependencies like OpenMP. Each deployment requires full compilation, but this produces a container optimized for Lambda's constraints.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lambda-runtime-optimizations">Lambda Runtime Optimizations<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#lambda-runtime-optimizations" class="hash-link" aria-label="Direct link to Lambda Runtime Optimizations" title="Direct link to Lambda Runtime Optimizations">‚Äã</a></h3>
<p>The Lambda environment requires specific threading configurations to prevent model initialization failures:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Critical environment overrides for Lambda</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">'OMP_NUM_THREADS'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'1'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">'OMP_THREAD_LIMIT'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'1'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">'GGML_OPENMP'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'OFF'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">os</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">'KMP_DUPLICATE_LIB_OK'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'TRUE'</span><br></span></code></pre></div></div>
<p>These settings prevent the threading conflicts that plague many ML workloads in Lambda's sandboxed environment.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started">Getting Started<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#getting-started" class="hash-link" aria-label="Direct link to Getting Started" title="Direct link to Getting Started">‚Äã</a></h2>
<p>The complete working implementation is available at <strong><a href="https://github.com/manu-mishra/one-bit-llm-on-lambda" target="_blank" rel="noopener noreferrer">github.com/manu-mishra/one-bit-llm-on-lambda</a></strong>. The deployment process is streamlined into three modular scripts:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-initialize-and-download">1. Initialize and Download<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#1-initialize-and-download" class="hash-link" aria-label="Direct link to 1. Initialize and Download" title="Direct link to 1. Initialize and Download">‚Äã</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">git clone https://github.com/manu-mishra/one-bit-llm-on-lambda.git</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd one-bit-llm-on-lambda</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">./scripts/1-initialize.sh</span><br></span></code></pre></div></div>
<p><strong>Important:</strong> You need a Hugging Face token to download the BitNet model:</p>
<ul>
<li>Get your token from: <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">https://huggingface.co/settings/tokens</a></li>
<li>Create a token with "Read" permissions</li>
<li>The script includes retry logic if authentication fails</li>
</ul>
<p>Downloads BitNet source and model (~1.1GB) from Microsoft's Hugging Face repository.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-deploy-infrastructure">2. Deploy Infrastructure<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#2-deploy-infrastructure" class="hash-link" aria-label="Direct link to 2. Deploy Infrastructure" title="Direct link to 2. Deploy Infrastructure">‚Äã</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd cdk &amp;&amp; python -m venv .venv &amp;&amp; source .venv/bin/activate &amp;&amp; pip install -r requirements.txt &amp;&amp; cd ..</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">./scripts/2-deploy-lambda.sh</span><br></span></code></pre></div></div>
<p>Uses AWS CDK to provision AWS Lambda, Amazon ECR, IAM roles, and supporting infrastructure.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-test-inference">3. Test Inference<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#3-test-inference" class="hash-link" aria-label="Direct link to 3. Test Inference" title="Direct link to 3. Test Inference">‚Äã</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./scripts/3-test-lambda.sh</span><br></span></code></pre></div></div>
<p>Tests the deployment with a sample prompt. For performance testing across memory configurations:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./scripts/5-benchmark.sh</span><br></span></code></pre></div></div>
<p>The repository includes AWS CDK infrastructure code, Docker configuration, testing utilities, and documentation.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance-results">Performance Results<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#performance-results" class="hash-link" aria-label="Direct link to Performance Results" title="Direct link to Performance Results">‚Äã</a></h2>
<p>Benchmarking across different memory configurations:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="memory-configuration">Memory Configuration<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#memory-configuration" class="hash-link" aria-label="Direct link to Memory Configuration" title="Direct link to Memory Configuration">‚Äã</a></h3>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">LAMBDA_MEMORY_SIZE </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2048</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># 2GB recommended</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="test-results">Test Results<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#test-results" class="hash-link" aria-label="Direct link to Test Results" title="Direct link to Test Results">‚Äã</a></h3>
<table><thead><tr><th>Memory</th><th>Cold Start</th><th>Warm (10 tokens)</th><th>Warm (50 tokens)</th><th>Warm (100 tokens)</th></tr></thead><tbody><tr><td>2GB</td><td>12s</td><td>7s</td><td>18s</td><td>32s</td></tr><tr><td>6GB</td><td>13s</td><td>6s</td><td>18s</td><td>32s</td></tr><tr><td>10GB</td><td>12s</td><td>7s</td><td>18s</td><td>32s</td></tr></tbody></table>
<p><strong>Observations:</strong></p>
<ul>
<li>Cold start times: 12-13 seconds across memory configurations</li>
<li>Warm inference scales with token count</li>
<li>Memory above 2GB shows minimal improvement</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="inference-parameters">Inference Parameters<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#inference-parameters" class="hash-link" aria-label="Direct link to Inference Parameters" title="Direct link to Inference Parameters">‚Äã</a></h3>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token property">"prompt"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"User: Explain 1-bit quantization benefits\n\nAssistant:"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token property">"n_predict"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">32</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token property">"temperature"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">0.7</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token property">"top_p"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">0.9</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token property">"top_k"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">40</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token property">"repeat_penalty"</span><span class="token operator">:</span><span class="token plain"> </span><span class="token number">1.1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre></div></div>
<p>These parameters control response generation. The model handles conversational AI, code generation, and text analysis tasks.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring-and-debugging">Monitoring and Debugging<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#monitoring-and-debugging" class="hash-link" aria-label="Direct link to Monitoring and Debugging" title="Direct link to Monitoring and Debugging">‚Äã</a></h2>
<p>CloudWatch Logs capture everything:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># View recent logs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">aws logs tail /aws/lambda/{your-log-group-name} --follow</span><br></span></code></pre></div></div>
<p>Key metrics to monitor:</p>
<ul>
<li><strong>Cold start duration</strong>: 12-13 seconds</li>
<li><strong>Inference latency</strong>: Scales with token count (7s for 10 tokens, 32s for 100 tokens)</li>
<li><strong>Memory utilization</strong>: Monitor against allocated limit</li>
<li><strong>Error rates</strong>: Watch for OOM or timeout errors</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-notes">Implementation Notes<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#implementation-notes" class="hash-link" aria-label="Direct link to Implementation Notes" title="Direct link to Implementation Notes">‚Äã</a></h2>
<p>This deployment pattern shows that:</p>
<ul>
<li><strong>Quantized models</strong> can run on Lambda's CPU infrastructure</li>
<li><strong>Container-based deployment</strong> enables ML workloads on Lambda</li>
<li><strong>Performance scales</strong> with token generation requirements</li>
<li><strong>Cold start times</strong> are consistent across memory configurations</li>
</ul>
<p>The approach works for applications with sporadic inference needs where 12-13 second cold starts are acceptable.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What's Next?<a href="https://manumishra.com/blog/deploy-microsoft-bitnet-llm-on-aws-lambda#whats-next" class="hash-link" aria-label="Direct link to What's Next?" title="Direct link to What's Next?">‚Äã</a></h2>
<p>BitNet + Lambda deployment has specific trade-offs. 1.58-bit quantization enables serverless deployment but has accuracy limitations compared to full-precision models. This makes it suitable for specific use cases.</p>
<p>Areas of development include:</p>
<ul>
<li><strong>Quantization techniques</strong>: Improving model accuracy while maintaining efficiency</li>
<li><strong>Application matching</strong>: Finding use cases where the efficiency/accuracy trade-off works</li>
<li><strong>Hybrid workflows</strong>: Combining lightweight models with full-precision models for different tasks</li>
<li><strong>Model routing</strong>: Automatically selecting appropriate model sizes based on request complexity</li>
</ul>
<p>The field is developing, and current approaches may be replaced by better quantization methods or deployment patterns.</p>
<p><strong>Key takeaway</strong>: 1.58-bit quantization enables LLM deployment on Lambda's CPU infrastructure. This approach has specific use cases and performance characteristics, demonstrating one path for serverless AI inference without GPU requirements.</p>]]></content:encoded>
            <category>aws lambda</category>
            <category>llm</category>
            <category>quantization</category>
            <category>serverless</category>
            <category>bitnet</category>
            <category>machine learning</category>
            <category>cost optimization</category>
        </item>
        <item>
            <title><![CDATA[Threat Modeling for Autonomous AI - What OWASP Wants You to Know]]></title>
            <link>https://manumishra.com/blog/threat-modeling-autonomous-ai</link>
            <guid>https://manumishra.com/blog/threat-modeling-autonomous-ai</guid>
            <pubDate>Fri, 16 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[As large language models (LLMs) evolve from passive responders into autonomous agents that can reason, plan, and act‚Äîwelcome to the age of Agentic AI. These systems don't just generate answers; they browse the web, execute scripts, send emails, and even orchestrate other agents. And with that autonomy comes an entirely new class of cybersecurity threats.]]></description>
            <content:encoded><![CDATA[<p>As large language models (LLMs) evolve from passive responders into autonomous agents that can reason, plan, and act‚Äîwelcome to the age of Agentic AI. These systems don't just generate answers; they browse the web, execute scripts, send emails, and even orchestrate other agents. And with that autonomy comes an entirely new class of cybersecurity threats.</p>
<p>The OWASP Agentic AI: Threats and Mitigations report is the first of its kind to lay out a structured threat model tailored to the unique risks introduced by LLM-powered agents. From memory poisoning and cascading hallucinations to identity spoofing and rogue agents‚Äîthis is the new frontline of AI security.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-shift-from-passive-to-agentic-ai">The Shift from Passive to Agentic AI<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#the-shift-from-passive-to-agentic-ai" class="hash-link" aria-label="Direct link to The Shift from Passive to Agentic AI" title="Direct link to The Shift from Passive to Agentic AI">‚Äã</a></h2>
<p>Traditional LLMs operate within strict boundaries‚Äîthey receive prompts and generate responses based on their training data. Agentic AI systems, however, can:</p>
<ul>
<li>Make autonomous decisions based on goals and context</li>
<li>Access external tools and APIs to gather information</li>
<li>Execute actions in digital (and potentially physical) environments</li>
<li>Learn and adapt their strategies over time</li>
<li>Collaborate with other AI agents to achieve complex objectives</li>
</ul>
<p>This expanded capability set creates an entirely new attack surface that traditional security approaches aren't designed to address.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-owasp-agentic-ai-threat-model">The OWASP Agentic AI Threat Model<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#the-owasp-agentic-ai-threat-model" class="hash-link" aria-label="Direct link to The OWASP Agentic AI Threat Model" title="Direct link to The OWASP Agentic AI Threat Model">‚Äã</a></h2>
<p>The OWASP report identifies several critical threat categories unique to autonomous AI systems:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-agent-memory-manipulation">1. Agent Memory Manipulation<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#1-agent-memory-manipulation" class="hash-link" aria-label="Direct link to 1. Agent Memory Manipulation" title="Direct link to 1. Agent Memory Manipulation">‚Äã</a></h3>
<p>Unlike traditional systems where memory is protected by access controls, an AI agent's "memory" exists as context that can be manipulated through carefully crafted inputs.</p>
<p><strong>Key Threats:</strong></p>
<ul>
<li><strong>Context Poisoning</strong>: Injecting false information into the agent's working memory</li>
<li><strong>Memory Overflow</strong>: Exploiting limited context windows to force the agent to forget critical constraints or instructions</li>
<li><strong>Prompt Leaking</strong>: Tricking the agent into revealing sensitive parts of its configuration or instructions</li>
</ul>
<p><strong>Mitigations:</strong></p>
<ul>
<li>Implement memory segregation between system instructions and user inputs</li>
<li>Create immutable memory regions for critical constraints and safety guardrails</li>
<li>Regularly validate the consistency of the agent's memory state</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-tool-and-api-exploitation">2. Tool and API Exploitation<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#2-tool-and-api-exploitation" class="hash-link" aria-label="Direct link to 2. Tool and API Exploitation" title="Direct link to 2. Tool and API Exploitation">‚Äã</a></h3>
<p>Agentic AI systems often have access to external tools and APIs, creating potential pathways for attackers to exploit.</p>
<p><strong>Key Threats:</strong></p>
<ul>
<li><strong>Tool Injection</strong>: Manipulating the agent to use tools in unintended ways</li>
<li><strong>API Privilege Escalation</strong>: Tricking the agent into using APIs with higher privileges than necessary</li>
<li><strong>Chained Tool Attacks</strong>: Using sequences of seemingly benign tool calls that combine for malicious purposes</li>
</ul>
<p><strong>Mitigations:</strong></p>
<ul>
<li>Implement least-privilege access for all tool and API integrations</li>
<li>Create tool-specific safety boundaries and validation</li>
<li>Monitor and audit all tool usage patterns</li>
<li>Implement rate limiting and anomaly detection for tool calls</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-multi-agent-vulnerabilities">3. Multi-Agent Vulnerabilities<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#3-multi-agent-vulnerabilities" class="hash-link" aria-label="Direct link to 3. Multi-Agent Vulnerabilities" title="Direct link to 3. Multi-Agent Vulnerabilities">‚Äã</a></h3>
<p>As systems begin to deploy multiple agents that interact with each other, new attack vectors emerge.</p>
<p><strong>Key Threats:</strong></p>
<ul>
<li><strong>Agent Impersonation</strong>: Spoofing the identity of trusted agents</li>
<li><strong>Collaborative Exploitation</strong>: Using one compromised agent to manipulate others</li>
<li><strong>Consensus Manipulation</strong>: Influencing multi-agent decision processes through targeted attacks</li>
</ul>
<p><strong>Mitigations:</strong></p>
<ul>
<li>Implement strong agent authentication mechanisms</li>
<li>Create trust boundaries between agents with different privilege levels</li>
<li>Monitor inter-agent communications for anomalous patterns</li>
<li>Design consensus mechanisms resistant to manipulation</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-goal-and-planning-subversion">4. Goal and Planning Subversion<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#4-goal-and-planning-subversion" class="hash-link" aria-label="Direct link to 4. Goal and Planning Subversion" title="Direct link to 4. Goal and Planning Subversion">‚Äã</a></h3>
<p>Autonomous agents operate based on goals and planning algorithms, which creates unique vulnerabilities.</p>
<p><strong>Key Threats:</strong></p>
<ul>
<li><strong>Goal Injection</strong>: Subtly altering the agent's understanding of its objectives</li>
<li><strong>Planning Poisoning</strong>: Manipulating the agent's reasoning about how to achieve goals</li>
<li><strong>Reward Hacking</strong>: Exploiting the agent's optimization process to achieve unintended outcomes</li>
</ul>
<p><strong>Mitigations:</strong></p>
<ul>
<li>Implement explicit goal validation against safety constraints</li>
<li>Create multi-level planning oversight with safety checks</li>
<li>Design robust reward functions resistant to exploitation</li>
<li>Implement circuit breakers that halt execution when unexpected plans emerge</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementing-threat-modeling-for-agentic-ai">Implementing Threat Modeling for Agentic AI<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#implementing-threat-modeling-for-agentic-ai" class="hash-link" aria-label="Direct link to Implementing Threat Modeling for Agentic AI" title="Direct link to Implementing Threat Modeling for Agentic AI">‚Äã</a></h2>
<p>The OWASP report recommends a structured approach to threat modeling for autonomous AI systems:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-define-the-agent-boundary">1. Define the Agent Boundary<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#1-define-the-agent-boundary" class="hash-link" aria-label="Direct link to 1. Define the Agent Boundary" title="Direct link to 1. Define the Agent Boundary">‚Äã</a></h3>
<p>Clearly document:</p>
<ul>
<li>What capabilities and tools the agent can access</li>
<li>What data the agent can read and modify</li>
<li>What actions the agent can take autonomously vs. requiring approval</li>
<li>How the agent interacts with users, systems, and other agents</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-map-the-attack-surface">2. Map the Attack Surface<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#2-map-the-attack-surface" class="hash-link" aria-label="Direct link to 2. Map the Attack Surface" title="Direct link to 2. Map the Attack Surface">‚Äã</a></h3>
<p>Identify all potential entry points:</p>
<ul>
<li>User inputs and instructions</li>
<li>External data sources</li>
<li>Tool and API integrations</li>
<li>Inter-agent communications</li>
<li>Persistence mechanisms</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-identify-threats-using-stride-a">3. Identify Threats Using STRIDE-A<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#3-identify-threats-using-stride-a" class="hash-link" aria-label="Direct link to 3. Identify Threats Using STRIDE-A" title="Direct link to 3. Identify Threats Using STRIDE-A">‚Äã</a></h3>
<p>Extend the traditional STRIDE model with Autonomy considerations:</p>
<ul>
<li><strong>Spoofing</strong>: Can attackers impersonate users or other agents?</li>
<li><strong>Tampering</strong>: Can attackers modify the agent's memory or context?</li>
<li><strong>Repudiation</strong>: Can attackers deny actions taken by the agent?</li>
<li><strong>Information Disclosure</strong>: Can attackers extract sensitive information?</li>
<li><strong>Denial of Service</strong>: Can attackers disrupt the agent's functioning?</li>
<li><strong>Elevation of Privilege</strong>: Can attackers gain unauthorized capabilities?</li>
<li><strong>Autonomy Subversion</strong>: Can attackers manipulate the agent's goals or planning?</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-implement-defense-in-depth">4. Implement Defense in Depth<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#4-implement-defense-in-depth" class="hash-link" aria-label="Direct link to 4. Implement Defense in Depth" title="Direct link to 4. Implement Defense in Depth">‚Äã</a></h3>
<p>Create multiple layers of protection:</p>
<ul>
<li><strong>Prevention</strong>: Input validation, tool sandboxing, memory protection</li>
<li><strong>Detection</strong>: Anomaly monitoring, safety checking, goal validation</li>
<li><strong>Response</strong>: Circuit breakers, human oversight, rollback mechanisms</li>
<li><strong>Recovery</strong>: State restoration, incident analysis, continuous improvement</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-security-by-design-for-the-age-of-autonomous-ai">Conclusion: Security by Design for the Age of Autonomous AI<a href="https://manumishra.com/blog/threat-modeling-autonomous-ai#conclusion-security-by-design-for-the-age-of-autonomous-ai" class="hash-link" aria-label="Direct link to Conclusion: Security by Design for the Age of Autonomous AI" title="Direct link to Conclusion: Security by Design for the Age of Autonomous AI">‚Äã</a></h2>
<p>As AI systems gain greater autonomy, security can no longer be an afterthought. The OWASP Agentic AI report provides a crucial framework for understanding and addressing the unique security challenges of autonomous systems.</p>
<p>By implementing structured threat modeling early in the development process, organizations can harness the transformative potential of agentic AI while managing the novel risks these systems introduce. The goal isn't to limit innovation but to ensure that autonomous systems operate safely, reliably, and in alignment with human intentions‚Äîeven in the face of sophisticated attacks.</p>]]></content:encoded>
            <category>AI security</category>
            <category>threat modeling</category>
            <category>OWASP</category>
            <category>LLM</category>
            <category>autonomous agents</category>
        </item>
        <item>
            <title><![CDATA[From Data Chaos to Data Confidence - A Pragmatic Playbook for Self‚ÄëSustaining Data Governance]]></title>
            <link>https://manumishra.com/blog/data-governance-playbook</link>
            <guid>https://manumishra.com/blog/data-governance-playbook</guid>
            <pubDate>Fri, 02 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[‚ú® What you'll learn (tl;dr) In ~8 minutes you'll see why most data‚Äëgovernance efforts stall, how to turn governance into load‚Äëbearing scaffolding, and the exact roadmap, roles, and rituals that move you from ad‚Äëhoc chaos to self‚Äësustaining confidence‚Äîwithout freezing delivery.]]></description>
            <content:encoded><![CDATA[<p>‚ú® <strong>What you'll learn (tl;dr)</strong> In ~8 minutes you'll see why most data‚Äëgovernance efforts stall, how to turn governance into load‚Äëbearing scaffolding, and the exact roadmap, roles, and rituals that move you from ad‚Äëhoc chaos to self‚Äësustaining confidence‚Äîwithout freezing delivery.</p>
<p><strong>Big idea</strong>: Data governance isn't red tape; it's the scaffolding that lets strategic initiatives‚Äîfrom AI to customer experience‚Äîscale safely and evolve fast. Bake lightweight governance into culture, rituals, and engineering workflows so raw data turns into durable business value without slowing delivery.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-data-governance-paradox">The Data Governance Paradox<a href="https://manumishra.com/blog/data-governance-playbook#the-data-governance-paradox" class="hash-link" aria-label="Direct link to The Data Governance Paradox" title="Direct link to The Data Governance Paradox">‚Äã</a></h2>
<p>Most organizations find themselves caught in a frustrating paradox: they know data governance is essential, yet implementation efforts often stall or become bureaucratic obstacles to the very innovation they're meant to enable.</p>
<p>The problem isn't the concept of governance itself‚Äîit's our approach. Traditional governance frameworks tend to be:</p>
<ul>
<li>Too heavyweight and process-oriented</li>
<li>Disconnected from day-to-day engineering workflows</li>
<li>Focused on control rather than enablement</li>
<li>Implemented as a separate initiative rather than integrated into existing work</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reframing-data-governance-as-scaffolding">Reframing Data Governance as Scaffolding<a href="https://manumishra.com/blog/data-governance-playbook#reframing-data-governance-as-scaffolding" class="hash-link" aria-label="Direct link to Reframing Data Governance as Scaffolding" title="Direct link to Reframing Data Governance as Scaffolding">‚Äã</a></h2>
<p>Effective data governance should function like scaffolding on a construction site‚Äîproviding structure and safety without becoming the building itself. It should:</p>
<ul>
<li>Support and accelerate strategic initiatives, not compete with them</li>
<li>Grow and adapt as your data ecosystem evolves</li>
<li>Provide just enough structure to ensure safety and quality</li>
<li>Eventually become invisible as good practices become embedded in culture</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-self-sustaining-data-governance-roadmap">The Self-Sustaining Data Governance Roadmap<a href="https://manumishra.com/blog/data-governance-playbook#the-self-sustaining-data-governance-roadmap" class="hash-link" aria-label="Direct link to The Self-Sustaining Data Governance Roadmap" title="Direct link to The Self-Sustaining Data Governance Roadmap">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="phase-1-foundation-1-3-months">Phase 1: Foundation (1-3 months)<a href="https://manumishra.com/blog/data-governance-playbook#phase-1-foundation-1-3-months" class="hash-link" aria-label="Direct link to Phase 1: Foundation (1-3 months)" title="Direct link to Phase 1: Foundation (1-3 months)">‚Äã</a></h3>
<ul>
<li><strong>Identify your data domains</strong> and assign clear ownership</li>
<li><strong>Establish a lightweight data catalog</strong> focusing first on your most critical data assets</li>
<li><strong>Define minimum viable metadata standards</strong> that provide immediate value</li>
<li><strong>Create simple data quality checks</strong> that can be automated</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="phase-2-integration-3-6-months">Phase 2: Integration (3-6 months)<a href="https://manumishra.com/blog/data-governance-playbook#phase-2-integration-3-6-months" class="hash-link" aria-label="Direct link to Phase 2: Integration (3-6 months)" title="Direct link to Phase 2: Integration (3-6 months)">‚Äã</a></h3>
<ul>
<li><strong>Embed governance checkpoints</strong> into existing development workflows</li>
<li><strong>Implement automated policy enforcement</strong> where possible</li>
<li><strong>Establish regular data quality reviews</strong> tied to business outcomes</li>
<li><strong>Create feedback loops</strong> between data producers and consumers</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="phase-3-acceleration-6-12-months">Phase 3: Acceleration (6-12 months)<a href="https://manumishra.com/blog/data-governance-playbook#phase-3-acceleration-6-12-months" class="hash-link" aria-label="Direct link to Phase 3: Acceleration (6-12 months)" title="Direct link to Phase 3: Acceleration (6-12 months)">‚Äã</a></h3>
<ul>
<li><strong>Develop self-service capabilities</strong> for common data needs</li>
<li><strong>Implement data observability</strong> to proactively identify issues</li>
<li><strong>Create communities of practice</strong> around key data domains</li>
<li><strong>Measure and communicate governance value</strong> in business terms</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="phase-4-self-sustaining-12-months">Phase 4: Self-Sustaining (12+ months)<a href="https://manumishra.com/blog/data-governance-playbook#phase-4-self-sustaining-12-months" class="hash-link" aria-label="Direct link to Phase 4: Self-Sustaining (12+ months)" title="Direct link to Phase 4: Self-Sustaining (12+ months)">‚Äã</a></h3>
<ul>
<li><strong>Decentralize governance decisions</strong> to domain teams</li>
<li><strong>Continuously refine based on feedback</strong> and changing needs</li>
<li><strong>Celebrate and recognize</strong> good data stewardship</li>
<li><strong>Evolve governance as technology changes</strong></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-roles-in-modern-data-governance">Key Roles in Modern Data Governance<a href="https://manumishra.com/blog/data-governance-playbook#key-roles-in-modern-data-governance" class="hash-link" aria-label="Direct link to Key Roles in Modern Data Governance" title="Direct link to Key Roles in Modern Data Governance">‚Äã</a></h2>
<p>Effective governance requires clear roles, but they don't have to be full-time positions:</p>
<ul>
<li><strong>Data Domain Owners</strong>: Accountable for the quality and usability of data in their domain</li>
<li><strong>Data Stewards</strong>: Hands-on practitioners who implement governance within their teams</li>
<li><strong>Data Governance Council</strong>: Cross-functional group that sets priorities and resolves conflicts</li>
<li><strong>Data Platform Team</strong>: Provides the technical foundation for governance implementation</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rituals-that-make-governance-stick">Rituals That Make Governance Stick<a href="https://manumishra.com/blog/data-governance-playbook#rituals-that-make-governance-stick" class="hash-link" aria-label="Direct link to Rituals That Make Governance Stick" title="Direct link to Rituals That Make Governance Stick">‚Äã</a></h2>
<p>Sustainable governance requires regular touchpoints that keep it visible without becoming burdensome:</p>
<ul>
<li><strong>Weekly</strong>: Quick data quality checks and issue triage</li>
<li><strong>Monthly</strong>: Data domain reviews focused on improvements</li>
<li><strong>Quarterly</strong>: Governance retrospectives and priority setting</li>
<li><strong>Annually</strong>: Comprehensive data strategy alignment</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="measuring-success">Measuring Success<a href="https://manumishra.com/blog/data-governance-playbook#measuring-success" class="hash-link" aria-label="Direct link to Measuring Success" title="Direct link to Measuring Success">‚Äã</a></h2>
<p>Effective governance should demonstrate clear business value through:</p>
<ul>
<li><strong>Reduced time-to-insight</strong> for new analytics initiatives</li>
<li><strong>Increased trust</strong> in data-driven decisions</li>
<li><strong>Lower remediation costs</strong> from data issues</li>
<li><strong>Faster onboarding</strong> of new data sources</li>
<li><strong>Improved compliance</strong> with reduced manual effort</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-from-governance-to-confidence">Conclusion: From Governance to Confidence<a href="https://manumishra.com/blog/data-governance-playbook#conclusion-from-governance-to-confidence" class="hash-link" aria-label="Direct link to Conclusion: From Governance to Confidence" title="Direct link to Conclusion: From Governance to Confidence">‚Äã</a></h2>
<p>The ultimate goal isn't perfect governance‚Äîit's data confidence. When your organization can trust its data, move quickly without breaking things, and continuously improve data quality as part of normal operations, you've achieved the true purpose of governance.</p>
<p>By focusing on pragmatic implementation, clear ownership, and integration with existing workflows, you can transform data governance from a bureaucratic burden into a strategic enabler that accelerates innovation while managing risk.</p>]]></content:encoded>
            <category>data governance</category>
            <category>data management</category>
            <category>data strategy</category>
            <category>organizational culture</category>
        </item>
        <item>
            <title><![CDATA[Tackling Digital Standstill Through the Theory of Constraints - A New Lens on Technical Debt]]></title>
            <link>https://manumishra.com/blog/digital-standstill-theory-constraints</link>
            <guid>https://manumishra.com/blog/digital-standstill-theory-constraints</guid>
            <pubDate>Fri, 23 Aug 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">‚Äã</a></h2>
<p>In an age where digital transformation is more than just a buzzword, achieving optimum operational efficiency has become a vital focus for businesses. Companies striving to evolve and stay ahead often encounter the phenomenon of a 'Digital Standstill'‚Äîa term referring to the stagnation in innovation and development caused by accumulating technical debt.</p>
<p>In this article, I intend to shed light on how the Theory of Constraints can provide a systematic approach to overcoming the challenge posed by technical debt.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-theory-of-constraints">What is the Theory of Constraints?<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#what-is-the-theory-of-constraints" class="hash-link" aria-label="Direct link to What is the Theory of Constraints?" title="Direct link to What is the Theory of Constraints?">‚Äã</a></h2>
<p>Developed by Dr. Eliyahu Goldratt, the Theory of Constraints (ToC) is a management paradigm that posits a chain is only as strong as its weakest link. In the context of business, the weakest link or 'constraint' limits the performance of the entire system. The objective of ToC is to identify these constraints and strengthen them to elevate the system's overall throughput.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="technical-debt-the-invisible-enemy">Technical Debt: The Invisible Enemy<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#technical-debt-the-invisible-enemy" class="hash-link" aria-label="Direct link to Technical Debt: The Invisible Enemy" title="Direct link to Technical Debt: The Invisible Enemy">‚Äã</a></h2>
<p>Like financial debt, technical debt isn't inherently evil; it can provide short-term benefits such as faster time-to-market. The problem arises when this debt isn't "paid off" timely through code refactoring, documentation, or other methods to improve code quality. The result? A Digital Standstill, where the accrued debt impedes progress, much like a bottleneck in a manufacturing process.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="technical-debt-is-not-evil">Technical Debt is Not Evil<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#technical-debt-is-not-evil" class="hash-link" aria-label="Direct link to Technical Debt is Not Evil" title="Direct link to Technical Debt is Not Evil">‚Äã</a></h2>
<p>Contrary to popular opinion, technical debt is not always a byproduct of sloppy programming or lax project management. In many cases, it's a strategic decision, allowing companies to act more agilely. Technical debt can be compared to taking out a loan to speed up growth‚Äîbeneficial if managed well. The key is disciplined 'repayment,' which involves the continuous investment of time and resources in code quality, documentation, and system architecture.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="applying-theory-of-constraints-to-technical-debt">Applying Theory of Constraints to Technical Debt<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#applying-theory-of-constraints-to-technical-debt" class="hash-link" aria-label="Direct link to Applying Theory of Constraints to Technical Debt" title="Direct link to Applying Theory of Constraints to Technical Debt">‚Äã</a></h2>
<p>Just as ToC identifies and optimizes the 'constraints' or 'bottlenecks' in a process, it can be applied to technical debt management. Here's how:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-identify-the-constraint">Step 1: Identify the Constraint<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#step-1-identify-the-constraint" class="hash-link" aria-label="Direct link to Step 1: Identify the Constraint" title="Direct link to Step 1: Identify the Constraint">‚Äã</a></h3>
<p>Find out what elements of your technical debt are holding you back the most. Is it poorly documented code, outdated libraries, or perhaps inefficient algorithms?</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-exploit-the-constraint">Step 2: Exploit the Constraint<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#step-2-exploit-the-constraint" class="hash-link" aria-label="Direct link to Step 2: Exploit the Constraint" title="Direct link to Step 2: Exploit the Constraint">‚Äã</a></h3>
<p>Once identified, focus on optimizing this weakest link. Allocate resources to refactor the 'most expensive' parts of your debt, and make them more manageable.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-subordinate-all-else-to-the-constraint">Step 3: Subordinate All Else to the Constraint<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#step-3-subordinate-all-else-to-the-constraint" class="hash-link" aria-label="Direct link to Step 3: Subordinate All Else to the Constraint" title="Direct link to Step 3: Subordinate All Else to the Constraint">‚Äã</a></h3>
<p>Redirect resources from less critical tasks and focus on relieving the identified bottleneck. It might mean pausing new feature development briefly, but the long-term benefits often justify the short-term costs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-elevate-the-constraint">Step 4: Elevate the Constraint<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#step-4-elevate-the-constraint" class="hash-link" aria-label="Direct link to Step 4: Elevate the Constraint" title="Direct link to Step 4: Elevate the Constraint">‚Äã</a></h3>
<p>If you find that even after exploitation, the constraint is still a bottleneck, look for ways to remove it entirely, perhaps through significant refactoring or even a system overhaul.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5-repeat">Step 5: Repeat<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#step-5-repeat" class="hash-link" aria-label="Direct link to Step 5: Repeat" title="Direct link to Step 5: Repeat">‚Äã</a></h3>
<p>Once a constraint is removed or optimized, a new constraint will typically appear. The process is cyclical, and you must continue to identify new constraints and optimize them.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/digital-standstill-theory-constraints#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>The Theory of Constraints offers a powerful framework for systematically addressing and mitigating the limitations imposed by technical debt. As businesses strive to innovate and scale, understanding how to manage technical debt becomes increasingly crucial. By employing the Theory of Constraints, companies can effectively prioritize their 'debt repayment' strategy, thereby escaping the paralyzing grip of a Digital Standstill and paving the way for sustainable growth.</p>]]></content:encoded>
            <category>technical debt</category>
            <category>theory of constraints</category>
            <category>digital transformation</category>
            <category>software development</category>
        </item>
        <item>
            <title><![CDATA[The Three "C"s of COE - From Center to Centering to Culture of Excellence]]></title>
            <link>https://manumishra.com/blog/three-cs-of-coe</link>
            <guid>https://manumishra.com/blog/three-cs-of-coe</guid>
            <pubDate>Fri, 27 Oct 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Technological realm is always changing, and organizations must constantly navigate through turbulent waves and shifting currents. The compass guiding many on this voyage has been the Centers of Excellence (COE). But is the COE an eternal beacon, or does it have its sunset?]]></description>
            <content:encoded><![CDATA[<p>Technological realm is always changing, and organizations must constantly navigate through turbulent waves and shifting currents. The compass guiding many on this voyage has been the Centers of Excellence (COE). But is the COE an eternal beacon, or does it have its sunset?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-evolution-of-centers-of-excellence">The Evolution of Centers of Excellence<a href="https://manumishra.com/blog/three-cs-of-coe#the-evolution-of-centers-of-excellence" class="hash-link" aria-label="Direct link to The Evolution of Centers of Excellence" title="Direct link to The Evolution of Centers of Excellence">‚Äã</a></h2>
<p>Centers of Excellence have traditionally been established as centralized hubs of expertise, designed to standardize practices, drive innovation, and ensure quality across an organization. They've been the go-to solution for organizations looking to build competency in specific areas, from technology adoption to process improvement.</p>
<p>However, as organizations evolve and the pace of technological change accelerates, the traditional COE model is being challenged. The rigid structures and centralized control that once provided stability can now hinder agility and innovation. This has led to a rethinking of the COE concept, moving from a centralized "Center" to a more distributed "Centering" approach, and ultimately towards fostering a "Culture of Excellence" throughout the organization.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-three-cs-of-coe">The Three "C"s of COE<a href="https://manumishra.com/blog/three-cs-of-coe#the-three-cs-of-coe" class="hash-link" aria-label="Direct link to The Three &quot;C&quot;s of COE" title="Direct link to The Three &quot;C&quot;s of COE">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-center-of-excellence">1. Center of Excellence<a href="https://manumishra.com/blog/three-cs-of-coe#1-center-of-excellence" class="hash-link" aria-label="Direct link to 1. Center of Excellence" title="Direct link to 1. Center of Excellence">‚Äã</a></h3>
<p>The traditional Center of Excellence is characterized by:</p>
<ul>
<li>Centralized expertise and control</li>
<li>Standardized practices and methodologies</li>
<li>Formal governance structures</li>
<li>Focus on quality and consistency</li>
</ul>
<p>This model works well in stable environments where standardization and control are paramount. However, it can create bottlenecks and slow down innovation in fast-paced, dynamic contexts.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-centering-of-excellence">2. Centering of Excellence<a href="https://manumishra.com/blog/three-cs-of-coe#2-centering-of-excellence" class="hash-link" aria-label="Direct link to 2. Centering of Excellence" title="Direct link to 2. Centering of Excellence">‚Äã</a></h3>
<p>As organizations recognize the limitations of centralized control, many are shifting towards a "Centering of Excellence" approach:</p>
<ul>
<li>Distributed expertise with central coordination</li>
<li>Flexible guidelines rather than rigid standards</li>
<li>Collaborative governance</li>
<li>Focus on enablement and support</li>
</ul>
<p>This model balances the need for consistency with the flexibility required for innovation and agility. It recognizes that excellence can't be confined to a single center but must be nurtured throughout the organization.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-culture-of-excellence">3. Culture of Excellence<a href="https://manumishra.com/blog/three-cs-of-coe#3-culture-of-excellence" class="hash-link" aria-label="Direct link to 3. Culture of Excellence" title="Direct link to 3. Culture of Excellence">‚Äã</a></h3>
<p>The ultimate evolution is towards a "Culture of Excellence" where:</p>
<ul>
<li>Excellence is embedded in organizational values and behaviors</li>
<li>Everyone is empowered to innovate and improve</li>
<li>Self-governance based on shared principles</li>
<li>Focus on continuous learning and adaptation</li>
</ul>
<p>In this model, excellence isn't a department or a process‚Äîit's a mindset that permeates every aspect of the organization.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="making-the-transition">Making the Transition<a href="https://manumishra.com/blog/three-cs-of-coe#making-the-transition" class="hash-link" aria-label="Direct link to Making the Transition" title="Direct link to Making the Transition">‚Äã</a></h2>
<p>Transitioning from a Center to a Culture of Excellence doesn't happen overnight. It requires:</p>
<ul>
<li>Leadership commitment to distributed excellence</li>
<li>Investment in building capabilities across the organization</li>
<li>Tolerance for experimentation and learning from failure</li>
<li>Recognition and reward systems that reinforce the desired culture</li>
<li>Continuous communication and reinforcement of shared values</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/three-cs-of-coe#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>The journey from Center to Culture represents a fundamental shift in how organizations approach excellence. Rather than relying on a select group of experts to drive quality and innovation, forward-thinking organizations are recognizing that true excellence comes from creating an environment where everyone is empowered to contribute their best.</p>
<p>As you consider your organization's approach to excellence, ask yourself: Are we building a Center, or are we cultivating a Culture? The answer may determine your ability to navigate the ever-changing technological landscape successfully.</p>]]></content:encoded>
            <category>center of excellence</category>
            <category>organizational culture</category>
            <category>leadership</category>
            <category>transformation</category>
        </item>
        <item>
            <title><![CDATA[Priming Business Flywheel with Gen-AI]]></title>
            <link>https://manumishra.com/blog/priming-business-flywheel-genai</link>
            <guid>https://manumishra.com/blog/priming-business-flywheel-genai</guid>
            <pubDate>Thu, 31 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Achieving sustained growth is the ultimate dream for many businesses, but how to realize that dream is often elusive. One proven way is to leverage the "flywheel effect," a concept that advocates for creating a self-perpetuating growth cycle through customer satisfaction and word-of-mouth referrals. And as we move further into the age of AI, the potential for supercharging your flywheel becomes even more palpable. Here's a look at how incorporating Generative AI into your flywheel model can boost your business.]]></description>
            <content:encoded><![CDATA[<p>Achieving sustained growth is the ultimate dream for many businesses, but how to realize that dream is often elusive. One proven way is to leverage the "flywheel effect," a concept that advocates for creating a self-perpetuating growth cycle through customer satisfaction and word-of-mouth referrals. And as we move further into the age of AI, the potential for supercharging your flywheel becomes even more palpable. Here's a look at how incorporating Generative AI into your flywheel model can boost your business.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-flywheel-effect">What is the Flywheel Effect?<a href="https://manumishra.com/blog/priming-business-flywheel-genai#what-is-the-flywheel-effect" class="hash-link" aria-label="Direct link to What is the Flywheel Effect?" title="Direct link to What is the Flywheel Effect?">‚Äã</a></h2>
<p>Originally conceived by Jim Collins in his book "Good to Great," the flywheel effect is a business model that focuses on turning your customers into your greatest salespeople. The cycle typically has three main stages: Attract, Engage, and Delight. This self-sustaining system gains momentum with each happy customer, requiring less effort to maintain over time.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="components-of-the-flywheel-model">Components of the Flywheel Model:<a href="https://manumishra.com/blog/priming-business-flywheel-genai#components-of-the-flywheel-model" class="hash-link" aria-label="Direct link to Components of the Flywheel Model:" title="Direct link to Components of the Flywheel Model:">‚Äã</a></h2>
<p><strong>Attract</strong>: This phase involves using various channels such as SEO, targeted advertising, social media, and events to draw potential customers towards your product or service.</p>
<p><strong>Engage</strong>: After capturing the attention, businesses should make it easy for customers to understand the product, offering free trials and educational content to encourage self-service.</p>
<p><strong>Delight</strong>: This is where you make the product experience as effortless as possible. Customer support, extensive documentation, and solicited feedback help transform a user into a fan.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="challenges-of-the-flywheel-model">Challenges of the Flywheel Model<a href="https://manumishra.com/blog/priming-business-flywheel-genai#challenges-of-the-flywheel-model" class="hash-link" aria-label="Direct link to Challenges of the Flywheel Model" title="Direct link to Challenges of the Flywheel Model">‚Äã</a></h2>
<p>It sounds easy enough, but reducing friction at each customer lifecycle stage can be extremely challenging. How can Generative AI come to the rescue?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-generative-ai-can-help">How Generative AI Can Help<a href="https://manumishra.com/blog/priming-business-flywheel-genai#how-generative-ai-can-help" class="hash-link" aria-label="Direct link to How Generative AI Can Help" title="Direct link to How Generative AI Can Help">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-data-driven-personalization">1. Data-Driven Personalization<a href="https://manumishra.com/blog/priming-business-flywheel-genai#1-data-driven-personalization" class="hash-link" aria-label="Direct link to 1. Data-Driven Personalization" title="Direct link to 1. Data-Driven Personalization">‚Äã</a></h3>
<p>One of the challenges in the "Attract" phase is understanding what your target audience wants. Generative AI can analyze vast data sets and offer insights into consumer behavior, effectively allowing you to tailor your content for maximum attraction.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-automated-customer-service">2. Automated Customer Service<a href="https://manumishra.com/blog/priming-business-flywheel-genai#2-automated-customer-service" class="hash-link" aria-label="Direct link to 2. Automated Customer Service" title="Direct link to 2. Automated Customer Service">‚Äã</a></h3>
<p>Bots powered by Generative AI can handle the "Engage" phase by answering customer queries, guiding them through your products, and even assisting in purchasing. These bots can operate 24/7, thus ensuring that the flywheel never stops.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-quality-control-and-feedback-loop">3. Quality Control and Feedback Loop<a href="https://manumishra.com/blog/priming-business-flywheel-genai#3-quality-control-and-feedback-loop" class="hash-link" aria-label="Direct link to 3. Quality Control and Feedback Loop" title="Direct link to 3. Quality Control and Feedback Loop">‚Äã</a></h3>
<p>Generative AI can scrutinize customer feedback, product reviews, and other inputs during the "Delight" stage to identify areas for improvement. Automated survey tools and sentiment analysis can make determining what makes your customers tick easier.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-predictive-analysis-for-customer-retention">4. Predictive Analysis for Customer Retention<a href="https://manumishra.com/blog/priming-business-flywheel-genai#4-predictive-analysis-for-customer-retention" class="hash-link" aria-label="Direct link to 4. Predictive Analysis for Customer Retention" title="Direct link to 4. Predictive Analysis for Customer Retention">‚Äã</a></h3>
<p>Generative AI can help analyze historical data to predict future customer behavior, enabling proactive measures to increase retention. Customer churn prediction can significantly help to refine your Delight phase, ensuring the flywheel keeps spinning.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-streamlined-operations">5. Streamlined Operations<a href="https://manumishra.com/blog/priming-business-flywheel-genai#5-streamlined-operations" class="hash-link" aria-label="Direct link to 5. Streamlined Operations" title="Direct link to 5. Streamlined Operations">‚Äã</a></h3>
<p>Your internal operations, from procurement to after-sales service, can also benefit from AI, making the process frictionless and efficient. This efficiency can enhance the force applied to your flywheel, making it spin faster and more effectively.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="points-to-consider">Points to Consider<a href="https://manumishra.com/blog/priming-business-flywheel-genai#points-to-consider" class="hash-link" aria-label="Direct link to Points to Consider" title="Direct link to Points to Consider">‚Äã</a></h2>
<ul>
<li>Generative AI is a powerful tool requiring careful planning and execution.</li>
<li>As with any model, the flywheel effect is unsuitable for all types of businesses, particularly those dealing with highly customized or high-cost products.</li>
<li>The use of AI should align with your company's core values and objectives for a cohesive growth strategy.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/priming-business-flywheel-genai#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>The flywheel model has already proven its worth in creating self-sustaining business growth. Introducing Generative AI into this framework can revolutionize your approach to attract, engage, and delight customers. By leveraging the power of AI, you're not just keeping the flywheel spinning; you're accelerating it toward unprecedented growth. The future of business growth is not just about adding more force but about intelligently amplifying it. Welcome to the age of AI-powered flywheels.</p>]]></content:encoded>
            <category>generative AI</category>
            <category>business growth</category>
            <category>flywheel effect</category>
            <category>AI strategy</category>
        </item>
        <item>
            <title><![CDATA[Unified Systems - The Tech Trend You Never Knew You Needed]]></title>
            <link>https://manumishra.com/blog/unified-systems</link>
            <guid>https://manumishra.com/blog/unified-systems</guid>
            <pubDate>Thu, 27 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Trends come and go, but certain principles stand the test of time. One such enduring principle is that of the 'unified system'. Have you ever been frustrated by a tool that just wouldn't fit into your ecosystem of tools? Or discovered software you love, only to find it standing alone, incapable of integration within your established setup? Such experiences remind us of unified systems' pivotal role in delivering a seamless and satisfying user experience.]]></description>
            <content:encoded><![CDATA[<p>Trends come and go, but certain principles stand the test of time. One such enduring principle is that of the 'unified system'. Have you ever been frustrated by a tool that just wouldn't fit into your ecosystem of tools? Or discovered software you love, only to find it standing alone, incapable of integration within your established setup? Such experiences remind us of unified systems' pivotal role in delivering a seamless and satisfying user experience.</p>
<p>Take a moment to think of your most-loved software system. What makes it so appealing? Chances are, its ability to integrate effortlessly into your existing ecosystem is a major part of its appeal. That's the beauty of unified systems. In this blog, we'll explore what unified systems are, their roots, the significance of Non-Functional Requirements (NFRs) in these systems, their evolution, and the future of such systems in modern software development.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="understanding-unified-systems-from-roots-to-modern-day">Understanding Unified Systems: From Roots to Modern Day<a href="https://manumishra.com/blog/unified-systems#understanding-unified-systems-from-roots-to-modern-day" class="hash-link" aria-label="Direct link to Understanding Unified Systems: From Roots to Modern Day" title="Direct link to Understanding Unified Systems: From Roots to Modern Day">‚Äã</a></h2>
<p>Unified systems, often considered monolithic, are integrated entities designed to operate as a cohesive unit, typically managed and deployed as one. These systems have a rich history, deeply ingrained in the software world, built on principles of tight integration and seamless interaction.</p>
<p>However, in today's world of cloud computing, microservices, and distributed architectures, some consider these traditional, tightly-integrated systems as relics of the past. But this perception overlooks the enduring value of unified systems. Even as we break systems into microservices or serverless functions for the sake of scalability or resilience, our ultimate objective remains to deliver a unified, consistent, and high-quality user experience. Essentially, the principles underpinning unified systems are timeless and continue to guide modern software design.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-evolution-and-transformation-of-unified-systems">The Evolution and Transformation of Unified Systems<a href="https://manumishra.com/blog/unified-systems#the-evolution-and-transformation-of-unified-systems" class="hash-link" aria-label="Direct link to The Evolution and Transformation of Unified Systems" title="Direct link to The Evolution and Transformation of Unified Systems">‚Äã</a></h2>
<p>As we navigate the shifting currents of the tech landscape, the traditional unified systems are also evolving. Driven by technological advances and changing consumer expectations, new pillars have been added to the structure of these systems.</p>
<p>These pillars include cloud-native design, which ensures systems are optimized for the cloud environment; API-first development, which prioritizes API development in the product lifecycle to enhance integration and interaction; and DevOps practices, which bridge the gap between development and operations to ensure smoother, faster delivery cycles.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="non-functional-requirements-nfrs-the-enduring-core-of-unified-systems">Non-Functional Requirements (NFRs): The Enduring Core of Unified Systems<a href="https://manumishra.com/blog/unified-systems#non-functional-requirements-nfrs-the-enduring-core-of-unified-systems" class="hash-link" aria-label="Direct link to Non-Functional Requirements (NFRs): The Enduring Core of Unified Systems" title="Direct link to Non-Functional Requirements (NFRs): The Enduring Core of Unified Systems">‚Äã</a></h2>
<p>At the heart of any robust unified system are Non-Functional Requirements (NFRs). NFRs refer to system properties or characteristics like security, scalability, usability, and reliability. They form the bedrock upon which systems are designed and built. Focusing on NFRs during the design and development phase ensures the system's efficiency and maintainability and provides a superior user experience.</p>
<p>When we discuss NFRs, our minds often gravitate toward scalability, reliability, and security. Undoubtedly, these are crucial, but they only form part of the story. In the realm of modern unified systems, the plot extends beyond these to include the pivotal elements of integration and ease of development.</p>
<ul>
<li><strong>Ease of Integration</strong>: When building a unified system, it is important to facilitate a platform that can help integrate effectively with other components. Even if you are building a single SaaS product, you still want it to be pluggable into your customer's ecosystem.</li>
<li><strong>Interoperability</strong>: This ensures that different system components can work together effectively. In a unified system, interoperability is crucial as it enables seamless communication and collaboration between various system components, enhancing the overall functionality and user experience.</li>
<li><strong>Usability</strong>: This ensures the system is user-friendly and easy to navigate. In a unified system, usability is critical as it guarantees a seamless, intuitive user experience across the system.</li>
<li><strong>Modularity</strong>: This is the degree to which a system's components may be separated and recombined. For a unified system, modularity allows for the system to be flexible and adaptable, improving manageability and potential for reuse.</li>
<li><strong>Portability</strong>: This is the ease with which the system can be transferred from one environment to another. For a unified system, portability ensures that the system can adapt to new environments or platforms without excessive rework.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="defining-the-modern-unified-system-and-looking-to-the-future">Defining the Modern Unified System and Looking to the Future<a href="https://manumishra.com/blog/unified-systems#defining-the-modern-unified-system-and-looking-to-the-future" class="hash-link" aria-label="Direct link to Defining the Modern Unified System and Looking to the Future" title="Direct link to Defining the Modern Unified System and Looking to the Future">‚Äã</a></h2>
<p>Modern Unified System is built to ensure all parts function harmoniously, adapted to modern technologies, practices, and software development demands.</p>
<p>A unified system combines various components to work in concert and embodies modern software architecture principles like loose coupling, resilience, and scalability. These systems leverage the advantages of cloud-native design, API-first development, and DevOps practices while offering a unified, consistent user experience.</p>
<p>As trends like artificial intelligence, machine learning, and quantum computing continue to evolve, they will undoubtedly shape the future of unified systems. The challenge and opportunity for architects and developers will be to continue embodying the timeless principles of unified systems while leveraging these new technologies.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="decoupled-but-integrated">Decoupled but Integrated<a href="https://manumishra.com/blog/unified-systems#decoupled-but-integrated" class="hash-link" aria-label="Direct link to Decoupled but Integrated" title="Direct link to Decoupled but Integrated">‚Äã</a></h3>
<p>Flexible, scalable, robust components communicating and functioning together seamlessly. Read further about the following:</p>
<ul>
<li>Service Oriented Architecture (SOA)</li>
<li>Event-Driven Architecture</li>
<li>Microservices Architecture</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="api-first-design">API-First Design<a href="https://manumishra.com/blog/unified-systems#api-first-design" class="hash-link" aria-label="Direct link to API-First Design" title="Direct link to API-First Design">‚Äã</a></h3>
<p>Using APIs as a standard for system interaction enabling modular but unified architectures. Read further about the following:</p>
<ul>
<li>Producer-Consumer Pattern</li>
<li>Publish-Subscribe Pattern</li>
<li>Gateway Aggregation Pattern</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="automated-testing--deployment">Automated Testing &amp; Deployment<a href="https://manumishra.com/blog/unified-systems#automated-testing--deployment" class="hash-link" aria-label="Direct link to Automated Testing &amp; Deployment" title="Direct link to Automated Testing &amp; Deployment">‚Äã</a></h3>
<p>One cannot build a unified system without using automation. CI/CD pipelines are utilized for fast, reliable, and frequent updates, maintaining unity. Read further about the following:</p>
<ul>
<li>Continuous Integration / Continuous Deployment (CI/CD)</li>
<li>Blue-Green Deployment</li>
<li>Canary Releases</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cloud-native-approach">Cloud-Native Approach<a href="https://manumishra.com/blog/unified-systems#cloud-native-approach" class="hash-link" aria-label="Direct link to Cloud-Native Approach" title="Direct link to Cloud-Native Approach">‚Äã</a></h3>
<p>Using services provided by cloud platforms (public or private) for scalability, resilience, speed, and cost-effective scaling of individual components. Read further about the following:</p>
<ul>
<li>On-demand Scalability</li>
<li>Multitenancy</li>
<li>Elastic Load Balancing</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="interoperability">Interoperability<a href="https://manumishra.com/blog/unified-systems#interoperability" class="hash-link" aria-label="Direct link to Interoperability" title="Direct link to Interoperability">‚Äã</a></h3>
<p>Prioritizing the ability of different technologies to work together effectively. Read further about the following:</p>
<ul>
<li>Hub and Spoke Model</li>
<li>Adapter Pattern</li>
<li>Bridge Pattern</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="security">Security<a href="https://manumishra.com/blog/unified-systems#security" class="hash-link" aria-label="Direct link to Security" title="Direct link to Security">‚Äã</a></h3>
<p>A holistic approach that secures all system parts against increasing cyber threats. Read further about the following:</p>
<ul>
<li>Defense in Depth</li>
<li>Least Privilege Principle</li>
<li>Security by Design</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="user-centric-design">User-Centric Design<a href="https://manumishra.com/blog/unified-systems#user-centric-design" class="hash-link" aria-label="Direct link to User-Centric Design" title="Direct link to User-Centric Design">‚Äã</a></h3>
<p>Prioritizing user experience, ensuring all system parts provide a seamless user experience. Read further about the following:</p>
<ul>
<li>Customer Journey Mapping</li>
<li>Persona Development</li>
<li>Usability Testing</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-driven-decision-making">Data-Driven Decision Making<a href="https://manumishra.com/blog/unified-systems#data-driven-decision-making" class="hash-link" aria-label="Direct link to Data-Driven Decision Making" title="Direct link to Data-Driven Decision Making">‚Äã</a></h3>
<p>Using data and analytics to align system parts with organizational objectives and performance indicators. Read further about the following:</p>
<ul>
<li>Feedback Loop</li>
<li>Key Performance Indicators (KPIs) Development</li>
<li>Data-Driven Prototyping</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/unified-systems#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>In conclusion, a unified system is the ultimate outcome, no matter how we develop modern software systems.</p>
<p>I invite you, fellow developers, architects, and tech enthusiasts, to join in this exciting journey of transforming unified systems for tomorrow. Share your thoughts, experiences, and ideas on how we can continue to uphold the principles of unified systems while embracing the opportunities offered by new technologies.</p>]]></content:encoded>
            <category>unified systems</category>
            <category>software architecture</category>
            <category>NFRs</category>
            <category>integration</category>
            <category>cloud-native</category>
        </item>
        <item>
            <title><![CDATA[Rethinking API-First - Unveiling Its True Power in the AI Era]]></title>
            <link>https://manumishra.com/blog/api-first-ai-era</link>
            <guid>https://manumishra.com/blog/api-first-ai-era</guid>
            <pubDate>Tue, 18 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[APIs, or Application Programming Interfaces, are the bedrock of today's digital economy. They form the communication conduits between diverse software systems, facilitating seamless interaction. With AI becoming a game changer in reshaping businesses across sectors, an API-first approach is emerging as a non-negotiable strategy. In this article, we take a deep dive into the API-first approach, particularly in the era of AI, demystifying its core prerequisites and exploring its game-changing impacts.]]></description>
            <content:encoded><![CDATA[<p>APIs, or Application Programming Interfaces, are the bedrock of today's digital economy. They form the communication conduits between diverse software systems, facilitating seamless interaction. With AI becoming a game changer in reshaping businesses across sectors, an API-first approach is emerging as a non-negotiable strategy. In this article, we take a deep dive into the API-first approach, particularly in the era of AI, demystifying its core prerequisites and exploring its game-changing impacts.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="https://manumishra.com/blog/api-first-ai-era#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="customer-expectations">Customer Expectations<a href="https://manumishra.com/blog/api-first-ai-era#customer-expectations" class="hash-link" aria-label="Direct link to Customer Expectations" title="Direct link to Customer Expectations">‚Äã</a></h3>
<p>API-first places profound emphasis on catering to customer expectations. This approach is about delivering long-lived API interfaces that can weather the test of rapid technological evolution. It promotes constant innovation to keep businesses competitive and meet evolving user demands. An inherent focus on scalability ensures systems can handle growth without a dent in performance. Reliability and availability form the backbone of this strategy, promising uninterrupted user experiences. Importantly, this approach offers independence from the underlying infrastructure, liberating users from the need to comprehend complex system details.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="governance">Governance<a href="https://manumishra.com/blog/api-first-ai-era#governance" class="hash-link" aria-label="Direct link to Governance" title="Direct link to Governance">‚Äã</a></h3>
<p>API-first is synonymous with robust governance. APIs should be testable to guarantee peak functioning. They must also comply with standards that guide their design, development, and usage, fostering a cohesive architecture. The importance of comprehensive documentation is paramount‚Äîit empowers developers and upcoming AI agents in understanding and utilizing the APIs correctly. Effective versioning strategies manage changes over time, preserving backward compatibility. Centralized monitoring and analytics provide a window into API performance and usage patterns, leading to informed decision-making. Finally, the APIs should foster reusability and modularity, driving efficiency and consistency.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accessibility">Accessibility<a href="https://manumishra.com/blog/api-first-ai-era#accessibility" class="hash-link" aria-label="Direct link to Accessibility" title="Direct link to Accessibility">‚Äã</a></h3>
<p>Accessibility forms a key pillar of the API-first approach. Machine-friendly interfaces ensure APIs can be effortlessly consumed by other AI-based systems. Multi-device support amplifies accessibility, enabling APIs to function seamlessly across a variety of devices. Broad availability is a key facet, ensuring APIs are accessible anytime, anywhere.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="security">Security<a href="https://manumishra.com/blog/api-first-ai-era#security" class="hash-link" aria-label="Direct link to Security" title="Direct link to Security">‚Äã</a></h3>
<p>In the realm of API-first, security is a top priority, not a mere afterthought. This model includes user authentication features, sometimes even letting users bring their own identity for ease of use. It also guarantees data isolation, safeguarding sensitive information from unauthorized access. Advanced API authorization and data encryption techniques further fortify data security.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="https://manumishra.com/blog/api-first-ai-era#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">‚Äã</a></h3>
<p>API-first often aligns with a microservices architecture, breaking down complex applications into manageable, independent services. An API Gateway manages all API traffic, enhancing scalability, security, and manageability. The process of API design‚Äîdefining the endpoints, request/response formats‚Äîis a critical requirement. API lifecycle management, including design, deployment, and maintenance, is crucial to the success of an API-first approach.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="impact">Impact<a href="https://manumishra.com/blog/api-first-ai-era#impact" class="hash-link" aria-label="Direct link to Impact" title="Direct link to Impact">‚Äã</a></h2>
<p>The adoption of an API-first approach offers a wealth of benefits. It ensures a clear segregation of skills, allowing developers to focus on their areas of expertise. This strategy also reduces the risk of system failures by isolating failures to specific services instead of the entire system.</p>
<p>The API-first approach enhances both the developer and user experiences by offering well-documented, standard, and easy-to-use interfaces. It hastens the time to market by enabling parallel work and promotes system homogeneity and consistency through standardization. The API-first strategy offers flexibility to shift left or right in the development cycle, empowering teams to adapt swiftly to changing requirements or market conditions.</p>
<p>In the era of AI, an API-first approach facilitates AI integration. It makes it easier for AI systems to access, interact with, and learn from your data, rendering your system more adaptable to AI evolution. This approach ensures data is readily available and organized‚Äîa critical component for training and implementing AI models. APIs simplify the task of scaling systems, a requirement often necessitated by resource-intensive AI systems.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/api-first-ai-era#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>In conclusion, the API-first approach is more than just exposing APIs‚Äîit's a strategic choice that propels businesses to compete effectively in the dynamic digital landscape, particularly in the face of AI advancements. By prioritizing API development, organizations can harness the power of AI, drive innovation, and deliver exceptional customer experiences, setting themselves apart from the competition.</p>]]></content:encoded>
            <category>API</category>
            <category>API-first</category>
            <category>AI</category>
            <category>architecture</category>
            <category>microservices</category>
        </item>
        <item>
            <title><![CDATA[The Future with Large Language Models - A Technical Debt Worth Taking]]></title>
            <link>https://manumishra.com/blog/llm-technical-debt</link>
            <guid>https://manumishra.com/blog/llm-technical-debt</guid>
            <pubDate>Sun, 09 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[The Emergence of Generative AI and Large Language Models]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-emergence-of-generative-ai-and-large-language-models">The Emergence of Generative AI and Large Language Models<a href="https://manumishra.com/blog/llm-technical-debt#the-emergence-of-generative-ai-and-large-language-models" class="hash-link" aria-label="Direct link to The Emergence of Generative AI and Large Language Models" title="Direct link to The Emergence of Generative AI and Large Language Models">‚Äã</a></h2>
<p>The world has witnessed a meteoric rise in the use of artificial intelligence (AI) technologies over the past few years, with generative AI and large language models (LLMs) standing at the forefront. Generative AI, which includes the likes of LLMs, can generate creative and unique content, ranging from artwork to complex textual narratives. The idea of AI systems autonomously producing human-like content has transformed the AI landscape, opening up a plethora of possibilities.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-increasing-focus-of-businesses-on-generative-ai-and-llms">The Increasing Focus of Businesses on Generative AI and LLMs<a href="https://manumishra.com/blog/llm-technical-debt#the-increasing-focus-of-businesses-on-generative-ai-and-llms" class="hash-link" aria-label="Direct link to The Increasing Focus of Businesses on Generative AI and LLMs" title="Direct link to The Increasing Focus of Businesses on Generative AI and LLMs">‚Äã</a></h2>
<p>In a data-driven world, the capacity to generate, comprehend, and leverage data effectively is paramount. Businesses are not just observing the rise of generative AI and LLMs, but they are actively investing in these technologies, looking to harness their transformative potential. These technologies are revolutionizing industries by generating unique content, interpreting customer sentiments, automating customer interactions, and much more.</p>
<p>Here are a few specific use cases:</p>
<ul>
<li><strong>Financial Industry</strong>: Generative AI and LLMs are being employed for market analysis and financial forecasting. By analyzing historical data and global economic trends, they predict future market movements, assisting in more informed decision-making.</li>
<li><strong>Healthcare Sector</strong>: Generative AI is being used to predict disease outbreaks based on various health and environmental parameters. Additionally, LLMs are analyzing medical literature, facilitating better disease understanding and accelerating the drug discovery processes.</li>
<li><strong>Retail Industry</strong>: LLMs are powering chatbots that can understand and respond to customer queries efficiently and accurately. They are also used in sentiment analysis, interpreting customer reviews and social media posts to glean insights into consumer preferences and sentiments.</li>
<li><strong>Marketing and Advertising</strong>: Generative AI is used to create dynamic and personalized advertising content based on user preferences and behaviors. LLMs are employed to generate insightful reports on market trends, customer segments, and campaign effectiveness, assisting marketers in their strategic planning.</li>
<li><strong>Media and Entertainment</strong>: Generative AI is being used to create new forms of media and entertainment, from AI-composed music to automatically generated video scripts. LLMs help in scriptwriting by suggesting dialogues, predicting plot points, and even creating entire storylines.</li>
<li><strong>Education</strong>: LLMs are being used to create personalized learning materials, adapt to individual student's learning style and pace, and provide intelligent tutoring. They can also be employed to evaluate and provide feedback on student submissions.</li>
<li><strong>Transportation and Logistics</strong>: Generative AI is utilized in optimizing delivery routes, predicting shipment delays, and improving overall supply chain efficiency. LLMs can analyze historical and real-time data to provide insights for strategic planning and decision-making in this sector.</li>
<li><strong>Human Resources</strong>: Generative AI and LLMs are being used to automate the recruitment process, from screening resumes to scheduling interviews. They can also assist in employee training and performance evaluations.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-models-as-potential-technical-debt">Large Language Models as Potential Technical Debt<a href="https://manumishra.com/blog/llm-technical-debt#large-language-models-as-potential-technical-debt" class="hash-link" aria-label="Direct link to Large Language Models as Potential Technical Debt" title="Direct link to Large Language Models as Potential Technical Debt">‚Äã</a></h2>
<p>Despite the undeniable potential of LLMs, they pose a unique set of challenges that some architects equate to technical debt. These models, due to their complexity and size, require extensive computational resources, which translates into higher costs. The calculations in LLMs are orders of magnitude more expensive than those in smaller models, posing a scalability issue that could strain resources and lead to unsustainable maintenance costs. Common arguments against the use of LLMs are:</p>
<ul>
<li><strong>Long-term Costs</strong>: The computational resources required to train and run LLMs can result in high long-term costs. This includes expenses related to data storage, processing power, and energy consumption.</li>
<li><strong>Engineering Complexity</strong>: The size and complexity of LLMs can challenge traditional software engineering practices. Developing, maintaining, and scaling these models often requires specialized knowledge and tools, which can strain engineering teams.</li>
<li><strong>Accuracy Concerns</strong>: Although LLMs can generate high-quality outputs, their accuracy may vary, especially when dealing with niche or specialized topics. This can limit their effectiveness in certain use cases.</li>
<li><strong>Bias and Fairness</strong>: LLMs can unintentionally learn and propagate biases present in their training data, leading to fairness concerns. If not addressed, this can harm a company's reputation and even lead to legal issues.</li>
<li><strong>Interpretability and Transparency</strong>: LLMs, like many AI models, can often act as 'black boxes,' making it difficult to understand how they arrive at certain outputs. This lack of transparency can pose challenges in sectors where interpretability is crucial.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="leveraging-the-technical-debt-betting-on-the-future-of-large-language-models">Leveraging the Technical Debt: Betting on the Future of Large Language Models<a href="https://manumishra.com/blog/llm-technical-debt#leveraging-the-technical-debt-betting-on-the-future-of-large-language-models" class="hash-link" aria-label="Direct link to Leveraging the Technical Debt: Betting on the Future of Large Language Models" title="Direct link to Leveraging the Technical Debt: Betting on the Future of Large Language Models">‚Äã</a></h2>
<p>While the concept of technical debt often carries a negative connotation, it is important to view it as an investment in the context of LLMs. There are several compelling reasons to embrace this technological 'debt', and they are as follows:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="democratizing-llms-and-reducing-costs">Democratizing LLMs and Reducing Costs<a href="https://manumishra.com/blog/llm-technical-debt#democratizing-llms-and-reducing-costs" class="hash-link" aria-label="Direct link to Democratizing LLMs and Reducing Costs" title="Direct link to Democratizing LLMs and Reducing Costs">‚Äã</a></h3>
<p>Large corporations are making strides in democratizing LLM services, allowing businesses to pay for only what they use. Innovations in custom chips for inferencing and training are reducing these costs significantly over time. It's anticipated that as the technology matures and becomes more widespread, the costs associated with LLMs will reduce substantially, potentially turning this 'debt' into an affordable investment.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="reduced-data-quality-requirements">Reduced Data Quality Requirements<a href="https://manumishra.com/blog/llm-technical-debt#reduced-data-quality-requirements" class="hash-link" aria-label="Direct link to Reduced Data Quality Requirements" title="Direct link to Reduced Data Quality Requirements">‚Äã</a></h3>
<p>LLMs can effectively work with suboptimal data quality, reducing the onus on businesses to procure perfect data sets. These models can also be utilized to clean and refine data, further reducing the burden on data quality assurance teams.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="exploring-the-art-of-the-possible">Exploring the Art of the Possible<a href="https://manumishra.com/blog/llm-technical-debt#exploring-the-art-of-the-possible" class="hash-link" aria-label="Direct link to Exploring the Art of the Possible" title="Direct link to Exploring the Art of the Possible">‚Äã</a></h3>
<p>Generative AI and LLMs empower businesses to explore the art of the possible. They provide an avenue to deliver intelligent, innovative features to customers, drive business growth, and maintain a competitive edge in the fast-paced digital world.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="quick-market-testing">Quick Market Testing<a href="https://manumishra.com/blog/llm-technical-debt#quick-market-testing" class="hash-link" aria-label="Direct link to Quick Market Testing" title="Direct link to Quick Market Testing">‚Äã</a></h3>
<p>The 'deploy first, optimize later' strategy is yet another reason to leverage the technical debt associated with LLMs. Businesses can deliver innovative features to customers and gauge their response before investing time and resources into optimizing the model's implementation for efficiency. This approach allows for rapid prototyping, quick market feedback, and efficient utilization of resources.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-the-worthwhile-investment-in-large-language-models">Conclusion: The Worthwhile Investment in Large Language Models<a href="https://manumishra.com/blog/llm-technical-debt#conclusion-the-worthwhile-investment-in-large-language-models" class="hash-link" aria-label="Direct link to Conclusion: The Worthwhile Investment in Large Language Models" title="Direct link to Conclusion: The Worthwhile Investment in Large Language Models">‚Äã</a></h2>
<p>As we navigate the digital revolution, Large Language Models (LLMs) and generative AI technologies have become critical assets for businesses. Despite challenges like high long-term costs, complex engineering practices, varying accuracy, potential biases, and lack of transparency, the potential benefits of LLMs make them a worthwhile investment.</p>
<p>In this era of rapid technological evolution, viewing the adoption of LLMs not as a burden but as a strategic asset is crucial. Reduced data quality requirements, rapid testing and iteration, and the continued democratization of LLM services all signify that the 'technical debt' associated with LLMs is more of an investment for future gains.</p>
<p>As businesses continue to innovate and adapt, the successful integration of LLMs can lead to unprecedented opportunities and growth. It's about understanding and embracing this evolving journey, to ensure continued success in the era of generative AI.</p>]]></content:encoded>
            <category>AI</category>
            <category>LLM</category>
            <category>generative AI</category>
            <category>technical debt</category>
            <category>innovation</category>
        </item>
        <item>
            <title><![CDATA[The Craftsmanship of Software Engineering - Why We Should Objectify Tools, Not Debates]]></title>
            <link>https://manumishra.com/blog/software-craftsmanship</link>
            <guid>https://manumishra.com/blog/software-craftsmanship</guid>
            <pubDate>Tue, 09 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[The debates from 2015 are back.]]></description>
            <content:encoded><![CDATA[<p>The debates from 2015 are back.</p>
<p>Microservices v/s Monolith.<br>
<!-- -->Server v/s serverless.<br>
<!-- -->Stateless v/s Stateful<br>
<!-- -->No-SQL v/s SQL<br>
<!-- -->Screwdriver v/s Hammer<br>
<!-- -->Bread v/s Cake</p>
<p>These arguments have become the equivalent of a wrestling match in the software engineering world, where the last man standing gets the privilege to write code his way. And, of course, we all know that the only way to code is their way, right?</p>
<p>While we're engrossed in these seemingly infinite debates, guess what we're not doing? You've nailed it. We're not paying attention to the real craftsmanship of software engineering. Who cares about creating effective, sustainable, and scalable software solutions when we can spend our time arguing about whether or not to use microservices?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-distraction-of-debate">The Distraction of Debate<a href="https://manumishra.com/blog/software-craftsmanship#the-distraction-of-debate" class="hash-link" aria-label="Direct link to The Distraction of Debate" title="Direct link to The Distraction of Debate">‚Äã</a></h2>
<p>We the engineers, the creators of the digital world, spending our valuable time debating about the tools we use, rather than focusing on the craft itself. Let's not forget that in our fast-paced technical industry, distractions are plentiful. There will always be a new trend, a new tool, a new buzzword vying for our attention. The media, with its endless cycle of hype and controversy, can often amplify these distractions, leading us away from the core of our craft.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-art-of-objectifying-tools">The Art of Objectifying Tools<a href="https://manumishra.com/blog/software-craftsmanship#the-art-of-objectifying-tools" class="hash-link" aria-label="Direct link to The Art of Objectifying Tools" title="Direct link to The Art of Objectifying Tools">‚Äã</a></h2>
<p>Tools are important. They make our job easier. But, they are just that - tools. They are means to an end, not the end itself. If we can remember this simple truth, maybe we can get back to the real craftsmanship of software engineering. What I mean is that we need to view them as objects that help us achieve our goals, rather than subjects of intense debate and fanatical loyalty.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-real-value-of-craftsmanship">The Real Value of Craftsmanship<a href="https://manumishra.com/blog/software-craftsmanship#the-real-value-of-craftsmanship" class="hash-link" aria-label="Direct link to The Real Value of Craftsmanship" title="Direct link to The Real Value of Craftsmanship">‚Äã</a></h2>
<p>While we're locked in our circular debates, the digital world is moving, coding is happening, applications are being developed, and users are either benefiting or suffering. It's time we reclaim the attention that our craft deserves and innovate for the betterment of our users.</p>
<p>Remember, it's not about whether we opted for a monolith or microservices, but how effectively we used our chosen tools to deliver a high-quality product. That's the essence of our craft. That's where the real value of our work lies.</p>
<p>As engineers, we are the pioneers of the digital frontier. It's our innovations that push the boundaries of what's possible. It's our craft that brings concepts to life, that turns lines of code into transformative digital experiences.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="call-to-action">Call to Action<a href="https://manumishra.com/blog/software-craftsmanship#call-to-action" class="hash-link" aria-label="Direct link to Call to Action" title="Direct link to Call to Action">‚Äã</a></h2>
<p>So here's a gentle reminder to step back from the hype, to quiet the noise, and to focus on what truly matters. Let's put the debates aside, pick up our tools, and get back to our craft. Let's create software that's efficient, scalable, and maintainable. Let's write code that's readable and understandable.</p>
<p>In short, let's get back to the real work of software engineering. Let's create, innovate, and push the boundaries of what's possible. Trust me, your end users, and indeed the world, will thank you for it.</p>
<hr>
<blockquote>
<p>"The tools we use have a profound (and devious!) influence on our thinking habits, and, therefore, on our thinking abilities."</p>
<p>‚Äî Edsger Dijkstra</p>
</blockquote>]]></content:encoded>
            <category>software engineering</category>
            <category>craftsmanship</category>
            <category>tools</category>
            <category>debates</category>
        </item>
        <item>
            <title><![CDATA[Software Engineer vs. Developer through the Lens of Socratic Questioning]]></title>
            <link>https://manumishra.com/blog/software-engineer-vs-developer</link>
            <guid>https://manumishra.com/blog/software-engineer-vs-developer</guid>
            <pubDate>Mon, 01 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Have you ever encountered a situation where a leader uses Socratic questioning on the wrong audience? For example, asking a PHP developer why users are complaining about high cloud bills or questioning a backend engineer about a low website score on search engines. In the realm of software engineering, it's important to understand the distinctions between software engineers and software developers.]]></description>
            <content:encoded><![CDATA[<p>Have you ever encountered a situation where a leader uses Socratic questioning on the wrong audience? For example, asking a PHP developer why users are complaining about high cloud bills or questioning a backend engineer about a low website score on search engines. In the realm of software engineering, it's important to understand the distinctions between software engineers and software developers.</p>
<p>While there may not be a concrete difference, tech leaders should be aware of the nuances between these roles, especially when engaging in Socratic questioning. In this article, we will delve into their primary differences, explore situations where one role may not efficiently perform the other's responsibilities, and discuss the importance of organizations differentiating these roles and aligning them with platform and feature development.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-1-differences-between-these-roles">üîé 1. Differences between these roles:<a href="https://manumishra.com/blog/software-engineer-vs-developer#-1-differences-between-these-roles" class="hash-link" aria-label="Direct link to üîé 1. Differences between these roles:" title="Direct link to üîé 1. Differences between these roles:">‚Äã</a></h2>
<p><strong>Education &amp; Training</strong>: Software engineers typically hold formal degrees in computer science or related fields, while software developers may have similar backgrounds or be self-taught programmers from various disciplines.</p>
<p><strong>Scope of Work</strong>: Software engineers emphasize the design, planning, and high-level implementation of software systems, while software developers hone in on coding and bringing software systems to life, following given designs and specifications. This hands-on experience with software implementation often enables developers to work effectively with customers, as they can better understand and address their needs directly.</p>
<p><strong>Problem-solving Approach</strong>: Software engineers apply theoretical and systematic approaches, using engineering principles and methodologies, whereas software developers adopt a more pragmatic approach, focusing on specific tasks and coding.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="Ô∏è-2-examples-where-one-cant-effectively-do-the-job-of-another">‚ö†Ô∏è 2. Examples where one can't effectively do the job of another:<a href="https://manumishra.com/blog/software-engineer-vs-developer#%EF%B8%8F-2-examples-where-one-cant-effectively-do-the-job-of-another" class="hash-link" aria-label="Direct link to ‚ö†Ô∏è 2. Examples where one can't effectively do the job of another:" title="Direct link to ‚ö†Ô∏è 2. Examples where one can't effectively do the job of another:">‚Äã</a></h2>
<p><strong>Complex System Design and Architecture</strong>: Software engineers excel in designing and developing intricate software systems, while software developers may not possess the necessary knowledge and experience.</p>
<p><strong>Detailed Implementation and Coding</strong>: Software developers shine in implementing specific features and debugging complex code, while software engineers may not be as efficient due to their broader focus.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-3-why-organizations-should-differentiate-these-roles-and-align-them-to-platform-and-feature-development">üéØ 3. Why organizations should differentiate these roles and align them to platform and feature development:<a href="https://manumishra.com/blog/software-engineer-vs-developer#-3-why-organizations-should-differentiate-these-roles-and-align-them-to-platform-and-feature-development" class="hash-link" aria-label="Direct link to üéØ 3. Why organizations should differentiate these roles and align them to platform and feature development:" title="Direct link to üéØ 3. Why organizations should differentiate these roles and align them to platform and feature development:">‚Äã</a></h2>
<p><strong>Resource Allocation</strong>: Distinguishing between the roles can help organizations allocate resources effectively, with software engineers focusing on platform development and software developers on feature development.</p>
<p><strong>Role Clarity</strong>: Differentiating the roles provides clarity in responsibilities, ensuring an efficient software development process where engineers and developers focus on their respective strengths.</p>
<p><strong>Tailored Growth Opportunities</strong>: Organizations can offer targeted growth and learning opportunities to their team members based on their roles, contributing to higher job satisfaction and increased retention.</p>
<p><strong>Streamlined Development Process</strong>: Aligning software engineers to platform development and software developers to feature development can result in a more efficient and streamlined development process, maximizing the overall productivity of the team.</p>
<p>Remember that the distinctions between software engineers and developers are not strict, and their roles can overlap in many situations. Understanding their unique strengths and leveraging them in specific scenarios, such as platform and feature development, can lead to more efficient and successful software development projects.</p>]]></content:encoded>
            <category>software engineering</category>
            <category>software development</category>
            <category>career</category>
            <category>leadership</category>
        </item>
        <item>
            <title><![CDATA[KTLO Can Lead to Digital Inertia and Hinder Digital Transformation]]></title>
            <link>https://manumishra.com/blog/digital-inertia</link>
            <guid>https://manumishra.com/blog/digital-inertia</guid>
            <pubDate>Thu, 27 Apr 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[As a technology leader, you know that keeping the lights on is essential. But if you're too focused on KTLO, you could set your organization up for failure.]]></description>
            <content:encoded><![CDATA[<p>As a technology leader, you know that keeping the lights on is essential. But if you're too focused on KTLO, you could set your organization up for failure.</p>
<p>KTLO, or "keep the lights on," is the tendency of organizations to focus on maintaining existing systems and processes at the expense of new initiatives. This can lead to digital inertia, the tendency of organizations to resist change in their digital systems and processes.</p>
<p>Digital inertia, or the reluctance to embrace new technologies and processes, can significantly impact your organization's digital core. In this post, we'll explore how digital inertia can introduce hidden costs and hinder your organization's ability to stay competitive in the digital age.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-impact-of-digital-inertia-on-your-digital-core">The Impact of Digital Inertia on Your Digital Core<a href="https://manumishra.com/blog/digital-inertia#the-impact-of-digital-inertia-on-your-digital-core" class="hash-link" aria-label="Direct link to The Impact of Digital Inertia on Your Digital Core" title="Direct link to The Impact of Digital Inertia on Your Digital Core">‚Äã</a></h2>
<p>By maintaining outdated systems and processes, digital inertia can introduce complexity and inefficiency into your organization's digital core. These systems may be difficult to maintain and update, leading to downtime and lost productivity. Additionally, legacy systems may be unable to integrate with newer technologies, limiting your ability to take advantage of the latest advancements in your digital core.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-hidden-costs-of-digital-inertia">The Hidden Costs of Digital Inertia<a href="https://manumishra.com/blog/digital-inertia#the-hidden-costs-of-digital-inertia" class="hash-link" aria-label="Direct link to The Hidden Costs of Digital Inertia" title="Direct link to The Hidden Costs of Digital Inertia">‚Äã</a></h2>
<p>By maintaining outdated systems and processes, your organization may incur hidden costs that impact your digital core. These include the cost of maintaining outdated hardware and software, training employees on outdated systems, and dealing with security threats and downtime caused by outdated systems. Moreover, the hidden costs associated with maintaining legacy systems and processes may reinforce digital inertia as organizations become more risk-averse and hesitant to invest in digital transformation initiatives.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="identify-if-you-are-fostering-digital-inertia">Identify if you are fostering digital inertia<a href="https://manumishra.com/blog/digital-inertia#identify-if-you-are-fostering-digital-inertia" class="hash-link" aria-label="Direct link to Identify if you are fostering digital inertia" title="Direct link to Identify if you are fostering digital inertia">‚Äã</a></h2>
<p>If the answer to the following questions is "Yes," you may be fostering digital inertia:</p>
<ul>
<li>Are you reducing the amount of money you spend on training and development?</li>
<li>Are you making decisions based on how things have always been done rather than considering new ways of doing things?</li>
<li>Are you afraid of making mistakes?</li>
<li>Are you worried about the cost of change?</li>
<li>Are you not sure how to implement new technologies and processes?</li>
<li>Are your decisions based on facing resistance from employees?</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-benefits-of-embracing-digital-transformation">The Benefits of Embracing Digital Transformation<a href="https://manumishra.com/blog/digital-inertia#the-benefits-of-embracing-digital-transformation" class="hash-link" aria-label="Direct link to The Benefits of Embracing Digital Transformation" title="Direct link to The Benefits of Embracing Digital Transformation">‚Äã</a></h2>
<p>By embracing digital transformation, your organization can improve its digital core and position itself for success in the future. Digital transformation can improve efficiency and productivity, reduce downtime, and increase agility. Additionally, digital transformation can help organizations take advantage of the latest advancements in technology, such as automation and artificial intelligence, and better meet the needs of their customers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-importance-of-a-strategic-approach">The Importance of a Strategic Approach<a href="https://manumishra.com/blog/digital-inertia#the-importance-of-a-strategic-approach" class="hash-link" aria-label="Direct link to The Importance of a Strategic Approach" title="Direct link to The Importance of a Strategic Approach">‚Äã</a></h2>
<p>Digital transformation requires a strategic approach beyond simply replacing outdated systems and processes. Organizations must be willing to rethink their processes and workflows and invest in the necessary infrastructure and talent to support digital transformation initiatives. By taking a strategic approach, organizations can ensure that their digital core is agile, efficient, and effective.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/digital-inertia#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<p>In conclusion, digital inertia can significantly impact your organization's digital core. By maintaining outdated systems and processes, your organization may incur hidden costs and hinder its ability to stay competitive in the digital age. By embracing digital transformation and taking a strategic approach to your organization's digital core, you can improve efficiency, reduce downtime, and increase agility. It's time to break free from digital inertia and embrace the future!</p>]]></content:encoded>
            <category>digital transformation</category>
            <category>technology leadership</category>
            <category>KTLO</category>
            <category>digital inertia</category>
        </item>
        <item>
            <title><![CDATA[Don't Keep The Lights On]]></title>
            <link>https://manumishra.com/blog/dont-keep-lights-on</link>
            <guid>https://manumishra.com/blog/dont-keep-lights-on</guid>
            <pubDate>Thu, 06 Apr 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Consolidate or Get Consolidated]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="consolidate-or-get-consolidated">Consolidate or Get Consolidated<a href="https://manumishra.com/blog/dont-keep-lights-on#consolidate-or-get-consolidated" class="hash-link" aria-label="Direct link to Consolidate or Get Consolidated" title="Direct link to Consolidate or Get Consolidated">‚Äã</a></h2>
<p>In 2023, businesses face a choice between consolidating their systems or facing the risk of being consolidated by their competitors. With economic uncertainty and rapid technological change, companies must stay agile and adaptable to stay ahead of the competition. However, many businesses are held back by legacy systems and processes that are no longer efficient or effective.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-cost-of-keeping-the-lights-on">The Cost of Keeping the Lights On<a href="https://manumishra.com/blog/dont-keep-lights-on#the-cost-of-keeping-the-lights-on" class="hash-link" aria-label="Direct link to The Cost of Keeping the Lights On" title="Direct link to The Cost of Keeping the Lights On">‚Äã</a></h2>
<p>Maintaining outdated products and services that don't generate revenue can drain a business's finances and resources. Legacy systems often require more maintenance and support, which can divert resources away from more profitable areas of the business. Additionally, businesses that fail to adapt to changing customer needs and preferences risk losing market share to more nimble competitors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-benefits-of-deprecating-legacy-systems">The Benefits of Deprecating Legacy Systems<a href="https://manumishra.com/blog/dont-keep-lights-on#the-benefits-of-deprecating-legacy-systems" class="hash-link" aria-label="Direct link to The Benefits of Deprecating Legacy Systems" title="Direct link to The Benefits of Deprecating Legacy Systems">‚Äã</a></h2>
<p>Deprecating legacy systems can bring a range of benefits to businesses. Businesses can lower their maintenance costs and increase efficiency by reducing the number of systems and processes that need support. Legacy systems often require specialized knowledge to maintain, which can be a bottleneck regarding resourcing. By reducing the number of systems to support, businesses can free up resources to focus on more important tasks.</p>
<p>Moreover, deprecating legacy systems can also improve customer satisfaction. Outdated systems can be slow and unreliable, leading to frustration among customers who are used to faster, more intuitive experiences. By upgrading to newer, more streamlined systems, businesses can provide a better customer experience and build loyalty.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="investing-in-customers-for-long-term-success">Investing in Customers for Long-Term Success<a href="https://manumishra.com/blog/dont-keep-lights-on#investing-in-customers-for-long-term-success" class="hash-link" aria-label="Direct link to Investing in Customers for Long-Term Success" title="Direct link to Investing in Customers for Long-Term Success">‚Äã</a></h2>
<p>One way to reduce the need for old duplicate systems is to invest in customers to help them migrate to newer services. This can be done through targeted marketing campaigns, personalized support, and incentives to upgrade. By making it easier for customers to switch to newer services, businesses can reduce the need to maintain outdated systems and processes.</p>
<p>For example, Adobe successfully transitioned from selling boxed software to a cloud-based subscription model by investing in customer education and support. By offering tutorials, webinars, and personalized support, Adobe was able to help its customers migrate to the new system smoothly. The result was a more efficient and profitable business model that better met customer needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://manumishra.com/blog/dont-keep-lights-on#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2>
<ul>
<li>Assess your current systems and processes to identify legacy systems that are no longer efficient or effective.</li>
<li>Consider deprecating legacy systems to lower maintenance costs, increase efficiency, and improve customer satisfaction.</li>
<li>Invest in customer education and support to help them migrate to newer services and reduce the need to maintain outdated systems and processes.</li>
<li>Reinvest resources into more profitable business areas by shedding outdated offerings and focusing on core areas.</li>
</ul>]]></content:encoded>
            <category>business strategy</category>
            <category>legacy systems</category>
            <category>digital transformation</category>
        </item>
    </channel>
</rss>